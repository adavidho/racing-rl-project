{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Car racing using simple DQN & Viz dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import random\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_shape, num_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(input_shape[-1], 32, kernel_size=8, stride=4)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
    "        self.fc1 = nn.Linear(self._calculate_conv_output(input_shape), 512)\n",
    "        self.fc2 = nn.Linear(512, num_actions)\n",
    "\n",
    "    def _calculate_conv_output(self, input_shape: tuple) -> int:\n",
    "        o = torch.zeros(1, *reversed(input_shape))\n",
    "        o = self.conv1(o)\n",
    "        o = self.conv2(o)\n",
    "        o = self.conv3(o)\n",
    "        return int(np.prod(o.size()))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = torch.relu(self.conv3(x))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        return self.fc2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory:\n",
    "    def __init__(self, capacity: int):\n",
    "        self.memory = deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size: int):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, input_shape: tuple, num_actions: int, memory_size: int, batch_size: int, gamma: float, epsilon: float, lr: float):\n",
    "        self.memory = ReplayMemory(memory_size)\n",
    "        self.batch_size = batch_size\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.num_actions = num_actions\n",
    "\n",
    "        self.policy_net = DQN(input_shape, num_actions).to(device)\n",
    "        self.target_net = DQN(input_shape, num_actions).to(device)\n",
    "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=lr)\n",
    "        self.update_target_net()\n",
    "\n",
    "    def update_target_net(self):\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "\n",
    "    def select_action(self, state: torch.Tensor):\n",
    "        if random.random() < self.epsilon:\n",
    "            return random.randrange(self.num_actions)\n",
    "        with torch.no_grad():\n",
    "            return self.policy_net(state).argmax(dim=1).item()\n",
    "\n",
    "    def train_step(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "\n",
    "        batch = self.memory.sample(self.batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "        states = torch.tensor(torch.stack(states, dim=1).squeeze(0), dtype=torch.float32, device=device)\n",
    "        actions = torch.tensor(actions, dtype=torch.int64, device=device)\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32, device=device)\n",
    "        next_states = torch.tensor(torch.stack(next_states, dim=1).squeeze(0), dtype=torch.float32, device=device)\n",
    "        dones = torch.tensor(dones, dtype=torch.float32, device=device)\n",
    "\n",
    "        q_values = self.policy_net(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "        next_q_values = self.target_net(next_states).max(1)[0]\n",
    "        expected_q_values = rewards + (self.gamma * next_q_values * (1 - dones))\n",
    "\n",
    "        loss = nn.functional.mse_loss(q_values, expected_q_values)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\janhe\\AppData\\Local\\Temp\\ipykernel_7980\\1491307797.py:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  states = torch.tensor(torch.stack(states, dim=1).squeeze(0), dtype=torch.float32, device=device)\n",
      "C:\\Users\\janhe\\AppData\\Local\\Temp\\ipykernel_7980\\1491307797.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  next_states = torch.tensor(torch.stack(next_states, dim=1).squeeze(0), dtype=torch.float32, device=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0, Total Reward: 8.8808664259928\n",
      "Episode 1, Total Reward: 5.974025974026045\n",
      "Episode 2, Total Reward: 3.3333333333333717\n",
      "Episode 3, Total Reward: 2.5806451612903794\n",
      "Episode 4, Total Reward: 21.66666666666667\n",
      "Episode 5, Total Reward: 16.363636363636278\n",
      "Episode 6, Total Reward: 31.28205128205117\n",
      "Episode 7, Total Reward: -5.185185185185176\n",
      "Episode 8, Total Reward: -0.6451612903225789\n",
      "Episode 9, Total Reward: 0.5479452054794737\n",
      "Episode 10, Total Reward: -2.558139534883724\n",
      "Episode 11, Total Reward: -3.221476510067088\n",
      "Episode 12, Total Reward: 0.06688963210704432\n",
      "Episode 13, Total Reward: 4.390243902439035\n",
      "Episode 14, Total Reward: 1.97802197802199\n",
      "Episode 15, Total Reward: -1.9277108433734949\n",
      "Episode 16, Total Reward: 2.1518987341772253\n",
      "Episode 17, Total Reward: 2.727272727272792\n",
      "Episode 18, Total Reward: -0.5194805194805112\n",
      "Episode 19, Total Reward: -3.3333333333333206\n",
      "Episode 20, Total Reward: -6.06271777003482\n",
      "Episode 21, Total Reward: 2.900763358778677\n",
      "Episode 22, Total Reward: -0.19801980198019611\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 17\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m200\u001b[39m):\n\u001b[0;32m     16\u001b[0m     action \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mselect_action(state)\n\u001b[1;32m---> 17\u001b[0m     next_state, reward, done, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m     next_state \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(next_state, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32, device\u001b[38;5;241m=\u001b[39mdevice)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mview((\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m96\u001b[39m, \u001b[38;5;241m96\u001b[39m))\n\u001b[0;32m     19\u001b[0m     total_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n",
      "File \u001b[1;32mc:\\Users\\janhe\\Desktop\\Uni\\Reinforcement Learning\\Abgabe Aufgabe 3\\trackmania-rl-project\\env\\lib\\site-packages\\gymnasium\\wrappers\\time_limit.py:57\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[0;32m     47\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001b[39;00m\n\u001b[0;32m     48\u001b[0m \n\u001b[0;32m     49\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     55\u001b[0m \n\u001b[0;32m     56\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 57\u001b[0m     observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_episode_steps:\n",
      "File \u001b[1;32mc:\\Users\\janhe\\Desktop\\Uni\\Reinforcement Learning\\Abgabe Aufgabe 3\\trackmania-rl-project\\env\\lib\\site-packages\\gymnasium\\wrappers\\order_enforcing.py:56\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling env.reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 56\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\janhe\\Desktop\\Uni\\Reinforcement Learning\\Abgabe Aufgabe 3\\trackmania-rl-project\\env\\lib\\site-packages\\gymnasium\\wrappers\\env_checker.py:51\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_step_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, action)\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\janhe\\Desktop\\Uni\\Reinforcement Learning\\Abgabe Aufgabe 3\\trackmania-rl-project\\env\\lib\\site-packages\\gymnasium\\envs\\box2d\\car_racing.py:553\u001b[0m, in \u001b[0;36mCarRacing.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    550\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mworld\u001b[38;5;241m.\u001b[39mStep(\u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m/\u001b[39m FPS, \u001b[38;5;241m6\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m30\u001b[39m, \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m30\u001b[39m)\n\u001b[0;32m    551\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mt \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m/\u001b[39m FPS\n\u001b[1;32m--> 553\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_render\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstate_pixels\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    555\u001b[0m step_reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    556\u001b[0m terminated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\janhe\\Desktop\\Uni\\Reinforcement Learning\\Abgabe Aufgabe 3\\trackmania-rl-project\\env\\lib\\site-packages\\gymnasium\\envs\\box2d\\car_racing.py:617\u001b[0m, in \u001b[0;36mCarRacing._render\u001b[1;34m(self, mode)\u001b[0m\n\u001b[0;32m    614\u001b[0m trans \u001b[38;5;241m=\u001b[39m pygame\u001b[38;5;241m.\u001b[39mmath\u001b[38;5;241m.\u001b[39mVector2((scroll_x, scroll_y))\u001b[38;5;241m.\u001b[39mrotate_rad(angle)\n\u001b[0;32m    615\u001b[0m trans \u001b[38;5;241m=\u001b[39m (WINDOW_W \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m+\u001b[39m trans[\u001b[38;5;241m0\u001b[39m], WINDOW_H \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m4\u001b[39m \u001b[38;5;241m+\u001b[39m trans[\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m--> 617\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_render_road\u001b[49m\u001b[43m(\u001b[49m\u001b[43mzoom\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrans\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mangle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    618\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcar\u001b[38;5;241m.\u001b[39mdraw(\n\u001b[0;32m    619\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msurf,\n\u001b[0;32m    620\u001b[0m     zoom,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    623\u001b[0m     mode \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstate_pixels_list\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstate_pixels\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    624\u001b[0m )\n\u001b[0;32m    626\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msurf \u001b[38;5;241m=\u001b[39m pygame\u001b[38;5;241m.\u001b[39mtransform\u001b[38;5;241m.\u001b[39mflip(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msurf, \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\janhe\\Desktop\\Uni\\Reinforcement Learning\\Abgabe Aufgabe 3\\trackmania-rl-project\\env\\lib\\site-packages\\gymnasium\\envs\\box2d\\car_racing.py:678\u001b[0m, in \u001b[0;36mCarRacing._render_road\u001b[1;34m(self, zoom, translation, angle)\u001b[0m\n\u001b[0;32m    669\u001b[0m         grass\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m    670\u001b[0m             [\n\u001b[0;32m    671\u001b[0m                 (GRASS_DIM \u001b[38;5;241m*\u001b[39m x \u001b[38;5;241m+\u001b[39m GRASS_DIM, GRASS_DIM \u001b[38;5;241m*\u001b[39m y \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m0\u001b[39m),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    675\u001b[0m             ]\n\u001b[0;32m    676\u001b[0m         )\n\u001b[0;32m    677\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m poly \u001b[38;5;129;01min\u001b[39;00m grass:\n\u001b[1;32m--> 678\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_draw_colored_polygon\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    679\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msurf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpoly\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrass_color\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mzoom\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtranslation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mangle\u001b[49m\n\u001b[0;32m    680\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    682\u001b[0m \u001b[38;5;66;03m# draw road\u001b[39;00m\n\u001b[0;32m    683\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m poly, color \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroad_poly:\n\u001b[0;32m    684\u001b[0m     \u001b[38;5;66;03m# converting to pixel coordinates\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\janhe\\Desktop\\Uni\\Reinforcement Learning\\Abgabe Aufgabe 3\\trackmania-rl-project\\env\\lib\\site-packages\\gymnasium\\envs\\box2d\\car_racing.py:775\u001b[0m, in \u001b[0;36mCarRacing._draw_colored_polygon\u001b[1;34m(self, surface, poly, color, zoom, translation, angle, clip)\u001b[0m\n\u001b[0;32m    769\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m clip \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28many\u001b[39m(\n\u001b[0;32m    770\u001b[0m     (\u001b[38;5;241m-\u001b[39mMAX_SHAPE_DIM \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m coord[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m WINDOW_W \u001b[38;5;241m+\u001b[39m MAX_SHAPE_DIM)\n\u001b[0;32m    771\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;241m-\u001b[39mMAX_SHAPE_DIM \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m coord[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m WINDOW_H \u001b[38;5;241m+\u001b[39m MAX_SHAPE_DIM)\n\u001b[0;32m    772\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m coord \u001b[38;5;129;01min\u001b[39;00m poly\n\u001b[0;32m    773\u001b[0m ):\n\u001b[0;32m    774\u001b[0m     gfxdraw\u001b[38;5;241m.\u001b[39maapolygon(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msurf, poly, color)\n\u001b[1;32m--> 775\u001b[0m     \u001b[43mgfxdraw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilled_polygon\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msurf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpoly\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolor\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CarRacing-v2\", continuous=False)\n",
    "input_shape = env.observation_space.shape\n",
    "num_actions = env.action_space.n\n",
    "\n",
    "agent = DQNAgent(input_shape, num_actions, memory_size=10000, batch_size=64, gamma=0.99, epsilon=0.1, lr=1e-4)\n",
    "\n",
    "num_episodes = 1000\n",
    "target_update_frequency = 10\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    state, info = env.reset()\n",
    "    state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0).view((-1, 3, 96, 96))\n",
    "    total_reward = 0\n",
    "\n",
    "    for _ in range(200):\n",
    "        action = agent.select_action(state)\n",
    "        next_state, reward, done, truncated, info = env.step(action)\n",
    "        next_state = torch.tensor(next_state, dtype=torch.float32, device=device).unsqueeze(0).view((-1, 3, 96, 96))\n",
    "        total_reward += reward\n",
    "\n",
    "        agent.memory.push(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "\n",
    "        agent.train_step()\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    if episode % target_update_frequency == 0:\n",
    "        agent.update_target_net()\n",
    "\n",
    "    print(f\"Episode {episode}, Total Reward: {total_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([96, 96, 3])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "state, reward, done, truncated, info = env.step(0)\n",
    "state = torch.tensor(state, device=device)\n",
    "state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZQAAAGVCAYAAADZmQcFAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy80BEi2AAAACXBIWXMAAA9hAAAPYQGoP6dpAAAN2ElEQVR4nO3dwY+cZR0H8N22a0Es0gKtgqAUPRAoBhOpgQse+QP0wrneTTiYeKCmB+WqCQc4eWivnDyQeDAciElLw62JsViSUozRtEWwtt3ueiAmPL/d7uzOfHfeZ2Y+n9vPGWcfOvPO932e3zzvu7y+vr6+BAAT2jP0AACYDwIFgAiBAkCEQAEgQqAAECFQAIgQKABECBQAIgQKABH7tvvEEx+fmOwvXS/155O9HGEHS33vIKNgK7dK/c9BRsHd3FPqQ4OMYte89ehbI59jhgJAhEABIEKgABCx7R7KDP0lxnFn6AEw0t6hB8CWHENmKABkCBQAIgQKABHT62yIrr5Z/+1fPYaWS+3eq8NyDPmaByBDoAAQIVAAiJheD8Vv6Ptm/bd/tWdSTwe9h8OqPaxa1/dvDpmhABAhUACIECgAROih8AXr77OnHlPew2HVnkl9PxbgeoZmKABECBQAIgQKABF6KHzB+vvscUz1TQ8FAMYjUACIECgARAy3quc39H1ZK/UCXodo5uih9G0Bv9PMUACIECgARAgUACL0UNjcAv6GfuboofRtAb/TzFAAiBAoAEQIFAAi+umh0Bc9lP45HeybHgoAjEegABAhUACI0ENhcwu4/jtzHEN9W8BjyAwFgAiBAkCEQAEgQg+FzdX7o9Afx1Df9FAAYDwCBYAIgQJAxHA9FFHWt9WhB8BIeih900MBgPEIFAAiBAoAEfahsDn7UGZPPaYWcA2/K+sj6uVpDWR6zFAAiBAoAEQIFAAi9FDYnPX32aOH0re6t2tlkFHsKjMUACIECgARAgWAiH56KPU32fU320yX9ffZoy/Zt93e21W/M2vP5vaIx0c9/9HRQzBDASBCoAAQIVAAiBiuh1LVaLOGP6wFvA7RzHN62Lfao6jv16Q9j/qdOUAf2kcQgAiBAkCEQAEgop8eiusQ9aWuv9b3o59PDv9nH0rfrg89gN1nhgJAhEABIEKgABDRz0q49d++6aH0zzHEwMxQAIgQKABECBQAIvpZCbf+2zf7gvrnGGJgZigARAgUACIECgAR/fRQRFvf9FD6p4fCwHyNAxAhUACIECgARPTTQ7H+2zc9lOGNuud4rWHKzFAAiBAoAEQIFAAi9FDYHj2U0WqPY1Rdex6jnr8+zqBgesxQAIgQKABECBQAIvRQ2J61oQcwodoD2mk/oz6+WU9p1v+NYEJmKABECBQAIgQKABH99FBqtC2X2m/wh7Xb+1Bq/2HSHkd9XH8Ddp0ZCgARAgWACIECQEQ/PZTaM6lR51pSw6o9iJulnnSfh/cXZp4ZCgARAgWACIECQEQ/PZSqXtvLGntf/jX0AIDemKEAECFQAIgQKABEzE4PBYCumaEAECFQAIgQKABE6KEAEGGGAkCEQAEgQqAAEKGHAkCEGQoAEQIFgAiBAkDE7vVQdnqP8VE1AF0zQwEgQqAAECFQAIjYfg/leqlrj6P2SNbGGQ4As8oMBYAIgQJAhEABIGL7PZTPd3EUAMw8MxQAIgQKABECBYAIgQJAhEABIEKgABAhUACIECgARAgUACIECgARAgWACIECQIRAASBCoAAQIVAAiBAoAEQIFAAiBAoAEQIFgAiBAkCEQAEgQqAAECFQAIgQKABECBQAIgQKABECBYAIgQJAhEABIEKgABAhUACIECgARAgUACIECgARAgWACIECQIRAASBCoAAQIVAAiBAoAEQIFAAiBAoAEQIFgAiBAkCEQAEgQqAAECFQAIgQKABECBQAIgQKABECBYAIgQJAhEABIEKgABAhUACIECgARAgUACIECgARAgWAiH1DDwCAASyXuqbBGOlghgJAhEABIEKgABChhwIwC+rp/8qIun67j3q89lTGYIYCQIRAASBCoAAQoYcCMAuOlDrQ80gzQwEgQqAAECFQAIgQKABEaMoDzII7pe7w29sMBYAIgQJAhEABIKLDVTgANtBDAWBRCBQAIgQKABEdrsIBsMHa0AMYzQwFgAiBAkCEQAEgQg8F5kW94VI9uldGPH6r1P+deEQkrQ49gNHMUACIECgARAgUACL0UGBaao9jb6lH9ThGPT7p0fx5qfVQ+mIfCgCLQqAAECFQAIjQQ4GlpY39jXqqNap/sZ3n1Lr+zaE5vexbvR9Kh3yEAIgQKABECBQAIvRQmE077XHstL+xiKdadV8MfdFDAWBRCBQAIgQKABF6KExHXZ/fX+qd9kCcCuXpofRNDwWARSFQAIgQKABE6KEwHfWT9sAQg2BLtYdSrzW2Pq2BsKn671/rDq4NZ4YCQIRAASBCoAAQoYfCdMzA/bAp6unmDOyDmGu1Z1Lfjw6+zc1QAIgQKABECBQAIjpYdWMhWH+fPXVfivewL3ooAMwrgQJAhEABIKKDVTcWwgxch4jC/VH61uHeLjMUACIECgARAgWAiH56KHUNfdQ9xUc9frXUHa43LpQZuA4RhR5K31aHHsBGZigARAgUACIECgAR21+5rj2Our46qqcxqgeSXkOv49ND6YseSv+cbvatw+80HxkAIgQKABECBYCI7a9cf6PUvV97qfZQbg8yCu7GvTX6Zx9K3zo8hsxQAIgQKABECBQAIsbfh9I7679963D9l8Ix1LcOjyEzFAAiBAoAEQIFgIj5vYKS9d++dXgdIgrHUN/0UACYVwIFgAiBAkCEHgrD6Gz99+t/+7Sp99zZusmztnfjudj1J+6Pjmlw9Riqe9HWpzUQNlX//Ws9wN5BMxQAIgQKABECBYAIPRSGMXAPZfXGalMf++kfmvrorXaAa/vac69LezYuUP/p3Z809b575+zwqqefnfXBFk7tmdT3Y4CPnxkKABECBYAIgQJAxJwt8n6JHkrfBl5/v/qXq039i8cPNPWvnzrU1P85fG9Tn3r34w2veby85sPff3iSIfanHlN6KH3RQwFgXggUACIECgAReigMo14qa8rXIVou+0j27m8/MPs/vdXUa19pH6/P3+w1545jqm8d9LTMUACIECgARAgUACLmt4dS+Q1932pPZZfX6x869lBTv3zm5fbP/+yPTb2nXNvr5dPt8xeC08++bX0Ln6nwEQEgQqAAECFQAIhYnB6Kezn0rb4fU97zcODyZ039yHufNPWdsu/k/M9/sOE1bjx4T35gPVmcb4vZ1MF3mhkKABECBYAIgQJAxOKsitb/0tuDjIK7WS31V6b755/73QdNvf/aza2f/9sPNvxv7/3qR8ERdcjpZ9/0UACYFwIFgAiBAkDE4vRQRGffBr4O0foOb2WyvnfO732yGfdD6ZseCgDzQqAAECFQAIhYnB6K9d++TXn999qFa039+z9fberflOev7Wt7Jm+dvbZUfau85gNPPTDe4HrlGOqbHgoA80KgABAhUACI0EOhD1Ne/73vq/c19fee+HZT3/pH21O5fd9KU//4oW9ueM0r5TXnjmOob3Uv13qpp7B1ygwFgAiBAkCEQAEgQg+FPkz7N/SH2vL+77b3P7my75Et/+/3f2fj/VKuHNrkifOsHlMd7IPgS+r7MYVvezMUACIECgARAgWACD0U+jDl9feVA+2+kou//GFbj/OaE4xnJtXTUT2UvuihADCrBAoAEQIFgIjF6aHU6KzXtanXvWG66r//ANchYofqt8ftQUbB3QzQ0zJDASBCoAAQIVAAiFicHkpdg/cb+r7UnskAv6Fnh5yO9m3U/VHqMVZ7YKs7/5M+EgBECBQAIgQKABGLuzKth9I3PZT+uT5e3z4r9b9LvQt778xQAIgQKABECBQAIhZ3Zbqu/7oOUV/qb+jpjx5K3wY4hsxQAIgQKABECBQAIvRQ6JN9Qf1zDFGYoQAQIVAAiBAoAEToodAnPZT+OYYozFAAiBAoAEQIFAAiltfX17d1Vfzl5XpTdgAWxXaiwgwFgAiBAkCEQAEgQqAAECFQAIgQKABECBQAIgQKABECBYAIgQJAhEABIGLs+6EcOnSoqY8cOdLUFy5caOrjx4839blz55q6Xidmz5426/bubW++8Oyzzzb12bNnR4wYgN1khgJAhEABIEKgABAxdg/ltddea+qDBw829dtvv93UL730UlMfO3asqVdWVpr6wIEDTV3vx/LYY4819Z077U3Iz58/v8moAdgtZigARAgUACIECgARY/dQas+i7gN58cUXm/rNN99s6hMnTjT15cuXt/x7hw8fbuozZ8409dGjR5taDwVgusxQAIgQKABECBQAIsbuodRra9Uex0cffdTUTz/9dFPXfSv1WmBra2tNfenSpaZ+8sknm/rKlStbjrf2bKr333+/qTf2YPaX+oktX2+UlaWvNfU9Sw9O9Ho3l/7a1LeWLk70egA7ZYYCQIRAASBCoAAQsbxeb0RytyeWa2m98MILTf3MM8809enTp5v61Vdfbeo33nijqW/cuNHUzz//fFN/+OGHTf3KK6809euvv97Uq6urTV33zVQnT55s6lOnTpVnPFfqc0uTqB2Txyd6taWlT5ZONvXfl+r4Aca3nagwQwEgQqAAECFQAIgYu4cya/RQAManhwLA1AgUACIECgARY1/La9a88847Wz5+8eKoa199Wl9xovHcHvHqO3XTtbuAgZmhABAhUACIECgARCzMPhQAxmcfCgBTI1AAiBAoAEQIFAAiBAoAEQIFgAiBAkDEtq/ltc3tKgAsKDMUACIECgARAgWACIECQIRAASBCoAAQIVAAiBAoAEQIFAAi/gdKIrluU2Fc9gAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(5, 5))\n",
    "plt.imshow(state)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.animation import FuncAnimation\n",
    "from IPython.display import HTML\n",
    "from pygame import display\n",
    "\n",
    "\n",
    "# Set game resolution\n",
    "WIN_SIZE = (1000, 1000)\n",
    "display.init()\n",
    "\n",
    "frames = []\n",
    "env = gym.make(\"CarRacing-v2\", continuous=False, render_mode=\"human\")\n",
    "env.reset()\n",
    "display.set_mode(WIN_SIZE)\n",
    "\n",
    "\n",
    "for i in range(100):\n",
    "    env.render()\n",
    "    s, r, terminated, truncated, info = env.step(3)  # 0-th action is no_op action\n",
    "    frames.append(s)\n",
    "\n",
    "# # Create animation\n",
    "# fig = plt.figure(figsize=(5, 5))\n",
    "# plt.axis('off')\n",
    "# im = plt.imshow(frames[0])\n",
    "# def animate(i):\n",
    "#     img = frames[i]\n",
    "#     im.set_array(img)\n",
    "#     return im,\n",
    "# anim = FuncAnimation(fig, animate, frames=len(frames))\n",
    "# HTML(anim.to_jshtml())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
