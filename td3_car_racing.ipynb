{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TD3 on Car Racing V2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains all the code required to train a Twin Delayed Deep Deterministic Policy Gradient (TD3) algorithm on the CarRacing-v2 Box2D environment. The TD3 algorithm is an extension of the Deep Deterministic Policy Gradient (DDPG) method, designed to address the overestimation bias in Q-learning, which can lead to more stable training and improved performance. To further ensure convergence we conduct hyperparameter tuning using the syne-tune package.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # General\n",
    "import platform\n",
    "\n",
    "assert platform.python_version() == \"3.10.14\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convergence and stability are common challenges in reinforcement learning. To address this, we employ the Syne Tune package for hyperparameter tuning. Syne Tune is an efficient and flexible tool for automatic hyperparameter optimization, which helps us identify the optimal set of parameters for our TD3 algorithm on the CarRacing-v2 environment. Due to the limited scope of the university project we choose not to include other optimizations such as tweaking the reward function. \n",
    "\n",
    "Specifically, we utilize the Asynchronous Successive Halving Algorithm (ASHA), which is designed to allocate resources to promising configurations early and terminate underperforming ones, thereby speeding up the hyperparameter search process. By fine-tuning these hyperparameters, we aim to achieve robust and consistent training results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/adavidho/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from syne_tune import Tuner\n",
    "from syne_tune.backend import PythonBackend\n",
    "from syne_tune.experiments import load_experiment\n",
    "from syne_tune.config_space import loguniform, uniform\n",
    "from syne_tune.optimizer.baselines import ASHA\n",
    "from syne_tune.stopping_criterion import StoppingCriterion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we have to define the search space. Here we choose the learning rate, the Polyak update coefficient ($\\tau$) and the discount factor ($\\gamma$) to be the most important hyperparameters to ensure stable training. The ranges are set based on intuition and defaults provided by the stable_baselines3 package.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the hyperparameter search space\n",
    "config_space = {\n",
    "    \"learning_rate\": loguniform(1e-8, 0.1),\n",
    "    \"tau\": loguniform(1e-8, 1),\n",
    "    \"gamma\": uniform(0.9, 0.999),\n",
    "    \"steps\": 100000,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we define the tuning function `train_hpo_model()` which will be called inside worker processes spawned by the Syne-Tune scheduler. As `train_hpo_model()` will run inside its on process we need to define all relevant imports inside this function.\n",
    "\n",
    "The Syne-Tune scheduler will then pass a sampled hyperparameter configuration to the function, which will create an instance of a stable-baselines3 TD3 model and start training it with a reporter callback which communicates with the main tuning process. Note that we use the 'CnnPolicy' as the observed states are images of the environment and that we add noise to actions to facilitate exploration during training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the tuning function\n",
    "def train_hpo_model(\n",
    "    learning_rate: float, tau: float, gamma: float, steps: int\n",
    ") -> None:\n",
    "    # Worker imports\n",
    "    import numpy as np\n",
    "    from stable_baselines3.common.env_util import make_vec_env\n",
    "    from stable_baselines3.common.callbacks import BaseCallback\n",
    "    from stable_baselines3.common.noise import NormalActionNoise\n",
    "    from stable_baselines3 import TD3\n",
    "\n",
    "    from syne_tune import Reporter\n",
    "\n",
    "    # Create the vectorized environment\n",
    "    env_id = \"CarRacing-v2\"\n",
    "    vec_env = make_vec_env(env_id, n_envs=4)\n",
    "\n",
    "    # Initialize the PPO agent with the given hyperparameters\n",
    "    n_actions = vec_env.action_space.shape[-1]\n",
    "    action_noise = NormalActionNoise(\n",
    "        mean=np.zeros(n_actions), sigma=0.1 * np.ones(n_actions)\n",
    "    )\n",
    "    model = TD3(\n",
    "        \"CnnPolicy\",\n",
    "        vec_env,\n",
    "        action_noise=action_noise,\n",
    "        learning_rate=learning_rate,\n",
    "        tau=tau,\n",
    "        gamma=gamma,\n",
    "        batch_size=32,\n",
    "        verbose=1,\n",
    "    )\n",
    "\n",
    "    report = Reporter()\n",
    "\n",
    "    class WorkerCallback(BaseCallback):\n",
    "        def _on_step(self) -> bool:\n",
    "            # Log the mean reward\n",
    "            mean_reward = sum(self.locals[\"rewards\"]) / len(\n",
    "                self.locals[\"rewards\"]\n",
    "            )\n",
    "            step = self.locals[\"num_collected_steps\"]\n",
    "            report(step=step, mean_reward=mean_reward)\n",
    "            return True\n",
    "\n",
    "    # Train the agent\n",
    "    worker_callback = WorkerCallback()\n",
    "    model.learn(total_timesteps=steps, callback=worker_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = \"mean_reward\"\n",
    "scheduler = ASHA(\n",
    "    config_space,\n",
    "    metric=metric,\n",
    "    max_resource_attr=\"steps\",\n",
    "    resource_attr=\"step\",\n",
    "    mode=\"max\",\n",
    ")\n",
    "trial_backend = PythonBackend(\n",
    "    tune_function=train_hpo_model, config_space=config_space\n",
    ")\n",
    "stop_criterion = StoppingCriterion(\n",
    "    max_wallclock_time=61200,\n",
    ")\n",
    "tuner = Tuner(\n",
    "    trial_backend=trial_backend,\n",
    "    scheduler=scheduler,\n",
    "    stop_criterion=stop_criterion,\n",
    "    n_workers=8,\n",
    "    save_tuner=False,\n",
    "    wait_trial_completion_when_stopping=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above cell all necessary tuning objects are created and tuning is started in the cell below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "Resource summary (last result is reported):\n",
      " trial_id     status  iter  learning_rate          tau        gamma      steps  step  mean_reward worker-time\n",
      "        0  Completed   722    6.24041e-06  6.47157e-03  1.24004e-01    3613.77     1      -12.272    7227.53\n",
      "        1    Stopped  1414    3.56044e-07  4.32651e-03  8.32033e-01    5717.71     1       27.673    11435.43\n",
      "        2    Stopped   835    6.50236e-06  5.85811e-03  8.92359e-01     4393.6     1        9.095    8787.20\n",
      "        3    Stopped  2467    7.73109e-06  6.45598e-03  6.62475e-01    2084.03     1       -3.816    4168.07\n",
      "        4  Completed   862    4.02646e-06  5.81391e-03  3.68519e-01   10883.68     1       34.372    21767.37\n",
      "        5    Stopped  1325    5.97696e-06  4.04024e-03  4.86856e-01    4499.14     1      -19.657    8998.27\n",
      "        6  Completed  1821    9.98881e-06  9.98381e-03  8.54684e-01    6858.61     1       45.120    13717.22\n",
      "        7  Completed  1427    2.78481e-06  8.89039e-03  4.18345e-01    2884.93     1      -19.307    5769.86\n",
      "        8    Stopped  1817    4.73605e-06  3.67964e-03  4.64164e-01   12049.32     1       11.912    24098.64\n",
      "        9    Stopped  1091    5.28496e-07  1.66823e-03  1.26986e-01    2773.15     1      -23.989    5546.30\n",
      "       10    Stopped  2188    4.87616e-06  5.50376e-04  2.44002e-01   10578.73     1       -1.707    21157.46\n",
      "       11  Completed  2341    8.83730e-06  4.98778e-04  3.30698e-01    6791.18     1      -28.360    13582.37\n",
      "       12    Stopped  2044    1.22936e-06  3.67262e-03  9.75777e-01     837.31     1      -62.583    1674.62\n",
      "       13    Stopped  1316    6.29075e-07  5.69225e-04  2.49476e-01    2805.57     1       -3.496    5611.14\n",
      "       14    Stopped  1365    3.95296e-06  2.18375e-03  4.77990e-01    7439.09     1       -2.113    14878.19\n",
      "       15  Completed   973    8.10159e-06  1.44072e-03  9.95417e-01      805.8     1       12.824    1611.61\n",
      "       16  Completed   157    5.68705e-06  3.62683e-03  8.97050e-01    7237.77     1       16.703    14475.55\n",
      "       17    Stopped  1604    4.90286e-06  7.05757e-03  7.56047e-01   10804.81     1      -13.266    21609.62\n",
      "       18  Completed   898    1.17702e-06  7.98079e-03  8.52879e-01    4430.82     1      -58.269    8861.63\n",
      "       19    Stopped  1912    5.60174e-07  5.02038e-03  1.69449e-01   12145.83     1       44.858    24291.66\n",
      "       20    Stopped  2225    5.99133e-06  1.16868e-03  1.19305e-01    7280.14     1       22.413    14560.28\n",
      "       21    Stopped  2050    8.98671e-06  2.96221e-03  4.46012e-01   11073.59     1      -41.339    22147.19\n",
      "       22  Completed  2443    9.03882e-06  6.60901e-03  7.70986e-01     7309.4     1       48.967    14618.80\n",
      "       23    Stopped   930    2.92222e-06  2.40113e-03  7.53456e-01    7023.36     1       56.578    14046.72\n",
      "       24  Completed  2315    1.14592e-06  9.36184e-03  3.23604e-01     539.66     1        7.392    1079.33\n",
      "       25  Completed   386    5.79498e-06  8.53660e-03  4.63410e-01    5108.61     1      -12.577    10217.21\n",
      "       26    Stopped  1184    2.88652e-06  3.93405e-03  6.43194e-01    4239.29     1       13.501    8478.59\n",
      "       27  Completed  2295    5.43618e-06  8.38760e-03  7.29776e-01   11011.15     1        8.937    22022.30\n",
      "       28  Completed  1871    7.21641e-06  8.44071e-03  8.15263e-01    5845.21     1      -40.691    11690.41\n",
      "       29  Completed  1693    5.66721e-05  7.25243e-06  9.75882e-01    7622.53     1       70.810    15245.06\n",
      "       30    Stopped  2146    1.35898e-06  1.09638e-03  7.01068e-01     1058.6     1       23.348    2117.20\n",
      "       31  Completed  1716    7.68527e-06  3.65469e-03  5.54270e-01    6265.96     1       31.636    12531.93\n",
      "       32  Completed  1381    7.75148e-06  3.35904e-03  9.57527e-01    3317.16     1       48.246    6634.31\n",
      "       33  Completed  2153    7.71645e-06  9.61875e-03  9.89196e-01     944.05     1      -15.844    1888.09\n",
      "       34    Stopped  2163    8.64061e-06  6.59363e-04  6.83768e-01    3818.51     1      -34.560    7637.02\n",
      "       35  Completed  2135    7.82837e-06  6.68097e-03  2.52126e-01   10362.55     1       -1.491    20725.10\n",
      "       36    Stopped   660    9.90288e-06  5.08074e-03  9.99809e-01    7922.68     1      -23.132    15845.35\n",
      "       37  Completed   168    7.30991e-06  8.27546e-03  8.37721e-01    8478.52     1       35.710    16957.04\n",
      "       38  Completed  1044    1.95589e-06  4.35802e-03  4.35839e-01    8305.24     1       14.938    16610.49\n",
      "       39  Completed   166    6.57174e-06  7.92010e-03  6.08174e-01    6015.98     1      -17.968    12031.97\n",
      "       40  Completed    93    2.29203e-06  4.88890e-03  4.47251e-01    8248.37     1       -8.484    16496.73\n",
      "       41    Stopped   946    1.36808e-06  9.65646e-03  9.37398e-01    9509.53     1       10.868    19019.06\n",
      "       42    Stopped  1178    2.23001e-06  6.69421e-03  2.59999e-01    6508.06     1      -13.456    13016.12\n",
      "       43  Completed   376    8.82731e-06  9.01022e-06  5.47279e-01   10397.43     1        1.803    20794.85\n",
      "       44    Stopped   765    5.05626e-06  8.31930e-03  7.05706e-01    4762.76     1      -36.976    9525.52\n",
      "       45  Completed   389    5.42170e-06  2.65336e-03  3.98716e-01   11037.47     1       -0.312    22074.94\n",
      "       46    Stopped   798    3.78248e-06  9.79895e-03  8.66797e-01    4290.21     1       -3.765    8580.43\n",
      "       47    Stopped  1280    1.69824e-06  6.03534e-03  9.15037e-01    5898.94     1        0.646    11797.87\n",
      "       48    Stopped  1106    7.06350e-06  5.90139e-03  7.15573e-01    8641.07     1      -10.870    17282.15\n",
      "       49  Completed   185    8.28315e-06  7.96023e-03  3.91741e-01    5907.87     1       -8.686    11815.75\n",
      "       50  Completed  1078    6.66377e-06  8.82797e-03  1.50330e-01   12174.83     1       23.607    24349.66\n",
      "       51  Completed   721    7.64305e-06  4.10406e-03  3.49520e-01    7794.82     1      -29.200    15589.65\n",
      "       52  Completed   658    2.39349e-08  7.60775e-03  6.29480e-01   10052.34     1      -18.558    20104.69\n",
      "       53    Stopped  2038    7.10998e-06  6.47657e-03  4.83918e-01    3907.78     1       14.892    7815.56\n",
      "       54  Completed  2451    4.46613e-06  7.42828e-03  2.54152e-01   12052.21     1        2.208    24104.41\n",
      "       55  Completed  1154    1.02517e-06  9.33944e-03  7.58541e-01    8633.65     1      -19.547    17267.29\n",
      "       56  Completed  2497    5.98805e-06  4.20944e-03  9.18328e-01    10327.7     1       -4.255    20655.41\n",
      "       57    Stopped   660    5.18629e-06  4.66962e-03  3.87648e-01    1851.78     1      -14.722    3703.57\n",
      "       58    Stopped  1880    7.82141e-06  2.62296e-03  8.17927e-01   11797.07     1       53.178    23594.13\n",
      "       59  Completed  2205    2.19620e-06  7.29238e-03  5.36631e-01    7982.04     1       34.374    15964.07\n",
      "       60    Stopped   195    3.48713e-06  9.05772e-03  2.80232e-01    5639.68     1       16.890    11279.37\n",
      "       61    Stopped   498    1.13174e-06  3.81085e-03  3.45347e-01     1604.2     1       31.666    3208.41\n",
      "       62  Completed   963    3.83988e-06  1.86261e-03  8.87258e-01    5884.16     1       -0.899    11768.32\n",
      "       63    Stopped  1316    2.14188e-07  9.11084e-04  1.82960e-01    2043.03     1        7.876    4086.06\n",
      "       64  Completed  2127    6.82210e-06  9.63086e-03  7.14384e-01    4034.53     1      -19.230    8069.07\n",
      "       65    Stopped  1332    5.23143e-06  1.89841e-03  9.75487e-01    6617.07     1       24.431    13234.15\n",
      "       66  Completed  1995    3.22210e-06  1.47419e-03  8.04626e-01     8106.5     1        5.870    16213.00\n",
      "       67  Completed   260    1.05623e-06  7.57757e-03  7.34053e-01   10119.99     1       25.544    20239.98\n",
      "       68  Completed    75    7.30907e-08  5.92081e-03  9.39390e-01    6627.35     1      -24.581    13254.70\n",
      "       69    Stopped   315    7.54915e-06  2.66015e-03  3.40738e-01    6164.66     1      -56.964    12329.32\n",
      "       70    Stopped  1577    5.25673e-06  6.43938e-03  5.62476e-01   10538.51     1       47.905    21077.02\n",
      "       71    Stopped   184    2.67247e-06  1.51880e-03  6.64949e-01    9272.16     1        4.506    18544.32\n",
      "       72    Stopped  2041    4.10355e-06  8.97587e-03  8.30404e-01     6628.0     1      -21.178    13255.99\n",
      "       73  Completed  2154    3.29460e-06  5.22712e-03  2.91164e-01    4342.37     1       21.778    8684.73\n",
      "       74    Stopped   716    3.75059e-06  4.12764e-03  1.72498e-01    8402.39     1        7.212    16804.78\n",
      "       75    Stopped  2075    8.81555e-06  2.83335e-03  5.64329e-01    6416.72     1      -16.633    12833.44\n",
      "       76    Stopped  2187    8.10893e-06  9.45539e-03  6.72852e-01    1539.49     1        1.953    3078.98\n",
      "       77  Completed   196    7.62780e-06  3.37322e-03  7.86878e-01    1497.01     1      -16.263    2994.03\n",
      "       78  Completed  1040    8.00630e-06  5.23254e-03  3.27868e-01    9344.18     1      -28.163    18688.36\n",
      "       79    Stopped  2010    4.38024e-06  4.60631e-03  4.04183e-01   11939.77     1        0.257    23879.53\n",
      "       80    Stopped   891    4.76952e-07  5.44874e-03  3.85446e-01    9930.02     1      -15.789    19860.05\n",
      "       81  Completed  1123    2.94033e-06  1.59129e-03  9.54175e-01     351.89     1       20.855     703.77\n",
      "       82  Completed  1135    2.67643e-06  7.20623e-03  1.96423e-01    6020.36     1       -6.465    12040.73\n",
      "       83    Stopped   126    4.72605e-06  1.84490e-03  5.92360e-01    5978.89     1      -11.149    11957.78\n",
      "       84    Stopped  2169    4.93882e-07  6.65270e-04  3.57140e-01    11300.1     1       21.021    22600.19\n",
      "       85    Stopped   728    9.51760e-06  4.03714e-03  9.84795e-01    3660.87     1      -47.157    7321.73\n",
      "       86    Stopped   617    8.22877e-06  3.51245e-03  8.47793e-01    4480.94     1      -47.326    8961.88\n",
      "       87  Completed  1210    4.19116e-06  3.96142e-03  2.81338e-01    1477.53     1      -10.139    2955.07\n",
      "       88    Stopped  2062    2.27542e-06  7.68064e-03  2.56151e-01    7451.87     1      -32.665    14903.74\n",
      "       89  Completed  1919    5.96660e-06  3.66109e-03  2.78094e-01    1842.45     1        4.508    3684.90\n",
      "       90    Stopped  1325    4.07436e-06  3.32888e-03  4.00887e-01    4826.87     1       24.426    9653.74\n",
      "       91    Stopped  1153    2.67457e-06  6.89339e-03  1.89058e-01    3939.67     1        1.087    7879.35\n",
      "       92    Stopped   599    6.03634e-06  3.12219e-04  7.15713e-01    6560.01     1       33.254    13120.01\n",
      "       93  Completed   452    3.08889e-06  6.50895e-03  5.14751e-01     9804.4     1       32.151    19608.79\n",
      "       94    Stopped  1570    4.55246e-06  7.81224e-03  6.58062e-01    2115.27     1      -21.213    4230.53\n",
      "       95  Completed  1180    1.81890e-06  4.94529e-03  7.84817e-01     3712.9     1      -38.903    7425.80\n",
      "       96    Stopped  1126    7.47998e-06  9.45431e-03  9.53332e-01   10536.59     1       38.820    21073.18\n",
      "       97    Stopped   162    1.24253e-06  5.40505e-03  9.86645e-01    8839.39     1       19.633    17678.78\n",
      "       98  Completed  2113    2.23151e-06  1.77526e-03  7.17017e-01     2022.3     1       31.130    4044.59\n",
      "       99  Completed  1808    4.22603e-06  3.74989e-03  8.04304e-01    9527.22     1        5.092    19054.44\n",
      "      100    Stopped  1904    4.33020e-06  7.24163e-03  1.72503e-01    1493.17     1      -11.636    2986.33\n",
      "      101  Completed  2358    9.33789e-07  8.47270e-03  5.95539e-01    5169.52     1       59.163    10339.05\n",
      "      102    Stopped   404    7.97700e-06  2.26759e-03  1.29408e-01     8415.5     1       -6.721    16831.00\n",
      "      103  Completed  1336    8.57787e-06  7.76135e-03  3.13973e-01    6792.74     1       32.334    13585.48\n",
      "      104    Stopped  1243    9.42773e-06  9.29325e-03  2.18457e-01   11506.17     1       24.271    23012.35\n",
      "      105  Completed   663    9.14366e-06  8.68843e-03  4.57212e-01    8586.78     1       25.088    17173.55\n",
      "      106  Completed    95    3.91180e-07  5.42608e-03  2.60070e-01   10912.03     1      -26.770    21824.07\n",
      "      107    Stopped   538    2.70731e-06  3.05229e-03  7.76281e-01    8590.68     1        9.175    17181.36\n",
      "      108    Stopped  2165    7.63232e-06  8.94317e-03  2.07004e-01    1520.98     1        6.209    3041.96\n",
      "      109    Stopped  1502    7.92494e-06  3.29496e-03  3.07185e-01    6788.46     1      -59.335    13576.92\n",
      "      110  Completed  2095    7.34799e-06  9.65943e-04  4.64656e-01     7640.8     1      -69.054    15281.60\n",
      "      111  Completed  1404    1.53823e-06  5.88221e-03  1.67584e-01    8077.51     1      -57.966    16155.02\n",
      "      112  Completed   277    9.01184e-06  3.35425e-03  2.97300e-01    9022.94     1       12.157    18045.88\n",
      "      113  Completed  1349    8.62970e-06  2.81547e-03  8.62521e-01    1758.27     1       -1.799    3516.54\n",
      "      114  Completed   250    2.61279e-06  4.12311e-03  7.87423e-01   11208.59     1      -30.630    22417.17\n",
      "      115    Stopped  2067    6.19471e-07  5.67365e-03  9.47399e-01    9807.02     1        0.503    19614.04\n",
      "      116    Stopped   268    1.85809e-06  7.19060e-03  3.67389e-01    7242.07     1      -23.598    14484.14\n",
      "      117    Stopped   995    2.35152e-07  9.22740e-03  1.37424e-01    5353.04     1      -24.070    10706.09\n",
      "      118    Stopped   540    6.91571e-06  7.50103e-05  3.79314e-01     632.35     1       60.324    1264.71\n",
      "      119  Completed   697    9.18093e-07  6.73316e-03  6.22306e-01     8832.2     1       50.370    17664.41\n",
      "      120    Stopped   431    9.54817e-06  3.05182e-03  5.73283e-01    6912.94     1      -31.778    13825.88\n",
      "      121  Completed    30    1.67312e-06  2.44387e-03  5.00613e-01    8868.15     1      -29.782    17736.29\n",
      "      122    Stopped  1702    9.12985e-07  1.41097e-03  1.80224e-01   10621.53     1       53.257    21243.07\n",
      "      123  Completed   160    8.32075e-06  1.32838e-03  8.53003e-01    7448.78     1      -10.373    14897.55\n",
      "      124  Completed  2409    7.86685e-06  8.20195e-03  5.47356e-01    6301.12     1      -26.833    12602.24\n",
      "      125    Stopped  1700    3.67572e-07  6.06962e-03  6.57033e-01    1895.71     1       23.384    3791.43\n",
      "      126  Completed  1480    9.68481e-06  8.86388e-03  6.47007e-01     2074.9     1       -0.322    4149.79\n",
      "      127    Stopped   743    7.90591e-06  4.47473e-03  1.70229e-01    3718.93     1      -32.801    7437.86\n",
      "      128    Stopped  2276    8.13653e-06  5.08668e-03  2.76730e-01    4813.69     1        7.853    9627.38\n",
      "      129  Completed  1420    9.16193e-06  2.18278e-03  6.57766e-01    4100.22     1      -17.723    8200.43\n",
      "      130    Stopped   755    2.97659e-06  3.76758e-03  1.86631e-01    4524.26     1       12.988    9048.51\n",
      "      131  Completed  1607    9.13485e-06  8.48091e-04  3.63070e-01    1066.48     1       10.488    2132.95\n",
      "      132    Stopped   232    8.63857e-06  5.80999e-03  9.14507e-01    3677.14     1      -12.312    7354.28\n",
      "      133    Stopped   905    8.29899e-07  7.58721e-03  8.80749e-01   10516.23     1      -12.498    21032.45\n",
      "      134    Stopped   561    2.44171e-06  2.60526e-03  3.71141e-01   11759.55     1      -40.719    23519.11\n",
      "      135  Completed  1722    5.35142e-06  1.10330e-03  7.15397e-01    6568.47     1       21.446    13136.93\n",
      "      136  Completed  2446    4.42568e-06  3.34307e-03  2.74323e-01    1220.11     1       22.156    2440.21\n",
      "      137    Stopped  1157    1.98385e-06  8.71310e-03  5.35181e-01    8407.72     1      -22.296    16815.44\n",
      "      138  Completed   583    3.37668e-06  2.83202e-03  4.08472e-01    5991.31     1       -4.887    11982.61\n",
      "      139  Completed   577    6.97295e-06  5.17885e-03  5.68773e-01    2264.31     1        1.358    4528.61\n",
      "      140  Completed  1969    9.50635e-06  3.64028e-03  5.24552e-01    6488.16     1      -53.790    12976.31\n",
      "      141    Stopped   538    8.29141e-07  8.67879e-03  4.05277e-01   12204.47     1      -30.910    24408.94\n",
      "      142    Stopped   556    6.72550e-06  5.08169e-03  1.00243e-01    1956.11     1       -2.933    3912.23\n",
      "      143    Stopped  1261    2.52661e-06  5.18996e-03  9.36971e-01     4429.1     1      -36.563    8858.19\n",
      "      144  Completed  1353    2.73115e-06  4.21235e-03  3.97836e-01   11946.29     1      -10.306    23892.58\n",
      "      145    Stopped   898    1.03911e-07  2.02930e-03  8.77179e-01   10635.03     1       -2.554    21270.06\n",
      "      146  Completed  1138    9.95287e-06  9.62555e-03  2.42661e-01    8129.26     1       36.389    16258.52\n",
      "      147  Completed   449    6.44338e-06  2.83240e-03  5.28439e-01    6529.82     1       -6.671    13059.64\n",
      "      148    Stopped  1290    6.62039e-06  4.86297e-03  8.21816e-01   11318.79     1      -49.283    22637.59\n",
      "      149    Stopped   434    6.73864e-06  9.19746e-03  5.04882e-01    9920.36     1        7.589    19840.72\n",
      "      150    Stopped  1835    8.90677e-06  6.75702e-03  8.20689e-01    8905.62     1      -36.099    17811.25\n",
      "      151  Completed  1895    2.02345e-06  3.79221e-03  5.74107e-01    6843.13     1       11.417    13686.25\n",
      "      152    Stopped  1849    1.18913e-06  7.45149e-03  9.98928e-01    5812.93     1      -11.774    11625.87\n",
      "      153    Stopped  2091    5.60829e-06  6.22188e-03  6.41073e-01    4032.08     1       26.543    8064.17\n",
      "      154    Stopped   915    7.52932e-06  3.32213e-03  1.50135e-01    1642.55     1      -40.153    3285.10\n",
      "      155    Stopped  1336    2.33540e-06  8.20847e-03  2.00541e-01    6224.03     1       47.961    12448.07\n",
      "      156  Completed  1026    6.50126e-07  8.76300e-04  6.01368e-01    4530.41     1       -1.129    9060.83\n",
      "      157    Stopped   867    7.29355e-06  8.51265e-04  1.31333e-01    3423.39     1      -21.167    6846.78\n",
      "      158    Stopped  1949    8.71928e-06  6.76137e-03  7.75058e-01    1517.84     1       -0.529    3035.67\n",
      "      159    Stopped  2047    7.79838e-06  2.85620e-03  3.16363e-01    8018.41     1      -44.135    16036.82\n",
      "      160    Stopped  1490    2.10331e-06  1.41195e-03  4.41081e-01    7140.23     1      -38.929    14280.45\n",
      "      161    Stopped    79    9.26865e-06  4.84408e-03  8.02573e-01   12130.52     1      -26.700    24261.03\n",
      "      162  Completed  1665    1.09926e-06  5.46290e-03  8.64458e-01    2366.64     1        6.159    4733.29\n",
      "      163    Stopped  1692    7.21963e-07  5.90912e-03  3.63093e-01    9258.24     1        5.433    18516.47\n",
      "      164  Completed  2251    9.09227e-06  7.16805e-03  7.24615e-01    10277.1     1        4.377    20554.20\n",
      "      165    Stopped  1726    2.07366e-06  5.33543e-05  1.21956e-01    7923.33     1      -38.509    15846.65\n",
      "      166    Stopped  1666    8.87345e-06  7.69484e-03  5.21077e-01    1379.32     1      -29.065    2758.64\n",
      "      167  Completed  2304    3.11492e-06  6.99606e-03  8.98044e-01    5558.43     1      -41.209    11116.87\n",
      "      168    Stopped  2047    1.75371e-06  1.69017e-03  9.45635e-01   10063.77     1        9.140    20127.55\n",
      "      169  Completed  1831    4.14023e-06  6.25268e-03  5.51446e-01     4019.8     1      -59.168    8039.61\n",
      "      170    Stopped   771    5.48606e-06  9.85191e-03  7.32674e-01    1245.04     1      -20.297    2490.08\n",
      "      171    Stopped  1133    7.41520e-06  2.36396e-03  6.08717e-01     670.34     1       -6.609    1340.67\n",
      "      172    Stopped  2052    1.04732e-06  1.41839e-03  7.59304e-01    5709.18     1        4.479    11418.37\n",
      "      173  Completed  2051    8.58197e-06  6.44530e-03  8.00382e-01    6193.61     1        0.169    12387.23\n",
      "      174  Completed  1580    8.09552e-06  4.45908e-03  8.91297e-01    9885.45     1      -27.128    19770.91\n",
      "      175    Stopped  1712    2.04756e-06  2.10894e-03  4.68353e-01    3641.48     1        8.063    7282.96\n",
      "      176  Completed  1874    4.76007e-06  1.08988e-03  8.35472e-01    6764.68     1       18.389    13529.37\n",
      "      177    Stopped  2270    2.32294e-06  2.60994e-03  9.11894e-01    2793.66     1      -24.174    5587.33\n",
      "      178    Stopped   552    4.54968e-06  1.54174e-03  2.41972e-01   12195.15     1      -30.990    24390.31\n",
      "      179    Stopped  1588    2.01095e-06  6.59825e-03  4.03878e-01    2404.04     1       31.872    4808.08\n",
      "      180  Completed  2312    7.70748e-06  9.68286e-03  7.47744e-01     9522.3     1        5.010    19044.59\n",
      "      181    Stopped  2323    5.03721e-06  8.74130e-03  2.13264e-01    2657.35     1       52.390    5314.70\n",
      "      182  Completed  1356    8.44378e-06  4.73303e-03  1.64795e-01   11043.93     1      -18.557    22087.87\n",
      "      183    Stopped  1750    9.32770e-06  9.48609e-04  6.99474e-01    1617.74     1       -4.179    3235.48\n",
      "      184    Stopped   537    6.71667e-06  6.57741e-03  9.96689e-01     6190.9     1       52.469    12381.79\n",
      "      185  Completed   160    3.40244e-06  1.73203e-03  2.86889e-01     8609.1     1        8.020    17218.20\n",
      "      186    Stopped  1851    4.13553e-06  1.79214e-03  1.19154e-01    6175.57     1       33.922    12351.14\n",
      "      187    Stopped   467    8.31047e-06  8.59668e-03  7.57647e-01     868.64     1       19.937    1737.28\n",
      "      188  Completed  2100    9.96434e-06  8.29562e-03  3.71859e-01    4113.49     1      -10.587    8226.98\n",
      "      189    Stopped   239    1.04386e-06  5.37521e-03  6.05058e-01    6072.31     1       20.965    12144.62\n",
      "      190  Completed   246    7.27842e-06  6.24082e-03  6.87265e-01    5155.45     1      -55.962    10310.91\n",
      "      191    Stopped  1353    2.56742e-06  8.03380e-03  2.85851e-01    9498.01     1       45.168    18996.02\n",
      "      192  Completed  1990    5.23011e-06  6.11715e-03  6.12968e-01   10407.84     1      -20.927    20815.69\n",
      "      193  Completed  1698    6.63541e-06  9.41759e-03  5.57668e-01    9810.18     1       30.478    19620.35\n",
      "      194    Stopped   995    3.01474e-06  7.79134e-03  9.43152e-01     1054.4     1       -0.559    2108.81\n",
      "      195  Completed  1729    4.65000e-07  3.04267e-03  9.63281e-01    5275.54     1       -0.146    10551.09\n",
      "      196    Stopped  1714    8.40312e-06  2.69227e-03  4.25801e-01     6787.2     1        3.630    13574.39\n",
      "      197  Completed    77    5.46466e-06  4.51708e-03  7.04096e-01     9523.2     1        8.704    19046.41\n",
      "      198    Stopped  1737    2.30305e-07  4.70876e-04  2.12032e-01    3462.11     1      -15.243    6924.21\n",
      "      199    Stopped  2223    2.10443e-06  3.00367e-03  3.77241e-01    4444.19     1       21.979    8888.37\n",
      "      200  Completed  1800    2.33201e-06  2.19815e-03  7.79221e-01     471.96     1      -14.435     943.92\n",
      "      201    Stopped  2102    5.76446e-06  9.63759e-04  9.98875e-01    5821.15     1       22.137    11642.30\n",
      "      202    Stopped  1267    6.37254e-06  8.99614e-03  6.97910e-01   10943.03     1       23.410    21886.07\n",
      "      203  Completed   759    2.84662e-06  4.63911e-03  4.63261e-01   10899.55     1      -45.987    21799.09\n",
      "      204    Stopped   681    9.54177e-06  1.45141e-03  7.08519e-01   12020.96     1       -6.416    24041.92\n",
      "      205    Stopped  1632    9.48011e-06  7.24315e-03  3.71475e-01    2775.44     1        3.884    5550.88\n",
      "      206    Stopped   302    8.03922e-06  4.98939e-03  8.30212e-01    5644.66     1      -15.378    11289.33\n",
      "      207    Stopped   663    5.56265e-06  5.29038e-03  8.93862e-01    2146.64     1       -2.413    4293.29\n",
      "      208    Stopped    40    5.97699e-06  4.18374e-03  7.01460e-01    2245.98     1       20.961    4491.97\n",
      "      209  Completed  1461    1.84652e-07  8.96124e-03  5.65591e-01   10190.31     1      -33.199    20380.63\n",
      "      210  Completed   826    8.26749e-06  7.08389e-03  1.20048e-01    7262.39     1       13.058    14524.78\n",
      "      211  Completed   698    6.03217e-06  4.49591e-03  9.56657e-01     762.57     1        1.299    1525.13\n",
      "      212  Completed  1908    2.70019e-06  7.90573e-03  9.74306e-01    3339.52     1       24.559    6679.04\n",
      "      213  Completed   587    5.45356e-06  7.99876e-03  4.33041e-01    9100.64     1      -37.981    18201.28\n",
      "      214    Stopped   434    6.83329e-06  6.36320e-03  6.85080e-01    5341.33     1      -44.323    10682.66\n",
      "      215  Completed  1699    1.62305e-06  9.77713e-03  7.90530e-01    2790.23     1      -12.001    5580.45\n",
      "      216    Stopped  1997    5.75154e-06  6.12347e-03  6.17784e-01    2871.84     1      -16.882    5743.69\n",
      "      217  Completed  1207    3.19050e-06  7.31629e-05  6.27881e-01   10280.23     1       26.839    20560.46\n",
      "      218  Completed  1975    1.00478e-06  6.24452e-03  5.77634e-01   11794.98     1       43.307    23589.95\n",
      "      219  Completed  1804    2.17689e-06  2.75731e-03  6.21034e-01   12188.86     1       20.833    24377.72\n",
      "      220  Completed  1969    1.17498e-06  7.61240e-03  7.37391e-01    5108.66     1      -16.771    10217.31\n",
      "      221  Completed   760    2.70686e-06  8.25983e-03  8.26922e-01     5039.1     1       -3.336    10078.19\n",
      "      222    Stopped  2017    9.01870e-06  3.56148e-03  6.23819e-01    1878.82     1      -30.676    3757.65\n",
      "      223    Stopped  1327    7.98600e-07  1.25530e-03  5.93835e-01     4091.6     1       18.604    8183.20\n",
      "      224    Stopped  2059    7.39383e-06  3.74279e-03  5.97298e-01   12018.58     1       17.768    24037.17\n",
      "      225  Completed  2468    3.26998e-06  3.95117e-03  9.58208e-01   10170.85     1       20.609    20341.70\n",
      "      226    Stopped   700    8.24437e-07  3.68942e-03  1.41177e-01    4323.93     1       15.787    8647.86\n",
      "      227  Completed  2032    1.49280e-06  1.93353e-03  1.50436e-01    6279.11     1       -4.234    12558.21\n",
      "      228  Completed   607    4.82023e-06  4.21004e-03  6.97032e-01    4055.62     1       31.963    8111.24\n",
      "      229  Completed   902    9.62996e-06  8.80865e-03  8.15675e-01    7283.55     1       -0.064    14567.10\n",
      "      230  Completed   370    9.48233e-06  4.84497e-03  7.96677e-01    1268.18     1       51.130    2536.37\n",
      "      231    Stopped  2131    3.79997e-06  6.89360e-03  6.60577e-01    7626.65     1      -42.710    15253.29\n",
      "      232  Completed  1391    5.25724e-06  5.08395e-03  1.40141e-01    6792.26     1      -20.998    13584.51\n",
      "      233    Stopped  2037    3.29754e-06  3.85976e-03  7.72383e-01     4141.4     1       21.237    8282.81\n",
      "      234    Stopped    56    6.43532e-06  4.69690e-03  5.13081e-01   10127.29     1      -27.177    20254.58\n",
      "      235  Completed  2126    5.04408e-06  2.10847e-03  3.57215e-01    1417.49     1      -39.882    2834.98\n",
      "      236    Stopped   365    1.04463e-06  3.06706e-03  9.40047e-01    6627.76     1      -48.505    13255.52\n",
      "      237    Stopped  1008    9.13622e-06  8.99929e-03  1.19876e-01    7029.45     1       22.812    14058.90\n",
      "      238  Completed   634    6.91284e-06  4.08797e-03  8.13070e-01    1953.35     1       -1.039    3906.70\n",
      "      239  Completed  1637    8.89611e-06  4.31143e-03  8.20458e-01    3678.83     1       27.047    7357.66\n",
      "      240    Stopped   516    5.63956e-06  7.70190e-04  6.84840e-01    1247.84     1        2.849    2495.68\n",
      "      241    Stopped   551    6.75380e-06  6.52752e-03  1.61932e-01    3555.13     1       42.518    7110.26\n",
      "      242  Completed   880    2.24335e-06  4.36274e-03  6.01247e-01    8831.99     1       13.569    17663.97\n",
      "      243    Stopped   408    1.92191e-07  9.57210e-03  2.25929e-01    8808.45     1      -29.841    17616.90\n",
      "      244    Stopped  1025    9.76674e-06  2.95389e-03  2.96890e-01    3808.87     1      -12.482    7617.75\n",
      "      245  Completed  2372    4.95645e-06  7.82979e-03  1.00492e-01      444.1     1       -4.297     888.19\n",
      "      246    Stopped  1776    8.83781e-06  5.23053e-03  6.48848e-01   10324.32     1        7.169    20648.65\n",
      "      247  Completed   186    1.73195e-07  9.06641e-03  7.37848e-01    4405.99     1       49.723    8811.99\n",
      "      248    Stopped  1331    7.48422e-06  9.30914e-03  9.99116e-01    1653.04     1        9.853    3306.08\n",
      "      249    Stopped   732    9.48101e-06  7.61969e-03  4.00835e-01   12168.26     1      -23.073    24336.53\n",
      "      250    Stopped  1663    7.52407e-06  8.08980e-03  9.55066e-01     7681.0     1      -37.454    15362.00\n",
      "      251  Completed  1572    6.89713e-06  3.60414e-04  3.45385e-01    3873.16     1      -39.402    7746.32\n",
      "      252  Completed   773    2.43994e-06  7.18935e-03  3.89240e-01    4293.94     1      -32.272    8587.87\n",
      "      253  Completed  1754    2.66408e-06  6.39652e-03  3.36373e-01     1273.5     1      -11.409    2547.00\n",
      "      254  Completed    86    2.75389e-06  2.42563e-03  2.86085e-01    1960.69     1       31.639    3921.37\n",
      "      255  Completed  1470    6.52620e-06  5.37327e-03  7.37246e-01   10911.78     1      -18.499    21823.56\n",
      "      256  Completed  1729    9.88597e-06  7.64097e-03  2.95514e-01     993.86     1       52.853    1987.71\n",
      "      257  Completed  2155    5.11854e-06  8.97129e-04  9.90279e-01    6634.59     1       23.641    13269.19\n",
      "      258    Stopped  1093    1.84750e-06  3.20633e-03  7.23537e-01    9113.35     1        1.914    18226.70\n",
      "      259    Stopped  1277    9.64989e-06  7.04090e-04  1.10679e-01    8064.97     1        2.803    16129.94\n",
      "      260  Completed  1405    6.43605e-06  2.28653e-03  5.63854e-01    9419.86     1      -14.597    18839.73\n",
      "      261    Stopped   802    2.73871e-06  4.76174e-03  6.07723e-01    4641.83     1       26.939    9283.65\n",
      "      262    Stopped  1132    4.08687e-06  9.13043e-03  6.91392e-01    4996.89     1       31.740    9993.78\n",
      "      263  Completed  1648    9.73269e-06  2.44447e-04  9.56044e-01   11501.93     1      -37.360    23003.87\n",
      "      264    Stopped  1176    5.27858e-06  1.22741e-03  7.96423e-01    7729.06     1        8.659    15458.12\n",
      "      265    Stopped   581    2.09339e-06  2.17680e-03  8.09880e-01    3579.66     1       57.209    7159.32\n",
      "      266  Completed  1706    8.79392e-06  1.99245e-03  7.40717e-01    4995.17     1      -13.016    9990.34\n",
      "      267    Stopped   793    5.51680e-06  7.88557e-04  3.69993e-01   11777.02     1       29.768    23554.03\n",
      "      268  Completed   560    9.36047e-06  1.67362e-03  7.74424e-01     7979.2     1      -13.472    15958.39\n",
      "      269    Stopped  1392    6.97623e-07  4.15293e-03  8.68196e-01    7527.63     1      -26.319    15055.26\n",
      "      270  Completed  1336    2.13318e-06  3.43283e-03  4.22351e-01    1712.64     1       27.869    3425.28\n",
      "      271    Stopped  2051    5.23034e-06  6.46522e-03  4.62936e-01   11066.56     1        2.187    22133.13\n",
      "      272  Completed  1431    8.32430e-06  8.19872e-03  6.12486e-01     1293.9     1       24.369    2587.80\n",
      "      273    Stopped   873    1.47476e-06  8.96466e-03  7.65837e-01    4013.94     1       20.007    8027.89\n",
      "      274    Stopped  1251    1.57770e-07  8.19364e-03  2.31945e-01     4220.7     1       24.346    8441.41\n",
      "      275    Stopped  2166    6.57598e-06  1.99330e-03  8.92277e-01    2717.22     1       27.454    5434.45\n",
      "      276  Completed  2482    7.22163e-06  9.20265e-03  4.19885e-01    3645.84     1      -21.089    7291.69\n",
      "      277  Completed   438    8.61930e-06  1.27202e-03  2.51194e-01   11177.09     1       -3.053    22354.18\n",
      "      278    Stopped  1182    5.80477e-06  7.29391e-04  1.52908e-01    3415.34     1      -26.383    6830.68\n",
      "      279  Completed  1386    1.71150e-06  2.37047e-03  4.39033e-01   10023.57     1        5.978    20047.13\n",
      "      280  Completed  1803    1.74526e-06  9.37928e-03  5.90945e-01    5504.45     1      -36.494    11008.91\n",
      "      281  Completed   762    1.86790e-06  7.65049e-03  6.28219e-01    1393.12     1       -0.534    2786.25\n",
      "      282  Completed   311    9.62336e-06  1.96833e-03  7.75207e-01    4883.13     1        7.821    9766.26\n",
      "      283  Completed  1366    3.39816e-06  1.47367e-03  4.10344e-01    2803.72     1      -14.658    5607.44\n",
      "      284  Completed  2261    3.38914e-06  4.25016e-04  5.31042e-01   11160.41     1       23.966    22320.83\n",
      "      285  Completed   170    9.33084e-06  6.26418e-03  2.86034e-01     3475.4     1      -40.752    6950.80\n",
      "      286    Stopped  2141    5.48057e-07  2.18969e-03  5.96937e-01     3044.7     1       30.368    6089.41\n",
      "      287  Completed   426    1.28101e-07  5.43423e-04  8.41353e-01    3216.66     1      -23.699    6433.33\n",
      "      288  Completed  1579    3.46279e-06  8.27633e-04  1.63786e-01   11881.88     1       44.055    23763.76\n",
      "      289  Completed  1043    1.11491e-07  3.87831e-04  8.32292e-01   10720.53     1       -9.239    21441.07\n",
      "      290    Stopped  1055    1.02418e-07  7.48403e-04  7.15343e-01    2941.95     1       34.997    5883.90\n",
      "      291  Completed  1946    8.49053e-06  6.87401e-03  1.26367e-01    4907.57     1       10.784    9815.14\n",
      "      292    Stopped  1978    9.83847e-06  7.52515e-03  5.04872e-01    6985.51     1        7.904    13971.03\n",
      "0 trials running, 293 finished (293 until the end), 61207.29s wallclock-time\n",
      "\n",
      "mean_reward: best 7.686551213264465 for trial-id 30\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "# Start hyperparameter tuning\n",
    "tuner.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get results\n",
    "tuner_path = tuner.tuner_path\n",
    "tuning_experiment = load_experiment(tuner_path)\n",
    "best_run = tuning_experiment.best_config()\n",
    "tuning_experiment.results.to_csv(\"artifacts/tune_logs/tuning_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tuning jobs: 293\n",
      "Best mean reward: 70.810\n",
      "Best tuning configuration:\n",
      " - config_learning_rate: 5.6672e-05\n",
      " - config_tau: 7.2524e-06\n",
      " - config_gamma: 9.7588e-01\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of tuning jobs:\", len(tuning_experiment.results))\n",
    "print(\"Best mean reward:\", round(best_run[\"mean_reward\"], 4))\n",
    "print(\"Best tuning configuration:\")\n",
    "for i in [\"config_learning_rate\", \"config_tau\", \"config_gamma\"]:\n",
    "    print(f\" - {i}: {best_run[i]:.4e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The approximation optimal hyperparameters fund in the tuning section above will now be used for training a final TD3 model. The training itself is very similar to the behavior of `train_hpo_model()`. However, instead of the reporter callback we now employ a `StopTrainingOnRewardThreshold` callback which determines an early stopping criterion in terms of a reward threshold. This callback is embedded inside the `EvalCallback` which is used to evaluate the model and create best model checkpoints.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.noise import NormalActionNoise\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "from stable_baselines3.common.callbacks import StopTrainingOnRewardThreshold\n",
    "from stable_baselines3 import TD3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we create the environment for training which is wrapped `Monitor`. This serves the purpose of tracking the reward of environment interactions and logging it into `artifacts/train_logs/monitor.csv`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the vectorized environment\n",
    "env_id = \"CarRacing-v2\"\n",
    "log_dir = \"./artifacts/train_logs/\"\n",
    "env = gym.make(env_id, domain_randomize=False, render_mode=\"rgb_array\")\n",
    "env = Monitor(env, log_dir, allow_early_resets=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we create a new model instance with the optimal hyperparameters found in the tuning section above. The code for training (also the tuning worker above) is based on this [TD3 Pendulum-v1](https://stable-baselines3.readthedocs.io/en/master/modules/td3.html) example from the stable-baselines3 website.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n"
     ]
    }
   ],
   "source": [
    "# Initialize optimal hyperparameters\n",
    "n_actions = env.action_space.shape[-1]\n",
    "action_noise = NormalActionNoise(\n",
    "    mean=np.zeros(n_actions), sigma=0.1 * np.ones(n_actions)\n",
    ")\n",
    "learning_rate = best_run[\"config_learning_rate\"]\n",
    "tau = best_run[\"config_tau\"]\n",
    "gamma = best_run[\"config_gamma\"]\n",
    "# Create the TD3 Agent\n",
    "model = TD3(\n",
    "    \"CnnPolicy\",\n",
    "    env,\n",
    "    action_noise=action_noise,\n",
    "    learning_rate=learning_rate,\n",
    "    tau=tau,\n",
    "    gamma=gamma,\n",
    "    batch_size=32,\n",
    "    verbose=1,\n",
    "    device=\"cuda\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use DummyVecEnv to create a vectorized environment\n",
    "eval_env = Monitor(gym.make(\"CarRacing-v2\"), log_dir)\n",
    "# Define callback for early stopping and include it in the eval callback\n",
    "callback_on_best = StopTrainingOnRewardThreshold(\n",
    "    reward_threshold=500, verbose=1\n",
    ")\n",
    "best_path = \"./artifacts/model/\"\n",
    "eval_callback = EvalCallback(\n",
    "    eval_env,\n",
    "    callback_on_new_best=callback_on_best,\n",
    "    verbose=1,\n",
    "    best_model_save_path=best_path,\n",
    "    log_path=\"./artifacts/logs/\",\n",
    "    eval_freq=1000,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of time_steps used for training is mostly based on resources limits. Configurations we found in other training examples of TD3 on the CarRacing-v2 environment typically used values two or three orders of magnitude larger which is outside the scope of this university project.\n",
    "\n",
    "- [Solving CarRacing with DDPG](https://github.com/lzhan144/Solving-CarRacing-with-DDPG/blob/master/car_racing.py)\n",
    "- [Torch CarRacing TD3](https://blog.csdn.net/Scc_hy/article/details/135179576)\n",
    "- [Control CartRacing-v2 environment using DQN from scratch](https://hiddenbeginner.github.io/study-notes/contents/tutorials/2023-04-20_CartRacing-v2_DQN.html)\n",
    "\n",
    "However, we found that the agent still converges with 1M time steps (1000 episodes) and our current hyperparameter configuration within around 10 hours.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adavidho/miniconda3/envs/rl/lib/python3.10/site-packages/stable_baselines3/common/callbacks.py:414: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.vec_transpose.VecTransposeImage object at 0x7effd97c34c0> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x7f00103b3b80>\n",
      "  warnings.warn(\"Training and eval env are not of the same type\" f\"{self.training_env} != {self.eval_env}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Eval num_timesteps=1000, episode_reward=-87.25 +/- 1.140372949288352 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -87.25   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 1000     | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.3249   | \n",
      " |    critic_loss     | 1.3312   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 899      | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=2000, episode_reward=-85.61 +/- 1.5287179023466968 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -85.61   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 2000     | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.1889   | \n",
      " |    critic_loss     | 1.5472   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 1899     | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=3000, episode_reward=-83.97 +/- 1.846118793426057 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -83.97   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 3000     | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.3186   | \n",
      " |    critic_loss     | 1.3297   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 2899     | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=4000, episode_reward=-82.33 +/- 1.9946027139631994 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -82.33   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 4000     | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.0865   | \n",
      " |    critic_loss     | 1.2716   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 3899     | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=5000, episode_reward=-80.68 +/- 1.9937734936837006 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -80.68   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 5000     | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 7.9787   | \n",
      " |    critic_loss     | 1.9947   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 4899     | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=6000, episode_reward=-79.04 +/- 1.4750700818579574 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -79.04   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 6000     | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.1375   | \n",
      " |    critic_loss     | 1.5344   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 5899     | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=7000, episode_reward=-82.52 +/- 1.2231893767721158 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -82.52   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 7000     | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.5933   | \n",
      " |    critic_loss     | 1.1483   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 6899     | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=8000, episode_reward=-86.01 +/- 1.6880898364095775 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -86.01   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 8000     | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 7.6372   | \n",
      " |    critic_loss     | 1.9093   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 7899     | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=9000, episode_reward=-89.5 +/- 1.723263546944465 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -89.5    | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 9000     | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.2699   | \n",
      " |    critic_loss     | 1.0675   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 8899     | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=10000, episode_reward=-92.98 +/- 1.9605453588486674 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -92.98   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 10000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 7.6875   | \n",
      " |    critic_loss     | 1.9219   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 9899     | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=11000, episode_reward=-96.47 +/- 1.145923442992157 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -96.47   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 11000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.9306   | \n",
      " |    critic_loss     | 1.7326   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 10899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=12000, episode_reward=-92.71 +/- 1.5031338822610905 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -92.71   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 12000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.1008   | \n",
      " |    critic_loss     | 1.2752   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 11899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=13000, episode_reward=-88.96 +/- 1.8354370452945066 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -88.96   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 13000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.1607   | \n",
      " |    critic_loss     | 1.5402   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 12899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=14000, episode_reward=-85.2 +/- 1.07865785326512 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -85.2    | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 14000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 7.8072   | \n",
      " |    critic_loss     | 1.9518   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 13899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=15000, episode_reward=-81.45 +/- 1.953012562954624 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -81.45   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 15000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.3387   | \n",
      " |    critic_loss     | 1.0847   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 14899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=16000, episode_reward=-77.69 +/- 1.6563047519120024 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -77.69   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 16000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.3612   | \n",
      " |    critic_loss     | 1.5903   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 15899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=17000, episode_reward=-77.34 +/- 1.724930998836977 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -77.34   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 17000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.1032   | \n",
      " |    critic_loss     | 1.5258   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 16899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=18000, episode_reward=-76.99 +/- 1.9952846874496712 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -76.99   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 18000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.6124   | \n",
      " |    critic_loss     | 1.6531   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 17899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=19000, episode_reward=-76.64 +/- 1.667839309339693 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -76.64   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 19000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.3576   | \n",
      " |    critic_loss     | 1.0894   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 18899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=20000, episode_reward=-76.28 +/- 1.8245389409000565 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -76.28   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 20000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.7702   | \n",
      " |    critic_loss     | 1.6925   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 19899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=21000, episode_reward=-75.93 +/- 1.4947175702804731 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -75.93   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 21000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.7131   | \n",
      " |    critic_loss     | 1.6783   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 20899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=22000, episode_reward=-76.45 +/- 1.486127678989648 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -76.45   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 22000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.4799   | \n",
      " |    critic_loss     | 1.6200   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 21899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=23000, episode_reward=-76.98 +/- 1.3166544391134174 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -76.98   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 23000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 7.1881   | \n",
      " |    critic_loss     | 1.7970   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 22899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=24000, episode_reward=-77.5 +/- 1.5694775644173231 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -77.5    | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 24000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.2683   | \n",
      " |    critic_loss     | 1.5671   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 23899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=25000, episode_reward=-78.02 +/- 1.6701033976139186 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -78.02   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 25000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.4075   | \n",
      " |    critic_loss     | 1.1019   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 24899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=26000, episode_reward=-78.55 +/- 1.4345548281939173 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -78.55   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 26000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.6749   | \n",
      " |    critic_loss     | 1.1687   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 25899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=27000, episode_reward=-77.32 +/- 1.5256549991783723 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -77.32   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 27000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.9413   | \n",
      " |    critic_loss     | 0.9853   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 26899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=28000, episode_reward=-76.1 +/- 1.4895184567795392 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -76.1    | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 28000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.5029   | \n",
      " |    critic_loss     | 1.3757   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 27899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=29000, episode_reward=-74.88 +/- 1.5317397271171957 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -74.88   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 29000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 7.5328   | \n",
      " |    critic_loss     | 1.8832   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 28899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=30000, episode_reward=-73.65 +/- 1.445043264401996 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -73.65   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 30000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 7.3668   | \n",
      " |    critic_loss     | 1.8417   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 29899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=31000, episode_reward=-72.43 +/- 1.4387687324024387 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -72.43   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 31000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.2689   | \n",
      " |    critic_loss     | 1.0672   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 30899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=32000, episode_reward=-73.13 +/- 1.689906540238781 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -73.13   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 32000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.9834   | \n",
      " |    critic_loss     | 0.9959   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 31899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=33000, episode_reward=-73.83 +/- 1.925657628971242 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -73.83   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 33000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.4274   | \n",
      " |    critic_loss     | 1.6068   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 32899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=34000, episode_reward=-74.53 +/- 1.5174956974522482 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -74.53   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 34000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.1637   | \n",
      " |    critic_loss     | 1.2909   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 33899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=35000, episode_reward=-75.23 +/- 1.060238334222813 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -75.23   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 35000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.9750   | \n",
      " |    critic_loss     | 1.4937   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 34899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=36000, episode_reward=-75.93 +/- 1.956317239533331 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -75.93   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 36000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.4426   | \n",
      " |    critic_loss     | 1.3606   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 35899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=37000, episode_reward=-76.83 +/- 1.7132290637587517 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -76.83   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 37000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.7395   | \n",
      " |    critic_loss     | 1.4349   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 36899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=38000, episode_reward=-77.73 +/- 1.8450240926186576 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -77.73   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 38000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.1457   | \n",
      " |    critic_loss     | 1.0364   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 37899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=39000, episode_reward=-78.63 +/- 1.9590286968320672 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -78.63   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 39000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.7568   | \n",
      " |    critic_loss     | 1.4392   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 38899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=40000, episode_reward=-79.53 +/- 1.4959489110836266 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -79.53   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 40000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.3620   | \n",
      " |    critic_loss     | 1.0905   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 39899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=41000, episode_reward=-80.43 +/- 1.2832645919531713 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -80.43   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 41000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 7.4256   | \n",
      " |    critic_loss     | 1.8564   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 40899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=42000, episode_reward=-81.78 +/- 1.2813712216804212 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -81.78   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 42000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 7.7678   | \n",
      " |    critic_loss     | 1.9420   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 41899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=43000, episode_reward=-83.13 +/- 1.6388461047800682 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -83.13   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 43000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.6519   | \n",
      " |    critic_loss     | 1.4130   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 42899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=44000, episode_reward=-84.48 +/- 1.7126171032015112 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -84.48   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 44000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.1509   | \n",
      " |    critic_loss     | 1.5377   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 43899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=45000, episode_reward=-85.83 +/- 1.0854904182359297 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -85.83   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 45000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.6271   | \n",
      " |    critic_loss     | 1.1568   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 44899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=46000, episode_reward=-87.18 +/- 1.0164483563466407 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -87.18   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 46000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.3792   | \n",
      " |    critic_loss     | 1.3448   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 45899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=47000, episode_reward=-82.47 +/- 1.7934599645698976 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -82.47   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 47000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.8607   | \n",
      " |    critic_loss     | 1.4652   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 46899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=48000, episode_reward=-77.76 +/- 1.7997506511544017 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -77.76   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 48000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.0968   | \n",
      " |    critic_loss     | 1.2742   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 47899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=49000, episode_reward=-73.05 +/- 1.6274051307842465 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -73.05   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 49000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.7695   | \n",
      " |    critic_loss     | 1.4424   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 48899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=50000, episode_reward=-68.34 +/- 1.8476249167934817 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -68.34   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 50000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.7375   | \n",
      " |    critic_loss     | 1.6844   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 49899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=51000, episode_reward=-63.62 +/- 1.6122364091598413 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -63.62   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 51000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.4629   | \n",
      " |    critic_loss     | 1.3657   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 50899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=52000, episode_reward=-64.4 +/- 1.5105855488665134 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -64.4    | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 52000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 7.3630   | \n",
      " |    critic_loss     | 1.8408   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 51899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=53000, episode_reward=-65.18 +/- 1.5966192581312373 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -65.18   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 53000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 7.0496   | \n",
      " |    critic_loss     | 1.7624   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 52899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=54000, episode_reward=-65.95 +/- 1.7678199950306346 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -65.95   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 54000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.3639   | \n",
      " |    critic_loss     | 1.0910   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 53899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=55000, episode_reward=-66.73 +/- 1.5358728085342963 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -66.73   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 55000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.6088   | \n",
      " |    critic_loss     | 1.6522   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 54899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=56000, episode_reward=-67.5 +/- 1.6627147542674023 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -67.5    | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 56000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.3163   | \n",
      " |    critic_loss     | 1.3291   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 55899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=57000, episode_reward=-71.49 +/- 1.8745678279008553 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -71.49   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 57000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.8629   | \n",
      " |    critic_loss     | 1.4657   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 56899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=58000, episode_reward=-75.48 +/- 1.3026002867089517 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -75.48   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 58000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.7566   | \n",
      " |    critic_loss     | 1.4392   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 57899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=59000, episode_reward=-79.47 +/- 1.9850606252549936 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -79.47   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 59000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.9666   | \n",
      " |    critic_loss     | 1.2416   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 58899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=60000, episode_reward=-83.45 +/- 1.5589317695212808 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -83.45   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 60000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.6396   | \n",
      " |    critic_loss     | 1.4099   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 59899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=61000, episode_reward=-87.44 +/- 1.2819909943868053 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -87.44   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 61000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.3808   | \n",
      " |    critic_loss     | 1.0952   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 60899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=62000, episode_reward=-87.45 +/- 1.4531138207506755 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -87.45   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 62000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.5107   | \n",
      " |    critic_loss     | 1.3777   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 61899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=63000, episode_reward=-87.46 +/- 1.1964231503893192 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -87.46   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 63000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.6628   | \n",
      " |    critic_loss     | 1.1657   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 62899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=64000, episode_reward=-87.47 +/- 1.2141809332114888 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -87.47   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 64000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.6787   | \n",
      " |    critic_loss     | 1.4197   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 63899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=65000, episode_reward=-87.48 +/- 1.7184151331498705 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -87.48   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 65000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 7.5847   | \n",
      " |    critic_loss     | 1.8962   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 64899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=66000, episode_reward=-87.48 +/- 1.4849503439727343 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -87.48   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 66000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.6013   | \n",
      " |    critic_loss     | 1.4003   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 65899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=67000, episode_reward=-82.21 +/- 1.5958436872307276 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -82.21   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 67000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.6637   | \n",
      " |    critic_loss     | 1.1659   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 66899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=68000, episode_reward=-76.94 +/- 1.4237556918474654 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -76.94   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 68000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.6630   | \n",
      " |    critic_loss     | 1.1657   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 67899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=69000, episode_reward=-71.67 +/- 1.133855846024698 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -71.67   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 69000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.3655   | \n",
      " |    critic_loss     | 1.0914   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 68899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=70000, episode_reward=-66.4 +/- 1.0977249010787058 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -66.4    | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 70000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.8539   | \n",
      " |    critic_loss     | 1.4635   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 69899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=71000, episode_reward=-61.13 +/- 1.3805003474382294 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -61.13   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 71000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 7.1674   | \n",
      " |    critic_loss     | 1.7919   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 70899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=72000, episode_reward=-67.32 +/- 1.2501479210694306 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -67.32   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 72000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.8832   | \n",
      " |    critic_loss     | 1.4708   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 71899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=73000, episode_reward=-73.51 +/- 1.2807060790749758 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -73.51   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 73000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.3083   | \n",
      " |    critic_loss     | 1.5771   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 72899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=74000, episode_reward=-79.7 +/- 1.9771214957375132 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -79.7    | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 74000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.5681   | \n",
      " |    critic_loss     | 1.3920   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 73899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=75000, episode_reward=-85.89 +/- 1.7907039603138983 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -85.89   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 75000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.2323   | \n",
      " |    critic_loss     | 1.5581   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 74899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=76000, episode_reward=-92.08 +/- 1.6209440334291774 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -92.08   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 76000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.8935   | \n",
      " |    critic_loss     | 1.7234   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 75899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=77000, episode_reward=-90.48 +/- 1.211821500299472 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -90.48   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 77000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.1380   | \n",
      " |    critic_loss     | 1.2845   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 76899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=78000, episode_reward=-88.88 +/- 1.601862037872674 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -88.88   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 78000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.3201   | \n",
      " |    critic_loss     | 1.5800   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 77899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=79000, episode_reward=-87.28 +/- 1.345034627411609 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -87.28   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 79000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.0293   | \n",
      " |    critic_loss     | 1.5073   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 78899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=80000, episode_reward=-85.68 +/- 1.6470520531856438 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -85.68   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 80000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.5060   | \n",
      " |    critic_loss     | 1.1265   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 79899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=81000, episode_reward=-84.08 +/- 1.8888773825571423 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -84.08   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 81000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 7.1127   | \n",
      " |    critic_loss     | 1.7782   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 80899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=82000, episode_reward=-80.94 +/- 1.6490458756335602 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -80.94   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 82000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.7393   | \n",
      " |    critic_loss     | 1.6848   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 81899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=83000, episode_reward=-77.81 +/- 1.2089067164924212 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -77.81   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 83000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.0050   | \n",
      " |    critic_loss     | 1.5013   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 82899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=84000, episode_reward=-74.67 +/- 1.784831350966532 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -74.67   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 84000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.0688   | \n",
      " |    critic_loss     | 1.0172   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 83899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=85000, episode_reward=-71.53 +/- 1.3986476746025485 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -71.53   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 85000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.7827   | \n",
      " |    critic_loss     | 1.6957   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 84899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=86000, episode_reward=-68.39 +/- 1.0130419781017825 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -68.39   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 86000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.9799   | \n",
      " |    critic_loss     | 0.9950   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 85899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=87000, episode_reward=-67.13 +/- 1.3413177990679177 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -67.13   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 87000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.6246   | \n",
      " |    critic_loss     | 1.4062   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 86899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=88000, episode_reward=-65.88 +/- 1.6514401027401064 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -65.88   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 88000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.5784   | \n",
      " |    critic_loss     | 1.1446   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 87899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=89000, episode_reward=-64.63 +/- 1.620203344704148 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -64.63   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 89000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.7062   | \n",
      " |    critic_loss     | 0.9265   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 88899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=90000, episode_reward=-63.37 +/- 1.8267457473429678 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -63.37   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 90000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.7494   | \n",
      " |    critic_loss     | 0.9374   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 89899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=91000, episode_reward=-62.12 +/- 1.454996752175537 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -62.12   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 91000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 7.3955   | \n",
      " |    critic_loss     | 1.8489   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 90899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=92000, episode_reward=-62.12 +/- 1.026899781037548 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -62.12   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 92000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.9824   | \n",
      " |    critic_loss     | 0.9956   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 91899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=93000, episode_reward=-62.11 +/- 1.8175062716258856 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -62.11   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 93000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 7.5819   | \n",
      " |    critic_loss     | 1.8955   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 92899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=94000, episode_reward=-62.11 +/- 1.2095055147745506 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -62.11   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 94000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.5932   | \n",
      " |    critic_loss     | 1.1483   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 93899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=95000, episode_reward=-62.1 +/- 1.3937085730898993 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -62.1    | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 95000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.6086   | \n",
      " |    critic_loss     | 1.6522   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 94899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=96000, episode_reward=-62.1 +/- 1.9070758980408442 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -62.1    | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 96000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.4128   | \n",
      " |    critic_loss     | 1.1032   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 95899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=97000, episode_reward=-64.22 +/- 1.5057409384084255 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -64.22   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 97000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.1980   | \n",
      " |    critic_loss     | 1.2995   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 96899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=98000, episode_reward=-66.33 +/- 1.292192360233312 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -66.33   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 98000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.8730   | \n",
      " |    critic_loss     | 0.9683   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 97899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=99000, episode_reward=-68.45 +/- 1.8408769636738556 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -68.45   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 99000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.5202   | \n",
      " |    critic_loss     | 1.3800   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 98899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=100000, episode_reward=-70.56 +/- 1.2527647443961514 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -70.56   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 100000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.0370   | \n",
      " |    critic_loss     | 1.0092   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 99899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=101000, episode_reward=-72.68 +/- 1.299993919449791 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -72.68   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 101000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.9405   | \n",
      " |    critic_loss     | 0.9851   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 100899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=102000, episode_reward=-69.95 +/- 1.2560592205318888 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -69.95   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 102000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.0393   | \n",
      " |    critic_loss     | 1.5098   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 101899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=103000, episode_reward=-67.22 +/- 1.1109247874610477 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -67.22   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 103000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.8781   | \n",
      " |    critic_loss     | 1.7195   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 102899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=104000, episode_reward=-64.49 +/- 1.2416545361253513 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -64.49   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 104000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.2845   | \n",
      " |    critic_loss     | 1.0711   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 103899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=105000, episode_reward=-61.75 +/- 1.5904072370976379 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -61.75   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 105000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.9746   | \n",
      " |    critic_loss     | 0.9936   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 104899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=106000, episode_reward=-59.02 +/- 1.3867852479281608 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -59.02   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 106000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.6675   | \n",
      " |    critic_loss     | 0.9169   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 105899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=107000, episode_reward=-61.3 +/- 1.9318936564537028 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -61.3    | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 107000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.0767   | \n",
      " |    critic_loss     | 1.0192   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 106899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=108000, episode_reward=-63.58 +/- 1.4336657444175276 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -63.58   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 108000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.0746   | \n",
      " |    critic_loss     | 1.5187   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 107899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=109000, episode_reward=-65.86 +/- 1.8872724408287334 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -65.86   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 109000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.9762   | \n",
      " |    critic_loss     | 0.9940   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 108899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=110000, episode_reward=-68.14 +/- 1.8173172973986245 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -68.14   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 110000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.6804   | \n",
      " |    critic_loss     | 1.1701   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 109899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=111000, episode_reward=-70.42 +/- 1.3800940150678074 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -70.42   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 111000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.0200   | \n",
      " |    critic_loss     | 1.5050   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 110899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=112000, episode_reward=-69.19 +/- 1.1087225898846407 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -69.19   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 112000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.3036   | \n",
      " |    critic_loss     | 1.0759   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 111899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=113000, episode_reward=-67.96 +/- 1.5072513834849643 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -67.96   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 113000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.0080   | \n",
      " |    critic_loss     | 1.0020   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 112899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=114000, episode_reward=-66.74 +/- 1.239586411221385 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -66.74   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 114000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.9953   | \n",
      " |    critic_loss     | 1.7488   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 113899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=115000, episode_reward=-65.51 +/- 1.2681940819254274 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -65.51   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 115000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.8656   | \n",
      " |    critic_loss     | 1.7164   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 114899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=116000, episode_reward=-64.28 +/- 1.63387999378234 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -64.28   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 116000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.6853   | \n",
      " |    critic_loss     | 1.1713   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 115899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=117000, episode_reward=-63.51 +/- 1.2064366995521674 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -63.51   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 117000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.3955   | \n",
      " |    critic_loss     | 1.3489   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 116899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=118000, episode_reward=-62.73 +/- 1.7325723130582356 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -62.73   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 118000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.0691   | \n",
      " |    critic_loss     | 1.0173   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 117899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=119000, episode_reward=-61.96 +/- 1.5478871974510988 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -61.96   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 119000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.9054   | \n",
      " |    critic_loss     | 1.4764   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 118899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=120000, episode_reward=-61.19 +/- 1.1031042769336181 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -61.19   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 120000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.8265   | \n",
      " |    critic_loss     | 1.2066   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 119899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=121000, episode_reward=-60.41 +/- 1.021748376383469 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -60.41   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 121000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.3280   | \n",
      " |    critic_loss     | 1.3320   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 120899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=122000, episode_reward=-62.36 +/- 1.3994447467963296 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -62.36   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 122000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 7.0711   | \n",
      " |    critic_loss     | 1.7678   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 121899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=123000, episode_reward=-64.3 +/- 1.7345353548862383 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -64.3    | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 123000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.8547   | \n",
      " |    critic_loss     | 0.9637   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 122899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=124000, episode_reward=-66.25 +/- 1.8906141764322828 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -66.25   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 124000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 7.1134   | \n",
      " |    critic_loss     | 1.7783   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 123899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=125000, episode_reward=-68.19 +/- 1.8674253171247779 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -68.19   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 125000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.3807   | \n",
      " |    critic_loss     | 1.3452   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 124899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=126000, episode_reward=-70.13 +/- 1.8079856742322384 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -70.13   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 126000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.8082   | \n",
      " |    critic_loss     | 1.4520   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 125899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=127000, episode_reward=-65.54 +/- 1.1722918932911588 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -65.54   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 127000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.9012   | \n",
      " |    critic_loss     | 1.4753   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 126899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=128000, episode_reward=-60.95 +/- 1.7061938165324482 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -60.95   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 128000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.9419   | \n",
      " |    critic_loss     | 1.4855   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 127899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=129000, episode_reward=-56.35 +/- 1.7122452128893126 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -56.35   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 129000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.1059   | \n",
      " |    critic_loss     | 1.2765   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 128899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=130000, episode_reward=-51.76 +/- 1.7918045756686694 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -51.76   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 130000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.8138   | \n",
      " |    critic_loss     | 0.9535   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 129899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=131000, episode_reward=-47.16 +/- 1.1267691238343889 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -47.16   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 131000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.3506   | \n",
      " |    critic_loss     | 1.0876   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 130899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=132000, episode_reward=-46.01 +/- 1.5855214313724462 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -46.01   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 132000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.0498   | \n",
      " |    critic_loss     | 1.2624   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 131899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=133000, episode_reward=-44.86 +/- 1.7839961061233565 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -44.86   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 133000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.5383   | \n",
      " |    critic_loss     | 1.6346   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 132899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=134000, episode_reward=-43.71 +/- 1.4557488629204034 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -43.71   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 134000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.9430   | \n",
      " |    critic_loss     | 1.2358   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 133899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=135000, episode_reward=-42.56 +/- 1.0781467582924404 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -42.56   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 135000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.7419   | \n",
      " |    critic_loss     | 1.4355   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 134899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=136000, episode_reward=-41.41 +/- 1.7567921274364195 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -41.41   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 136000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.0705   | \n",
      " |    critic_loss     | 1.0176   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 135899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=137000, episode_reward=-44.02 +/- 1.8151323015836005 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -44.02   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 137000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.7962   | \n",
      " |    critic_loss     | 1.6990   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 136899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=138000, episode_reward=-46.64 +/- 1.5748293997108922 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -46.64   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 138000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 7.1760   | \n",
      " |    critic_loss     | 1.7940   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 137899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=139000, episode_reward=-49.25 +/- 1.1966149155817838 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -49.25   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 139000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.7488   | \n",
      " |    critic_loss     | 1.6872   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 138899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=140000, episode_reward=-51.87 +/- 1.5174713742043262 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -51.87   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 140000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.1648   | \n",
      " |    critic_loss     | 1.5412   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 139899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=141000, episode_reward=-54.48 +/- 1.9271147769070267 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -54.48   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 141000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.4985   | \n",
      " |    critic_loss     | 1.6246   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 140899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=142000, episode_reward=-59.54 +/- 1.0010689736317444 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -59.54   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 142000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 7.0467   | \n",
      " |    critic_loss     | 1.7617   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 141899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=143000, episode_reward=-64.61 +/- 1.6588692647357237 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -64.61   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 143000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.8720   | \n",
      " |    critic_loss     | 1.2180   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 142899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=144000, episode_reward=-69.67 +/- 1.977484030613517 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -69.67   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 144000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.6360   | \n",
      " |    critic_loss     | 0.9090   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 143899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=145000, episode_reward=-74.73 +/- 1.5274416096756247 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -74.73   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 145000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.7159   | \n",
      " |    critic_loss     | 1.6790   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 144899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=146000, episode_reward=-79.79 +/- 1.5982786205619233 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -79.79   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 146000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.2788   | \n",
      " |    critic_loss     | 1.0697   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 145899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=147000, episode_reward=-76.08 +/- 1.492100092739138 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -76.08   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 147000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.0013   | \n",
      " |    critic_loss     | 1.2503   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 146899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=148000, episode_reward=-72.37 +/- 1.9291874094204449 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -72.37   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 148000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.6617   | \n",
      " |    critic_loss     | 1.1654   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 147899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=149000, episode_reward=-68.65 +/- 1.951243819077364 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -68.65   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 149000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.1520   | \n",
      " |    critic_loss     | 1.2880   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 148899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=150000, episode_reward=-64.94 +/- 1.4746038896406333 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -64.94   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 150000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.2821   | \n",
      " |    critic_loss     | 1.3205   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 149899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=151000, episode_reward=-61.22 +/- 1.3300151617987406 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -61.22   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 151000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.0311   | \n",
      " |    critic_loss     | 1.5078   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 150899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=152000, episode_reward=-59.65 +/- 1.631797791495801 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -59.65   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 152000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 7.1055   | \n",
      " |    critic_loss     | 1.7764   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 151899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=153000, episode_reward=-58.07 +/- 1.3359359618163045 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -58.07   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 153000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.0348   | \n",
      " |    critic_loss     | 1.5087   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 152899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=154000, episode_reward=-56.5 +/- 1.7979734073359934 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -56.5    | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 154000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.6237   | \n",
      " |    critic_loss     | 1.6559   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 153899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=155000, episode_reward=-54.93 +/- 1.6195672416139044 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -54.93   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 155000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.1155   | \n",
      " |    critic_loss     | 1.0289   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 154899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=156000, episode_reward=-53.35 +/- 1.6833562801694746 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -53.35   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 156000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.2344   | \n",
      " |    critic_loss     | 1.5586   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 155899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=157000, episode_reward=-60.07 +/- 1.8565121730200518 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -60.07   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 157000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.4029   | \n",
      " |    critic_loss     | 1.3507   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 156899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=158000, episode_reward=-66.79 +/- 1.5993491131649709 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -66.79   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 158000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.1563   | \n",
      " |    critic_loss     | 1.2891   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 157899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=159000, episode_reward=-73.51 +/- 1.364779696987903 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -73.51   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 159000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.7138   | \n",
      " |    critic_loss     | 1.6784   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 158899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=160000, episode_reward=-80.23 +/- 1.0690060936580026 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -80.23   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 160000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.5398   | \n",
      " |    critic_loss     | 0.8849   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 159899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=161000, episode_reward=-86.94 +/- 1.940562342353743 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -86.94   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 161000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 7.0275   | \n",
      " |    critic_loss     | 1.7569   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 160899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=162000, episode_reward=-83.59 +/- 1.9175181355167519 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -83.59   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 162000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.7437   | \n",
      " |    critic_loss     | 1.1859   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 161899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=163000, episode_reward=-80.24 +/- 1.1109223375021258 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -80.24   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 163000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.7060   | \n",
      " |    critic_loss     | 1.1765   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 162899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=164000, episode_reward=-76.88 +/- 1.4417543075406067 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -76.88   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 164000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.4964   | \n",
      " |    critic_loss     | 1.3741   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 163899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=165000, episode_reward=-73.52 +/- 1.1696448212564763 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -73.52   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 165000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.5718   | \n",
      " |    critic_loss     | 1.6429   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 164899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=166000, episode_reward=-70.16 +/- 1.0841774691903128 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -70.16   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 166000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.5808   | \n",
      " |    critic_loss     | 1.3952   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 165899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=167000, episode_reward=-77.03 +/- 1.8068025257612705 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -77.03   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 167000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.1805   | \n",
      " |    critic_loss     | 1.2951   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 166899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=168000, episode_reward=-83.89 +/- 1.7879280888270328 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -83.89   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 168000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.3457   | \n",
      " |    critic_loss     | 1.3364   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 167899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=169000, episode_reward=-90.75 +/- 1.1462725660808815 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -90.75   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 169000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.9256   | \n",
      " |    critic_loss     | 0.9814   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 168899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=170000, episode_reward=-97.61 +/- 1.192687777155784 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -97.61   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 170000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.7684   | \n",
      " |    critic_loss     | 1.4421   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 169899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=171000, episode_reward=-104.47 +/- 1.3439758889470843 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -104.47  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 171000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.2879   | \n",
      " |    critic_loss     | 1.0720   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 170899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=172000, episode_reward=-95.11 +/- 1.2733624097363014 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -95.11   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 172000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.3281   | \n",
      " |    critic_loss     | 1.0820   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 171899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=173000, episode_reward=-85.75 +/- 1.51125602428093 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -85.75   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 173000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.0434   | \n",
      " |    critic_loss     | 1.5108   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 172899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=174000, episode_reward=-76.39 +/- 1.5457930662150805 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -76.39   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 174000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.5636   | \n",
      " |    critic_loss     | 1.1409   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 173899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=175000, episode_reward=-67.03 +/- 1.0006678472812018 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -67.03   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 175000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 7.0495   | \n",
      " |    critic_loss     | 1.7624   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 174899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=176000, episode_reward=-57.67 +/- 1.4334768217796894 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -57.67   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 176000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.1735   | \n",
      " |    critic_loss     | 1.5434   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 175899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=177000, episode_reward=-57.01 +/- 1.665929688309371 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -57.01   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 177000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.2173   | \n",
      " |    critic_loss     | 1.0543   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 176899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=178000, episode_reward=-56.35 +/- 1.1457138333676258 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -56.35   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 178000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.8095   | \n",
      " |    critic_loss     | 1.4524   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 177899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=179000, episode_reward=-55.69 +/- 1.4374770596561288 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -55.69   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 179000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.0240   | \n",
      " |    critic_loss     | 1.5060   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 178899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=180000, episode_reward=-55.03 +/- 1.5738405607288393 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -55.03   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 180000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.8373   | \n",
      " |    critic_loss     | 1.2093   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 179899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=181000, episode_reward=-54.37 +/- 1.8886444183045636 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -54.37   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 181000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 7.1376   | \n",
      " |    critic_loss     | 1.7844   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 180899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=182000, episode_reward=-58.01 +/- 1.3004485366778549 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -58.01   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 182000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.9263   | \n",
      " |    critic_loss     | 1.2316   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 181899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=183000, episode_reward=-61.66 +/- 1.9384585542213255 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -61.66   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 183000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.8181   | \n",
      " |    critic_loss     | 1.2045   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 182899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=184000, episode_reward=-65.3 +/- 1.731030295020835 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -65.3    | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 184000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.3000   | \n",
      " |    critic_loss     | 1.3250   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 183899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=185000, episode_reward=-68.94 +/- 1.9651931842415062 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -68.94   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 185000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.1536   | \n",
      " |    critic_loss     | 1.0384   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 184899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=186000, episode_reward=-72.57 +/- 1.203452943210908 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -72.57   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 186000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.2996   | \n",
      " |    critic_loss     | 1.3249   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 185899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=187000, episode_reward=-78.13 +/- 1.186707771186059 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -78.13   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 187000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.9218   | \n",
      " |    critic_loss     | 1.2305   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 186899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=188000, episode_reward=-83.68 +/- 1.8261982961470977 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -83.68   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 188000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.8244   | \n",
      " |    critic_loss     | 1.4561   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 187899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=189000, episode_reward=-89.23 +/- 1.1529223321073196 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -89.23   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 189000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.9910   | \n",
      " |    critic_loss     | 1.2477   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 188899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=190000, episode_reward=-94.78 +/- 1.4694124677638034 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -94.78   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 190000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.3979   | \n",
      " |    critic_loss     | 0.8495   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 189899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=191000, episode_reward=-100.32 +/- 1.5271444552821545 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -100.32  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 191000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.4285   | \n",
      " |    critic_loss     | 1.1071   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 190899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=192000, episode_reward=-90.49 +/- 1.3024228976943357 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -90.49   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 192000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.4458   | \n",
      " |    critic_loss     | 0.8614   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 191899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=193000, episode_reward=-80.65 +/- 1.3932126959739883 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -80.65   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 193000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 7.1792   | \n",
      " |    critic_loss     | 1.7948   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 192899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=194000, episode_reward=-70.81 +/- 1.9452581031282905 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -70.81   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 194000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.3088   | \n",
      " |    critic_loss     | 1.5772   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 193899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=195000, episode_reward=-60.97 +/- 1.4600016427325162 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -60.97   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 195000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 7.1437   | \n",
      " |    critic_loss     | 1.7859   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 194899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=196000, episode_reward=-51.13 +/- 1.2944939392909172 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -51.13   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 196000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.1784   | \n",
      " |    critic_loss     | 1.0446   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 195899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=197000, episode_reward=-52.59 +/- 1.110405120359517 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -52.59   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 197000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.8121   | \n",
      " |    critic_loss     | 1.4530   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 196899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=198000, episode_reward=-54.05 +/- 1.111078808103648 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -54.05   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 198000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.7450   | \n",
      " |    critic_loss     | 1.6862   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 197899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=199000, episode_reward=-55.51 +/- 1.8543985454942686 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -55.51   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 199000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.0168   | \n",
      " |    critic_loss     | 1.0042   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 198899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=200000, episode_reward=-56.96 +/- 1.6187680311868482 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -56.96   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 200000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.3433   | \n",
      " |    critic_loss     | 1.5858   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 199899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=201000, episode_reward=-58.42 +/- 1.4030304923704398 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -58.42   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 201000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.7401   | \n",
      " |    critic_loss     | 0.9350   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 200899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=202000, episode_reward=-60.79 +/- 1.8920519913236657 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -60.79   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 202000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.7585   | \n",
      " |    critic_loss     | 1.1896   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 201899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=203000, episode_reward=-63.16 +/- 1.829707750300406 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -63.16   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 203000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.2270   | \n",
      " |    critic_loss     | 0.8068   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 202899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=204000, episode_reward=-65.53 +/- 1.8271400819722392 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -65.53   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 204000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.7159   | \n",
      " |    critic_loss     | 1.6790   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 203899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=205000, episode_reward=-67.89 +/- 1.6420722636687497 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -67.89   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 205000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.4742   | \n",
      " |    critic_loss     | 1.1185   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 204899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=206000, episode_reward=-70.26 +/- 1.4134688740511145 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -70.26   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 206000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.0744   | \n",
      " |    critic_loss     | 1.5186   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 205899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=207000, episode_reward=-79.78 +/- 1.507083322537014 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -79.78   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 207000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.5029   | \n",
      " |    critic_loss     | 0.8757   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 206899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=208000, episode_reward=-89.29 +/- 1.2199500016524532 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -89.29   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 208000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.5394   | \n",
      " |    critic_loss     | 1.3849   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 207899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=209000, episode_reward=-98.81 +/- 1.760698097965607 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -98.81   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 209000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.6544   | \n",
      " |    critic_loss     | 1.4136   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 208899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=210000, episode_reward=-108.32 +/- 1.635770228143643 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -108.32  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 210000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.5578   | \n",
      " |    critic_loss     | 1.3895   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 209899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=211000, episode_reward=-117.83 +/- 1.3320343170862958 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -117.83  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 211000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.4439   | \n",
      " |    critic_loss     | 0.8610   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 210899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=212000, episode_reward=-111.75 +/- 1.5799559707243058 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -111.75  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 212000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.1971   | \n",
      " |    critic_loss     | 1.5493   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 211899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=213000, episode_reward=-105.68 +/- 1.768117851201188 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -105.68  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 213000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.2026   | \n",
      " |    critic_loss     | 1.3006   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 212899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=214000, episode_reward=-99.6 +/- 1.128171552833463 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -99.6    | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 214000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.4739   | \n",
      " |    critic_loss     | 1.1185   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 213899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=215000, episode_reward=-93.52 +/- 1.808232149909229 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -93.52   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 215000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.8347   | \n",
      " |    critic_loss     | 1.2087   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 214899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=216000, episode_reward=-87.43 +/- 1.1802312420246386 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -87.43   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 216000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.6351   | \n",
      " |    critic_loss     | 1.4088   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 215899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=217000, episode_reward=-81.53 +/- 1.5110626599580383 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -81.53   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 217000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.6308   | \n",
      " |    critic_loss     | 1.6577   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 216899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=218000, episode_reward=-75.63 +/- 1.2705082135720946 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -75.63   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 218000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.6721   | \n",
      " |    critic_loss     | 0.9180   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 217899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=219000, episode_reward=-69.72 +/- 1.8833014667287657 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -69.72   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 219000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.8580   | \n",
      " |    critic_loss     | 1.2145   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 218899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=220000, episode_reward=-63.81 +/- 1.5381971700418842 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -63.81   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 220000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.6193   | \n",
      " |    critic_loss     | 1.4048   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 219899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=221000, episode_reward=-57.9 +/- 1.874917783681706 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -57.9    | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 221000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.0358   | \n",
      " |    critic_loss     | 1.2589   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 220899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=222000, episode_reward=-55.51 +/- 1.1857746767221309 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -55.51   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 222000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.6121   | \n",
      " |    critic_loss     | 1.4030   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 221899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=223000, episode_reward=-53.12 +/- 1.7933335509805852 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -53.12   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 223000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.6272   | \n",
      " |    critic_loss     | 0.9068   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 222899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=224000, episode_reward=-50.73 +/- 1.3075258114177686 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -50.73   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 224000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.6130   | \n",
      " |    critic_loss     | 1.6532   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 223899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=225000, episode_reward=-48.34 +/- 1.6286421864962741 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -48.34   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 225000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.8152   | \n",
      " |    critic_loss     | 1.7038   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 224899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=226000, episode_reward=-45.94 +/- 1.4159548294705235 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -45.94   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 226000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.2315   | \n",
      " |    critic_loss     | 1.3079   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 225899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=227000, episode_reward=-44.27 +/- 1.358017418017186 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -44.27   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 227000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.4841   | \n",
      " |    critic_loss     | 1.1210   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 226899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=228000, episode_reward=-42.6 +/- 1.4185492137154114 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -42.6    | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 228000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.4373   | \n",
      " |    critic_loss     | 1.6093   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 227899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=229000, episode_reward=-40.93 +/- 1.3560988185238227 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -40.93   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 229000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.2746   | \n",
      " |    critic_loss     | 1.5687   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 228899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=230000, episode_reward=-39.25 +/- 1.894421333058078 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -39.25   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 230000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.7322   | \n",
      " |    critic_loss     | 1.1830   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 229899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=231000, episode_reward=-37.57 +/- 1.8744035383246223 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -37.57   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 231000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.4202   | \n",
      " |    critic_loss     | 1.6051   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 230899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=232000, episode_reward=-33.66 +/- 1.2993605927156289 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -33.66   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 232000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.3580   | \n",
      " |    critic_loss     | 1.0895   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 231899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=233000, episode_reward=-29.75 +/- 1.988342218905296 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -29.75   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 233000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.7041   | \n",
      " |    critic_loss     | 1.4260   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 232899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=234000, episode_reward=-25.84 +/- 1.847145059270784 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -25.84   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 234000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.8751   | \n",
      " |    critic_loss     | 1.7188   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 233899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=235000, episode_reward=-21.92 +/- 1.5666262564989033 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -21.92   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 235000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.6651   | \n",
      " |    critic_loss     | 1.4163   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 234899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=236000, episode_reward=-18.0 +/- 1.8248932793483634 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -18.0    | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 236000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.1804   | \n",
      " |    critic_loss     | 1.0451   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 235899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=237000, episode_reward=-23.87 +/- 1.4328686832443798 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -23.87   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 237000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.4235   | \n",
      " |    critic_loss     | 0.8559   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 236899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=238000, episode_reward=-29.73 +/- 1.1014707265050865 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -29.73   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 238000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.5892   | \n",
      " |    critic_loss     | 1.1473   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 237899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=239000, episode_reward=-35.6 +/- 1.780666915944229 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -35.6    | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 239000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.7392   | \n",
      " |    critic_loss     | 1.6848   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 238899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=240000, episode_reward=-41.46 +/- 1.874430045753733 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -41.46   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 240000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.8078   | \n",
      " |    critic_loss     | 1.7019   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 239899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=241000, episode_reward=-47.31 +/- 1.4077996233419454 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -47.31   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 241000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.3469   | \n",
      " |    critic_loss     | 1.0867   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 240899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=242000, episode_reward=-44.34 +/- 1.8778548263473693 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -44.34   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 242000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.3587   | \n",
      " |    critic_loss     | 1.5897   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 241899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=243000, episode_reward=-41.36 +/- 1.0671110033213087 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -41.36   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 243000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.8033   | \n",
      " |    critic_loss     | 1.4508   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 242899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=244000, episode_reward=-38.38 +/- 1.6337882096434408 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -38.38   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 244000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.8167   | \n",
      " |    critic_loss     | 1.7042   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 243899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=245000, episode_reward=-35.4 +/- 1.8729956611334893 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -35.4    | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 245000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.9764   | \n",
      " |    critic_loss     | 1.7441   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 244899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=246000, episode_reward=-32.41 +/- 1.9361259275754672 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -32.41   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 246000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.1903   | \n",
      " |    critic_loss     | 1.0476   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 245899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=247000, episode_reward=-32.73 +/- 1.0191454878822865 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -32.73   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 247000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.8115   | \n",
      " |    critic_loss     | 0.9529   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 246899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=248000, episode_reward=-33.04 +/- 1.708504289364452 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -33.04   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 248000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.1443   | \n",
      " |    critic_loss     | 1.0361   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 247899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=249000, episode_reward=-33.35 +/- 1.9564833471626601 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -33.35   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 249000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.2317   | \n",
      " |    critic_loss     | 1.0579   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 248899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=250000, episode_reward=-33.65 +/- 1.5486491647087015 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -33.65   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 250000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.7398   | \n",
      " |    critic_loss     | 1.4350   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 249899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=251000, episode_reward=-33.96 +/- 1.605578224908851 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -33.96   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 251000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.9302   | \n",
      " |    critic_loss     | 1.2325   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 250899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=252000, episode_reward=-31.33 +/- 1.53821450744705 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -31.33   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 252000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.0909   | \n",
      " |    critic_loss     | 0.7727   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 251899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=253000, episode_reward=-28.7 +/- 1.689180175248172 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -28.7    | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 253000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.4502   | \n",
      " |    critic_loss     | 1.3625   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 252899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=254000, episode_reward=-26.07 +/- 1.7618319634709703 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -26.07   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 254000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.8972   | \n",
      " |    critic_loss     | 1.2243   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 253899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=255000, episode_reward=-23.43 +/- 1.8071693440048682 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -23.43   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 255000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.9562   | \n",
      " |    critic_loss     | 1.7391   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 254899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=256000, episode_reward=-20.8 +/- 1.578096832971884 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -20.8    | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 256000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.6177   | \n",
      " |    critic_loss     | 0.9044   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 255899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=257000, episode_reward=-24.5 +/- 1.1698705476371773 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -24.5    | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 257000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.0141   | \n",
      " |    critic_loss     | 0.7535   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 256899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=258000, episode_reward=-28.2 +/- 1.0374617900618361 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -28.2    | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 258000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.2067   | \n",
      " |    critic_loss     | 1.0517   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 257899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=259000, episode_reward=-31.9 +/- 1.351067258007478 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -31.9    | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 259000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.8887   | \n",
      " |    critic_loss     | 0.9722   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 258899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=260000, episode_reward=-35.6 +/- 1.4168729428085105 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -35.6    | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 260000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.2935   | \n",
      " |    critic_loss     | 1.5734   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 259899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=261000, episode_reward=-39.29 +/- 1.059780894389821 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -39.29   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 261000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.4155   | \n",
      " |    critic_loss     | 1.6039   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 260899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=262000, episode_reward=-34.71 +/- 1.1712695708113205 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -34.71   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 262000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.6637   | \n",
      " |    critic_loss     | 1.4159   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 261899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=263000, episode_reward=-30.12 +/- 1.5043747020208948 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -30.12   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 263000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.9291   | \n",
      " |    critic_loss     | 1.4823   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 262899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=264000, episode_reward=-25.54 +/- 1.9082820799152318 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -25.54   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 264000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.6716   | \n",
      " |    critic_loss     | 0.9179   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 263899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=265000, episode_reward=-20.94 +/- 1.8149202945574219 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -20.94   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 265000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.7726   | \n",
      " |    critic_loss     | 0.9431   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 264899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=266000, episode_reward=-16.35 +/- 1.9777930723763029 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -16.35   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 266000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.1791   | \n",
      " |    critic_loss     | 1.0448   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 265899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=267000, episode_reward=-23.05 +/- 1.6476528918206175 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -23.05   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 267000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.0933   | \n",
      " |    critic_loss     | 1.5233   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 266899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=268000, episode_reward=-29.74 +/- 1.4078194873828578 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -29.74   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 268000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.5042   | \n",
      " |    critic_loss     | 1.1260   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 267899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=269000, episode_reward=-36.44 +/- 1.8940109887310468 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -36.44   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 269000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.4753   | \n",
      " |    critic_loss     | 0.8688   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 268899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=270000, episode_reward=-43.13 +/- 1.853354198705309 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -43.13   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 270000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.3496   | \n",
      " |    critic_loss     | 0.8374   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 269899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=271000, episode_reward=-49.81 +/- 1.533720806210316 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -49.81   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 271000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.6525   | \n",
      " |    critic_loss     | 0.9131   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 270899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=272000, episode_reward=-43.42 +/- 1.5437776999879853 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -43.42   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 272000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.7171   | \n",
      " |    critic_loss     | 0.9293   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 271899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=273000, episode_reward=-37.01 +/- 1.530897006412573 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -37.01   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 273000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.5380   | \n",
      " |    critic_loss     | 1.1345   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 272899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=274000, episode_reward=-30.61 +/- 1.2578059600575768 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -30.61   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 274000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.2252   | \n",
      " |    critic_loss     | 1.3063   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 273899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=275000, episode_reward=-24.2 +/- 1.6729756822215813 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -24.2    | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 275000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.2540   | \n",
      " |    critic_loss     | 0.8135   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 274899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=276000, episode_reward=-17.78 +/- 1.8069744757884174 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -17.78   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 276000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.2504   | \n",
      " |    critic_loss     | 0.8126   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 275899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=277000, episode_reward=-17.04 +/- 1.8905944041743399 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -17.04   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 277000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.5727   | \n",
      " |    critic_loss     | 1.3932   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 276899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=278000, episode_reward=-16.3 +/- 1.591916841464966 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -16.3    | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 278000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.3004   | \n",
      " |    critic_loss     | 1.3251   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 277899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=279000, episode_reward=-15.55 +/- 1.683926090152045 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -15.55   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 279000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.5834   | \n",
      " |    critic_loss     | 1.3958   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 278899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=280000, episode_reward=-14.79 +/- 1.5299467726591347 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -14.79   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 280000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.0405   | \n",
      " |    critic_loss     | 0.7601   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 279899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=281000, episode_reward=-14.04 +/- 1.6446158448213375 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -14.04   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 281000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.3632   | \n",
      " |    critic_loss     | 1.5908   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 280899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=282000, episode_reward=-16.87 +/- 1.7241910023301654 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -16.87   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 282000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.7097   | \n",
      " |    critic_loss     | 1.6774   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 281899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=283000, episode_reward=-19.7 +/- 1.8802453954270915 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -19.7    | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 283000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.7410   | \n",
      " |    critic_loss     | 1.1853   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 282899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=284000, episode_reward=-22.52 +/- 1.3674960219903758 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -22.52   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 284000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.4199   | \n",
      " |    critic_loss     | 1.1050   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 283899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=285000, episode_reward=-25.34 +/- 1.9785388307778713 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -25.34   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 285000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.5220   | \n",
      " |    critic_loss     | 1.6305   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 284899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=286000, episode_reward=-28.16 +/- 1.729266997756528 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -28.16   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 286000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.1988   | \n",
      " |    critic_loss     | 1.5497   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 285899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=287000, episode_reward=-21.47 +/- 1.8917497793311013 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -21.47   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 287000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.5739   | \n",
      " |    critic_loss     | 1.3935   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 286899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=288000, episode_reward=-14.77 +/- 1.587199919222002 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -14.77   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 288000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.4016   | \n",
      " |    critic_loss     | 1.6004   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 287899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=289000, episode_reward=-8.08 +/- 1.253727113256141 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -8.08    | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 289000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.2879   | \n",
      " |    critic_loss     | 0.8220   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 288899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=290000, episode_reward=-1.37 +/- 1.7714139095533548 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -1.37    | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 290000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.1895   | \n",
      " |    critic_loss     | 1.5474   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 289899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=291000, episode_reward=5.33 +/- 1.651899152964836 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 5.33     | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 291000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.8967   | \n",
      " |    critic_loss     | 1.2242   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 290899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=292000, episode_reward=5.0 +/- 1.5831903640643445 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 5.0      | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 292000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.4198   | \n",
      " |    critic_loss     | 1.1050   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 291899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=293000, episode_reward=4.68 +/- 1.0368382764669741 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 4.68     | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 293000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.5306   | \n",
      " |    critic_loss     | 1.3827   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 292899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=294000, episode_reward=4.36 +/- 1.6957170555350356 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 4.36     | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 294000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.1368   | \n",
      " |    critic_loss     | 0.7842   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 293899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=295000, episode_reward=4.04 +/- 1.0836964858617775 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 4.04     | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 295000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.9801   | \n",
      " |    critic_loss     | 1.2450   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 294899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=296000, episode_reward=3.73 +/- 1.1502861028492695 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 3.73     | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 296000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.8480   | \n",
      " |    critic_loss     | 1.4620   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 295899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=297000, episode_reward=3.2 +/- 1.748661330355672 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 3.2      | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 297000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.4397   | \n",
      " |    critic_loss     | 0.8599   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 296899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=298000, episode_reward=2.68 +/- 1.4001174474319242 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 2.68     | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 298000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.0507   | \n",
      " |    critic_loss     | 1.2627   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 297899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=299000, episode_reward=2.16 +/- 1.3908837276456412 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 2.16     | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 299000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.5492   | \n",
      " |    critic_loss     | 1.1373   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 298899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=300000, episode_reward=1.64 +/- 1.1262271211465813 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 1.64     | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 300000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.5841   | \n",
      " |    critic_loss     | 0.8960   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 299899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=301000, episode_reward=1.13 +/- 1.8624549462361775 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 1.13     | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 301000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.9484   | \n",
      " |    critic_loss     | 1.4871   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 300899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=302000, episode_reward=-1.26 +/- 1.8069489232017033 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -1.26    | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 302000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.5922   | \n",
      " |    critic_loss     | 1.3980   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 301899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=303000, episode_reward=-3.64 +/- 1.2409736750359324 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -3.64    | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 303000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.9966   | \n",
      " |    critic_loss     | 0.7491   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 302899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=304000, episode_reward=-6.02 +/- 1.6440546247018488 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -6.02    | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 304000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.0009   | \n",
      " |    critic_loss     | 1.2502   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 303899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=305000, episode_reward=-8.4 +/- 1.1482366711897183 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -8.4     | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 305000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.7262   | \n",
      " |    critic_loss     | 1.4315   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 304899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=306000, episode_reward=-10.78 +/- 1.4648264415177792 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -10.78   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 306000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.3282   | \n",
      " |    critic_loss     | 1.0821   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 305899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=307000, episode_reward=-11.07 +/- 1.3424027419971303 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -11.07   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 307000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.0890   | \n",
      " |    critic_loss     | 1.2723   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 306899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=308000, episode_reward=-11.35 +/- 1.8389061530825541 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -11.35   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 308000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.3925   | \n",
      " |    critic_loss     | 1.3481   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 307899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=309000, episode_reward=-11.63 +/- 1.91168934586346 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -11.63   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 309000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.5856   | \n",
      " |    critic_loss     | 1.1464   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 308899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=310000, episode_reward=-11.91 +/- 1.6373646210318142 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -11.91   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 310000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.0983   | \n",
      " |    critic_loss     | 1.5246   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 309899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=311000, episode_reward=-12.18 +/- 1.0943788413137197 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -12.18   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 311000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.5771   | \n",
      " |    critic_loss     | 1.1443   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 310899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=312000, episode_reward=-10.45 +/- 1.5550041229587857 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -10.45   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 312000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.8933   | \n",
      " |    critic_loss     | 1.2233   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 311899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=313000, episode_reward=-8.72 +/- 1.7743769202957602 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -8.72    | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 313000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.7358   | \n",
      " |    critic_loss     | 1.1839   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 312899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=314000, episode_reward=-6.99 +/- 1.3315316399492834 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -6.99    | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 314000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.5637   | \n",
      " |    critic_loss     | 1.1409   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 313899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=315000, episode_reward=-5.25 +/- 1.340487916529677 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -5.25    | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 315000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.0516   | \n",
      " |    critic_loss     | 1.2629   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 314899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=316000, episode_reward=-3.51 +/- 1.6866169006911425 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -3.51    | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 316000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.5752   | \n",
      " |    critic_loss     | 1.6438   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 315899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=317000, episode_reward=-4.56 +/- 1.7631126742627168 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -4.56    | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 317000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.3701   | \n",
      " |    critic_loss     | 1.3425   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 316899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=318000, episode_reward=-5.62 +/- 1.9195646406272369 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -5.62    | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 318000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.9549   | \n",
      " |    critic_loss     | 1.4887   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 317899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=319000, episode_reward=-6.67 +/- 1.9607459027227456 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -6.67    | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 319000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.6538   | \n",
      " |    critic_loss     | 1.4135   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 318899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=320000, episode_reward=-7.71 +/- 1.2651050400956758 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -7.71    | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 320000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.8529   | \n",
      " |    critic_loss     | 1.2132   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 319899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=321000, episode_reward=-8.75 +/- 1.8697063127726419 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -8.75    | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 321000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.2911   | \n",
      " |    critic_loss     | 1.5728   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 320899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=322000, episode_reward=-7.4 +/- 1.50029665564791 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -7.4     | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 322000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.3259   | \n",
      " |    critic_loss     | 1.5815   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 321899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=323000, episode_reward=-6.04 +/- 1.0152099725451285 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -6.04    | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 323000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.3701   | \n",
      " |    critic_loss     | 0.8425   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 322899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=324000, episode_reward=-4.68 +/- 1.8643926690334585 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -4.68    | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 324000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.4510   | \n",
      " |    critic_loss     | 1.6127   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 323899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=325000, episode_reward=-3.31 +/- 1.1965216905669234 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -3.31    | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 325000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.6433   | \n",
      " |    critic_loss     | 1.6608   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 324899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=326000, episode_reward=-1.94 +/- 1.8453360665036875 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -1.94    | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 326000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.8349   | \n",
      " |    critic_loss     | 1.4587   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 325899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=327000, episode_reward=-4.08 +/- 1.2254000982933997 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -4.08    | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 327000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.7323   | \n",
      " |    critic_loss     | 1.1831   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 326899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=328000, episode_reward=-6.22 +/- 1.7533655398347667 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -6.22    | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 328000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.1343   | \n",
      " |    critic_loss     | 1.2836   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 327899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=329000, episode_reward=-8.35 +/- 1.0291820578448023 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -8.35    | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 329000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.2821   | \n",
      " |    critic_loss     | 1.0705   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 328899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=330000, episode_reward=-10.48 +/- 1.086747726219818 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -10.48   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 330000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.2400   | \n",
      " |    critic_loss     | 1.5600   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 329899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=331000, episode_reward=-12.6 +/- 1.9092563978688597 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -12.6    | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 331000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.6719   | \n",
      " |    critic_loss     | 0.9180   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 330899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=332000, episode_reward=-10.38 +/- 1.043549224270559 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -10.38   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 332000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.2155   | \n",
      " |    critic_loss     | 1.5539   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 331899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=333000, episode_reward=-8.16 +/- 1.1288024171422486 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -8.16    | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 333000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.4750   | \n",
      " |    critic_loss     | 1.3688   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 332899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=334000, episode_reward=-5.94 +/- 1.396416759873843 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -5.94    | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 334000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.7098   | \n",
      " |    critic_loss     | 0.6774   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 333899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=335000, episode_reward=-3.71 +/- 1.566409241323241 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -3.71    | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 335000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.1552   | \n",
      " |    critic_loss     | 0.7888   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 334899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=336000, episode_reward=-1.47 +/- 1.222647694796225 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -1.47    | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 336000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.9191   | \n",
      " |    critic_loss     | 1.4798   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 335899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=337000, episode_reward=-5.39 +/- 1.5873772480788024 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -5.39    | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 337000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.7761   | \n",
      " |    critic_loss     | 1.1940   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 336899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=338000, episode_reward=-9.3 +/- 1.0061416024857883 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -9.3     | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 338000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.8641   | \n",
      " |    critic_loss     | 1.2160   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 337899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=339000, episode_reward=-13.2 +/- 1.2904535373548887 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -13.2    | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 339000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.5674   | \n",
      " |    critic_loss     | 0.8919   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 338899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=340000, episode_reward=-17.1 +/- 1.8229462224732238 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -17.1    | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 340000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.8324   | \n",
      " |    critic_loss     | 0.7081   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 339899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=341000, episode_reward=-21.0 +/- 1.876644882299125 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -21.0    | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 341000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.6609   | \n",
      " |    critic_loss     | 1.1652   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 340899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=342000, episode_reward=-15.39 +/- 1.5541205479226095 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -15.39   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 342000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.7368   | \n",
      " |    critic_loss     | 0.6842   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 341899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=343000, episode_reward=-9.79 +/- 1.5344205711552839 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -9.79    | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 343000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.2108   | \n",
      " |    critic_loss     | 1.5527   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 342899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=344000, episode_reward=-4.18 +/- 1.431289599114434 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -4.18    | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 344000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.1803   | \n",
      " |    critic_loss     | 1.2951   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 343899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=345000, episode_reward=1.44 +/- 1.6415554245250568 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 1.44     | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 345000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.0851   | \n",
      " |    critic_loss     | 0.7713   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 344899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=346000, episode_reward=7.06 +/- 1.326964792455391 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 7.06     | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 346000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.5021   | \n",
      " |    critic_loss     | 1.3755   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 345899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=347000, episode_reward=6.44 +/- 1.6975322741203795 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 6.44     | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 347000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.8574   | \n",
      " |    critic_loss     | 0.9643   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 346899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=348000, episode_reward=5.84 +/- 1.879430663514376 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 5.84     | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 348000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.6036   | \n",
      " |    critic_loss     | 1.6509   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 347899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=349000, episode_reward=5.23 +/- 1.5115661279879575 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 5.23     | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 349000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.8712   | \n",
      " |    critic_loss     | 0.7178   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 348899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=350000, episode_reward=4.64 +/- 1.587660335169859 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 4.64     | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 350000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.7958   | \n",
      " |    critic_loss     | 1.1990   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 349899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=351000, episode_reward=4.04 +/- 1.9997307872371701 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 4.04     | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 351000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.6220   | \n",
      " |    critic_loss     | 0.9055   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 350899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=352000, episode_reward=4.64 +/- 1.2319924337036317 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 4.64     | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 352000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.2272   | \n",
      " |    critic_loss     | 1.3068   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 351899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=353000, episode_reward=5.25 +/- 1.741240619302879 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 5.25     | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 353000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.9888   | \n",
      " |    critic_loss     | 1.2472   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 352899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=354000, episode_reward=5.86 +/- 1.603146654978599 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 5.86     | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 354000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.5270   | \n",
      " |    critic_loss     | 1.1318   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 353899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=355000, episode_reward=6.48 +/- 1.0150735853601913 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 6.48     | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 355000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.4612   | \n",
      " |    critic_loss     | 1.6153   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 354899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=356000, episode_reward=7.1 +/- 1.220791673324808 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 7.1      | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 356000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.8269   | \n",
      " |    critic_loss     | 1.2067   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 355899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=357000, episode_reward=7.37 +/- 1.446662579609903 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 7.37     | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 357000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.0829   | \n",
      " |    critic_loss     | 0.7707   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 356899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=358000, episode_reward=7.64 +/- 1.4396209636733328 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 7.64     | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 358000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.9875   | \n",
      " |    critic_loss     | 0.7469   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 357899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=359000, episode_reward=7.91 +/- 1.2430527669768656 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 7.91     | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 359000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.9693   | \n",
      " |    critic_loss     | 1.4923   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 358899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=360000, episode_reward=8.19 +/- 1.656288471110246 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 8.19     | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 360000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.8430   | \n",
      " |    critic_loss     | 0.9608   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 359899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=361000, episode_reward=8.48 +/- 1.7161904973612359 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 8.48     | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 361000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.7835   | \n",
      " |    critic_loss     | 0.6959   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 360899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=362000, episode_reward=10.28 +/- 1.5878571422511902 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 10.28    | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 362000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.5261   | \n",
      " |    critic_loss     | 1.6315   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 361899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=363000, episode_reward=12.07 +/- 1.2176026204107733 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 12.07    | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 363000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.7485   | \n",
      " |    critic_loss     | 0.9371   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 362899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=364000, episode_reward=13.88 +/- 1.6190267883634277 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 13.88    | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 364000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.9945   | \n",
      " |    critic_loss     | 0.9986   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 363899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=365000, episode_reward=15.69 +/- 1.2210303538256293 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 15.69    | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 365000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.7302   | \n",
      " |    critic_loss     | 0.9325   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 364899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=366000, episode_reward=17.5 +/- 1.7042079820414924 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 17.5     | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 366000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.4448   | \n",
      " |    critic_loss     | 1.3612   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 365899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=367000, episode_reward=19.71 +/- 1.5910016624467582 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 19.71    | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 367000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.3787   | \n",
      " |    critic_loss     | 1.0947   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 366899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=368000, episode_reward=21.92 +/- 1.413426458798164 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 21.92    | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 368000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.1083   | \n",
      " |    critic_loss     | 1.5271   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 367899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=369000, episode_reward=24.14 +/- 1.5396391763208956 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 24.14    | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 369000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.0234   | \n",
      " |    critic_loss     | 1.5058   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 368899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=370000, episode_reward=26.36 +/- 1.8634914437144916 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 26.36    | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 370000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.5095   | \n",
      " |    critic_loss     | 1.1274   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 369899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=371000, episode_reward=28.58 +/- 1.9311246921933307 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 28.58    | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 371000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.7799   | \n",
      " |    critic_loss     | 1.1950   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 370899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=372000, episode_reward=19.49 +/- 1.2125180595969436 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 19.49    | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 372000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.9764   | \n",
      " |    critic_loss     | 1.2441   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 371899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=373000, episode_reward=10.41 +/- 1.1177156134741957 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 10.41    | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 373000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.5629   | \n",
      " |    critic_loss     | 0.8907   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 372899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=374000, episode_reward=1.32 +/- 1.5639634891854137 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 1.32     | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 374000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.7745   | \n",
      " |    critic_loss     | 0.6936   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 373899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=375000, episode_reward=-7.75 +/- 1.5154994984017387 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -7.75    | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 375000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.7410   | \n",
      " |    critic_loss     | 1.1852   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 374899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=376000, episode_reward=-16.83 +/- 1.8374664926561866 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -16.83   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 376000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.7129   | \n",
      " |    critic_loss     | 1.4282   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 375899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=377000, episode_reward=-6.12 +/- 1.8166986939540022 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -6.12    | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 377000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.1754   | \n",
      " |    critic_loss     | 1.5439   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 376899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=378000, episode_reward=4.58 +/- 1.4914787322443983 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 4.58     | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 378000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.5341   | \n",
      " |    critic_loss     | 1.3835   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 377899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=379000, episode_reward=15.29 +/- 1.4605515555140518 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 15.29    | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 379000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.5844   | \n",
      " |    critic_loss     | 0.8961   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 378899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=380000, episode_reward=26.01 +/- 1.1374467133852284 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 26.01    | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 380000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.5544   | \n",
      " |    critic_loss     | 1.1386   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 379899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=381000, episode_reward=36.73 +/- 1.7883329887479886 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 36.73    | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 381000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.6209   | \n",
      " |    critic_loss     | 1.1552   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 380899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=382000, episode_reward=28.07 +/- 1.6077954807661268 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 28.07    | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 382000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.7492   | \n",
      " |    critic_loss     | 0.6873   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 381899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=383000, episode_reward=19.41 +/- 1.7395316263187783 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 19.41    | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 383000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.6313   | \n",
      " |    critic_loss     | 0.6578   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 382899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=384000, episode_reward=10.76 +/- 1.5513319421711287 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 10.76    | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 384000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.6401   | \n",
      " |    critic_loss     | 0.9100   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 383899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=385000, episode_reward=2.12 +/- 1.4397388860285376 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 2.12     | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 385000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.9719   | \n",
      " |    critic_loss     | 0.9930   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 384899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=386000, episode_reward=-6.52 +/- 1.3996430126507813 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -6.52    | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 386000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.9238   | \n",
      " |    critic_loss     | 0.7310   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 385899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=387000, episode_reward=-1.95 +/- 1.7901890450687679 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | -1.95    | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 387000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.7283   | \n",
      " |    critic_loss     | 1.4321   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 386899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=388000, episode_reward=2.64 +/- 1.3704830023644132 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 2.64     | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 388000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.6098   | \n",
      " |    critic_loss     | 1.1524   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 387899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=389000, episode_reward=7.22 +/- 1.6287292847453967 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 7.22     | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 389000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.3100   | \n",
      " |    critic_loss     | 0.8275   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 388899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=390000, episode_reward=11.81 +/- 1.7189334780572991 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 11.81    | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 390000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.6396   | \n",
      " |    critic_loss     | 0.9099   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 389899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=391000, episode_reward=16.41 +/- 1.9142432356988266 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 16.41    | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 391000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.8699   | \n",
      " |    critic_loss     | 1.2175   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 390899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=392000, episode_reward=20.53 +/- 1.445758146802703 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 20.53    | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 392000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.7178   | \n",
      " |    critic_loss     | 1.4295   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 391899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=393000, episode_reward=24.65 +/- 1.0626310983779577 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 24.65    | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 393000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.9661   | \n",
      " |    critic_loss     | 1.2415   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 392899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=394000, episode_reward=28.78 +/- 1.1754454345714545 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 28.78    | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 394000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.5839   | \n",
      " |    critic_loss     | 0.8960   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 393899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=395000, episode_reward=32.91 +/- 1.9244288032338386 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 32.91    | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 395000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.5737   | \n",
      " |    critic_loss     | 1.1434   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 394899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=396000, episode_reward=37.05 +/- 1.3659244277813654 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 37.05    | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 396000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.5959   | \n",
      " |    critic_loss     | 1.3990   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 395899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=397000, episode_reward=37.08 +/- 1.0931659144015995 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 37.08    | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 397000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.0657   | \n",
      " |    critic_loss     | 1.2664   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 396899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=398000, episode_reward=37.12 +/- 1.08613095143863 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 37.12    | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 398000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.3221   | \n",
      " |    critic_loss     | 1.3305   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 397899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=399000, episode_reward=37.16 +/- 1.241569825248384 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 37.16    | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 399000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.6586   | \n",
      " |    critic_loss     | 0.9147   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 398899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=400000, episode_reward=37.2 +/- 1.1298728573095462 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 37.2     | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 400000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.3698   | \n",
      " |    critic_loss     | 0.8425   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 399899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=401000, episode_reward=37.25 +/- 1.1844621499460872 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 37.25    | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 401000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.3929   | \n",
      " |    critic_loss     | 0.8482   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 400899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=402000, episode_reward=41.25 +/- 1.9062242602449153 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 41.25    | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 402000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.7207   | \n",
      " |    critic_loss     | 0.6802   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 401899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=403000, episode_reward=45.26 +/- 1.8936636915783218 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 45.26    | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 403000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.6430   | \n",
      " |    critic_loss     | 1.4107   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 402899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=404000, episode_reward=49.28 +/- 1.6871851866167935 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 49.28    | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 404000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.2847   | \n",
      " |    critic_loss     | 1.0712   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 403899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=405000, episode_reward=53.3 +/- 1.6271570970301825 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 53.3     | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 405000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.3787   | \n",
      " |    critic_loss     | 0.8447   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 404899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=406000, episode_reward=57.32 +/- 1.1486344427783395 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 57.32    | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 406000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.0762   | \n",
      " |    critic_loss     | 1.2690   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 405899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=407000, episode_reward=56.01 +/- 1.3527994668176506 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 56.01    | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 407000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.3235   | \n",
      " |    critic_loss     | 1.5809   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 406899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=408000, episode_reward=54.71 +/- 1.1007554322926885 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 54.71    | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 408000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.4417   | \n",
      " |    critic_loss     | 1.1104   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 407899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=409000, episode_reward=53.41 +/- 1.3881363920753667 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 53.41    | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 409000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.4642   | \n",
      " |    critic_loss     | 1.3660   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 408899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=410000, episode_reward=52.12 +/- 1.627072051316575 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 52.12    | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 410000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.8739   | \n",
      " |    critic_loss     | 0.7185   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 409899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=411000, episode_reward=50.83 +/- 1.872115973355839 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 50.83    | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 411000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.0744   | \n",
      " |    critic_loss     | 1.2686   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 410899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=412000, episode_reward=57.96 +/- 1.2859610494915716 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 57.96    | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 412000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.0933   | \n",
      " |    critic_loss     | 1.2733   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 411899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=413000, episode_reward=65.11 +/- 1.1135927951162408 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 65.11    | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 413000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.3548   | \n",
      " |    critic_loss     | 1.3387   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 412899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=414000, episode_reward=72.25 +/- 1.9934730953249034 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 72.25    | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 414000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.9976   | \n",
      " |    critic_loss     | 0.9994   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 413899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=415000, episode_reward=79.4 +/- 1.2171627317684819 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 79.4     | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 415000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.1605   | \n",
      " |    critic_loss     | 1.5401   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 414899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=416000, episode_reward=86.56 +/- 1.2381999953248788 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 86.56    | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 416000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.6244   | \n",
      " |    critic_loss     | 1.4061   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 415899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=417000, episode_reward=82.02 +/- 1.812569818934577 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 82.02    | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 417000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.5194   | \n",
      " |    critic_loss     | 0.8799   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 416899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=418000, episode_reward=77.49 +/- 1.5637951751013457 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 77.49    | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 418000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.8534   | \n",
      " |    critic_loss     | 0.7133   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 417899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=419000, episode_reward=72.96 +/- 1.4468283921256435 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 72.96    | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 419000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.2659   | \n",
      " |    critic_loss     | 1.0665   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 418899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=420000, episode_reward=68.44 +/- 1.7167579060030667 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 68.44    | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 420000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.6787   | \n",
      " |    critic_loss     | 0.9197   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 419899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=421000, episode_reward=63.92 +/- 1.1091829179211867 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 63.92    | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 421000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.7269   | \n",
      " |    critic_loss     | 1.1817   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 420899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=422000, episode_reward=70.08 +/- 1.4915321486173307 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 70.08    | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 422000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.1385   | \n",
      " |    critic_loss     | 0.7846   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 421899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=423000, episode_reward=76.25 +/- 1.0049949773719034 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 76.25    | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 423000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.9711   | \n",
      " |    critic_loss     | 1.4928   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 422899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=424000, episode_reward=82.43 +/- 1.6542035060317084 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 82.43    | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 424000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.7555   | \n",
      " |    critic_loss     | 1.4389   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 423899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=425000, episode_reward=88.6 +/- 1.6395389257865802 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 88.6     | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 425000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.6874   | \n",
      " |    critic_loss     | 1.1718   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 424899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=426000, episode_reward=94.78 +/- 1.5207806181283212 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 94.78    | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 426000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.0949   | \n",
      " |    critic_loss     | 1.2737   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 425899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=427000, episode_reward=103.39 +/- 1.9724384249392934 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 103.39   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 427000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.6047   | \n",
      " |    critic_loss     | 1.4012   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 426899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=428000, episode_reward=112.0 +/- 1.0828311803425166 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 112.0    | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 428000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.8575   | \n",
      " |    critic_loss     | 1.2144   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 427899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=429000, episode_reward=120.61 +/- 1.508328731410489 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 120.61   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 429000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.0433   | \n",
      " |    critic_loss     | 1.2608   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 428899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=430000, episode_reward=129.23 +/- 1.4809084258370857 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 129.23   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 430000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.6083   | \n",
      " |    critic_loss     | 0.9021   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 429899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=431000, episode_reward=137.85 +/- 1.0364628359245447 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 137.85   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 431000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.0070   | \n",
      " |    critic_loss     | 1.5017   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 430899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=432000, episode_reward=135.31 +/- 1.0160800848891944 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 135.31   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 432000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.1392   | \n",
      " |    critic_loss     | 1.5348   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 431899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=433000, episode_reward=132.78 +/- 1.5439780293026006 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 132.78   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 433000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.5307   | \n",
      " |    critic_loss     | 0.6327   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 432899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=434000, episode_reward=130.24 +/- 1.4865293804877555 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 130.24   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 434000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.4815   | \n",
      " |    critic_loss     | 0.6204   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 433899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=435000, episode_reward=127.72 +/- 1.0628270579338364 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 127.72   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 435000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.6099   | \n",
      " |    critic_loss     | 0.6525   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 434899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=436000, episode_reward=125.19 +/- 1.2703386163813373 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 125.19   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 436000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.9055   | \n",
      " |    critic_loss     | 0.7264   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 435899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=437000, episode_reward=127.69 +/- 1.6506184128984582 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 127.69   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 437000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.2610   | \n",
      " |    critic_loss     | 0.8153   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 436899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=438000, episode_reward=130.19 +/- 1.3475278012388063 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 130.19   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 438000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.4548   | \n",
      " |    critic_loss     | 1.3637   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 437899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=439000, episode_reward=132.69 +/- 1.2983671509491614 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 132.69   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 439000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.6953   | \n",
      " |    critic_loss     | 1.1738   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 438899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=440000, episode_reward=135.2 +/- 1.2924047036969872 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 135.2    | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 440000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.8740   | \n",
      " |    critic_loss     | 0.7185   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 439899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=441000, episode_reward=137.71 +/- 1.568319209624463 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 137.71   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 441000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.5086   | \n",
      " |    critic_loss     | 1.1271   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 440899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=442000, episode_reward=141.8 +/- 1.0017940810722967 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 141.8    | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 442000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.7990   | \n",
      " |    critic_loss     | 1.1998   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 441899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=443000, episode_reward=145.9 +/- 1.8756255560931852 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 145.9    | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 443000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.7679   | \n",
      " |    critic_loss     | 1.4420   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 442899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=444000, episode_reward=150.0 +/- 1.3270963319504248 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 150.0    | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 444000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.2777   | \n",
      " |    critic_loss     | 1.0694   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 443899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=445000, episode_reward=154.1 +/- 1.9949470653347787 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 154.1    | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 445000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.6927   | \n",
      " |    critic_loss     | 0.9232   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 444899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=446000, episode_reward=158.21 +/- 1.524426913445283 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 158.21   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 446000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.4020   | \n",
      " |    critic_loss     | 1.3505   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 445899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=447000, episode_reward=157.79 +/- 1.1926939839978128 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 157.79   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 447000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.7535   | \n",
      " |    critic_loss     | 1.1884   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 446899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=448000, episode_reward=157.36 +/- 1.9369974843429456 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 157.36   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 448000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.8986   | \n",
      " |    critic_loss     | 0.9746   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 447899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=449000, episode_reward=156.94 +/- 1.3076034777456331 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 156.94   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 449000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.0863   | \n",
      " |    critic_loss     | 1.0216   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 448899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=450000, episode_reward=156.52 +/- 1.6961296214773807 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 156.52   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 450000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.7160   | \n",
      " |    critic_loss     | 0.9290   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 449899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=451000, episode_reward=156.11 +/- 1.5288666743435422 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 156.11   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 451000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.2108   | \n",
      " |    critic_loss     | 1.0527   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 450899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=452000, episode_reward=157.86 +/- 1.0807554603970533 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 157.86   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 452000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.1003   | \n",
      " |    critic_loss     | 1.2751   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 451899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=453000, episode_reward=159.62 +/- 1.1316052259599345 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 159.62   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 453000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.4057   | \n",
      " |    critic_loss     | 1.1014   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 452899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=454000, episode_reward=161.37 +/- 1.1113551743664858 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 161.37   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 454000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.3372   | \n",
      " |    critic_loss     | 1.0843   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 453899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=455000, episode_reward=163.13 +/- 1.126047265734376 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 163.13   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 455000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.3551   | \n",
      " |    critic_loss     | 1.3388   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 454899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=456000, episode_reward=164.9 +/- 1.2654438085893442 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 164.9    | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 456000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.0413   | \n",
      " |    critic_loss     | 0.7603   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 455899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=457000, episode_reward=159.99 +/- 1.575502811115732 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 159.99   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 457000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.7715   | \n",
      " |    critic_loss     | 1.4429   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 456899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=458000, episode_reward=155.08 +/- 1.9200887524432348 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 155.08   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 458000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.7014   | \n",
      " |    critic_loss     | 1.1753   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 457899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=459000, episode_reward=150.18 +/- 1.7721934541764046 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 150.18   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 459000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.2387   | \n",
      " |    critic_loss     | 1.3097   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 458899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=460000, episode_reward=145.28 +/- 1.5338234677688576 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 145.28   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 460000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.6295   | \n",
      " |    critic_loss     | 0.6574   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 459899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=461000, episode_reward=140.38 +/- 1.1973610316240717 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 140.38   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 461000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.0581   | \n",
      " |    critic_loss     | 1.2645   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 460899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=462000, episode_reward=140.24 +/- 1.318674843191812 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 140.24   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 462000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.4882   | \n",
      " |    critic_loss     | 0.6220   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 461899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=463000, episode_reward=140.11 +/- 1.9865682527796698 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 140.11   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 463000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.6709   | \n",
      " |    critic_loss     | 0.6677   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 462899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=464000, episode_reward=139.98 +/- 1.8199387734747556 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 139.98   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 464000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.2719   | \n",
      " |    critic_loss     | 1.3180   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 463899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=465000, episode_reward=139.85 +/- 1.3505367120322185 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 139.85   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 465000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.8676   | \n",
      " |    critic_loss     | 1.4669   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 464899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=466000, episode_reward=139.72 +/- 1.018050920094072 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 139.72   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 466000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.7274   | \n",
      " |    critic_loss     | 1.1818   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 465899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=467000, episode_reward=143.12 +/- 1.7515488272714377 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 143.12   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 467000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.5174   | \n",
      " |    critic_loss     | 0.6294   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 466899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=468000, episode_reward=146.52 +/- 1.8483143255562058 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 146.52   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 468000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.6699   | \n",
      " |    critic_loss     | 1.4175   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 467899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=469000, episode_reward=149.92 +/- 1.3307838299002253 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 149.92   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 469000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.6538   | \n",
      " |    critic_loss     | 0.9135   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 468899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=470000, episode_reward=153.33 +/- 1.1764159379345833 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 153.33   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 470000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.5670   | \n",
      " |    critic_loss     | 1.3917   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 469899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=471000, episode_reward=156.73 +/- 1.0569093979489312 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 156.73   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 471000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.3965   | \n",
      " |    critic_loss     | 0.8491   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 470899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=472000, episode_reward=155.47 +/- 1.037309178435052 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 155.47   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 472000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.1923   | \n",
      " |    critic_loss     | 0.5481   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 471899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=473000, episode_reward=154.2 +/- 1.3108255426184403 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 154.2    | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 473000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.5562   | \n",
      " |    critic_loss     | 0.6391   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 472899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=474000, episode_reward=152.94 +/- 1.850379460540026 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 152.94   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 474000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.2149   | \n",
      " |    critic_loss     | 1.0537   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 473899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=475000, episode_reward=151.68 +/- 1.0200550894230855 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 151.68   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 475000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.7584   | \n",
      " |    critic_loss     | 0.9396   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 474899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=476000, episode_reward=150.42 +/- 1.3119960769307926 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 150.42   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 476000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.1202   | \n",
      " |    critic_loss     | 0.7800   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 475899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=477000, episode_reward=157.63 +/- 1.0750504826237068 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 157.63   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 477000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.6426   | \n",
      " |    critic_loss     | 1.1607   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 476899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=478000, episode_reward=164.84 +/- 1.835927869438732 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 164.84   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 478000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.3576   | \n",
      " |    critic_loss     | 1.3394   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 477899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=479000, episode_reward=172.05 +/- 1.99457808965684 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 172.05   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 479000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.3794   | \n",
      " |    critic_loss     | 1.3449   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 478899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=480000, episode_reward=179.26 +/- 1.942963889281831 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 179.26   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 480000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.1554   | \n",
      " |    critic_loss     | 0.7888   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 479899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=481000, episode_reward=186.47 +/- 1.4229473246831859 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 186.47   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 481000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.7185   | \n",
      " |    critic_loss     | 0.9296   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 480899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=482000, episode_reward=178.71 +/- 1.8292448799876442 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 178.71   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 482000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.5362   | \n",
      " |    critic_loss     | 1.1341   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 481899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=483000, episode_reward=170.95 +/- 1.8485334260118862 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 170.95   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 483000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.8648   | \n",
      " |    critic_loss     | 0.7162   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 482899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=484000, episode_reward=163.19 +/- 1.08025420059527 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 163.19   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 484000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.0668   | \n",
      " |    critic_loss     | 1.5167   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 483899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=485000, episode_reward=155.44 +/- 1.6809912035930212 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 155.44   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 485000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.8770   | \n",
      " |    critic_loss     | 1.2193   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 484899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=486000, episode_reward=147.68 +/- 1.8962474386795098 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 147.68   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 486000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.8299   | \n",
      " |    critic_loss     | 0.7075   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 485899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=487000, episode_reward=151.51 +/- 1.6684222771227921 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 151.51   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 487000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.1760   | \n",
      " |    critic_loss     | 0.5440   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 486899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=488000, episode_reward=155.34 +/- 1.0828605719453357 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 155.34   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 488000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.1192   | \n",
      " |    critic_loss     | 0.7798   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 487899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=489000, episode_reward=159.17 +/- 1.5262812890487631 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 159.17   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 489000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.1179   | \n",
      " |    critic_loss     | 0.7795   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 488899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=490000, episode_reward=163.01 +/- 1.160526436451377 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 163.01   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 490000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.5724   | \n",
      " |    critic_loss     | 0.8931   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 489899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=491000, episode_reward=166.84 +/- 1.4292059140085682 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 166.84   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 491000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.5352   | \n",
      " |    critic_loss     | 1.1338   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 490899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=492000, episode_reward=160.79 +/- 1.0688590217017584 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 160.79   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 492000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.7855   | \n",
      " |    critic_loss     | 0.6964   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 491899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=493000, episode_reward=154.73 +/- 1.0641561054859547 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 154.73   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 493000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.2470   | \n",
      " |    critic_loss     | 1.3117   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 492899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=494000, episode_reward=148.68 +/- 1.9910334587674097 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 148.68   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 494000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.4713   | \n",
      " |    critic_loss     | 1.1178   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 493899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=495000, episode_reward=142.63 +/- 1.5611536420273295 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 142.63   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 495000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.6940   | \n",
      " |    critic_loss     | 0.9235   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 494899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=496000, episode_reward=136.58 +/- 1.5891430676143512 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 136.58   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 496000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.3674   | \n",
      " |    critic_loss     | 1.0918   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 495899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=497000, episode_reward=150.57 +/- 1.2531034609078104 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 150.57   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 497000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.8928   | \n",
      " |    critic_loss     | 0.9732   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 496899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=498000, episode_reward=164.57 +/- 1.9495089035818953 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 164.57   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 498000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.8424   | \n",
      " |    critic_loss     | 0.9606   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 497899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=499000, episode_reward=178.56 +/- 1.894686554997374 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 178.56   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 499000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.1823   | \n",
      " |    critic_loss     | 1.0456   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 498899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=500000, episode_reward=192.56 +/- 1.7411112958014734 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 192.56   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 500000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.0566   | \n",
      " |    critic_loss     | 1.2641   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 499899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=501000, episode_reward=206.55 +/- 1.5987594729157126 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 206.55   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 501000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.1625   | \n",
      " |    critic_loss     | 0.7906   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 500899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=502000, episode_reward=200.23 +/- 1.178976281040446 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 200.23   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 502000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.0552   | \n",
      " |    critic_loss     | 0.7638   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 501899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=503000, episode_reward=193.9 +/- 1.8580139293771154 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 193.9    | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 503000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.4114   | \n",
      " |    critic_loss     | 0.6028   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 502899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=504000, episode_reward=187.57 +/- 1.1310499585691698 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 187.57   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 504000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.2519   | \n",
      " |    critic_loss     | 1.3130   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 503899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=505000, episode_reward=181.24 +/- 1.1579831117834072 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 181.24   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 505000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.3470   | \n",
      " |    critic_loss     | 1.0868   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 504899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=506000, episode_reward=174.92 +/- 1.5203679569653716 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 174.92   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 506000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.4159   | \n",
      " |    critic_loss     | 1.1040   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 505899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=507000, episode_reward=174.12 +/- 1.2145273077034648 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 174.12   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 507000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.0577   | \n",
      " |    critic_loss     | 0.7644   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 506899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=508000, episode_reward=173.32 +/- 1.886297265701008 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 173.32   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 508000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.9740   | \n",
      " |    critic_loss     | 0.9935   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 507899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=509000, episode_reward=172.52 +/- 1.0762442180225547 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 172.52   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 509000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.3343   | \n",
      " |    critic_loss     | 1.3336   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 508899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=510000, episode_reward=171.72 +/- 1.5341665860527387 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 171.72   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 510000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.3904   | \n",
      " |    critic_loss     | 0.8476   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 509899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=511000, episode_reward=170.91 +/- 1.5675312504786225 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 170.91   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 511000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.2414   | \n",
      " |    critic_loss     | 0.8103   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 510899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=512000, episode_reward=167.57 +/- 1.9055806367004489 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 167.57   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 512000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.6740   | \n",
      " |    critic_loss     | 1.4185   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 511899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=513000, episode_reward=164.23 +/- 1.2923281674831926 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 164.23   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 513000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.5118   | \n",
      " |    critic_loss     | 1.3779   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 512899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=514000, episode_reward=160.89 +/- 1.3749709423741 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 160.89   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 514000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.7658   | \n",
      " |    critic_loss     | 0.9414   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 513899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=515000, episode_reward=157.55 +/- 1.9785703214233883 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 157.55   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 515000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.6539   | \n",
      " |    critic_loss     | 0.6635   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 514899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=516000, episode_reward=154.2 +/- 1.173049913272087 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 154.2    | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 516000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.2435   | \n",
      " |    critic_loss     | 0.8109   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 515899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=517000, episode_reward=153.12 +/- 1.795160131449446 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 153.12   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 517000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.6324   | \n",
      " |    critic_loss     | 1.4081   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 516899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=518000, episode_reward=152.05 +/- 1.8083938420292087 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 152.05   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 518000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.0533   | \n",
      " |    critic_loss     | 1.2633   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 517899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=519000, episode_reward=150.97 +/- 1.6794137081814162 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 150.97   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 519000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.8183   | \n",
      " |    critic_loss     | 1.2046   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 518899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=520000, episode_reward=149.88 +/- 1.0339475371244309 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 149.88   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 520000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.4279   | \n",
      " |    critic_loss     | 0.6070   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 519899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=521000, episode_reward=148.8 +/- 1.3328404992567071 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 148.8    | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 521000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.2938   | \n",
      " |    critic_loss     | 1.3235   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 520899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=522000, episode_reward=160.82 +/- 1.2444412090348211 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 160.82   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 522000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.9831   | \n",
      " |    critic_loss     | 1.2458   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 521899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=523000, episode_reward=172.84 +/- 1.9679482726256468 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 172.84   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 523000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.2811   | \n",
      " |    critic_loss     | 0.8203   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 522899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=524000, episode_reward=184.85 +/- 1.2674202159704206 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 184.85   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 524000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.9975   | \n",
      " |    critic_loss     | 1.2494   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 523899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=525000, episode_reward=196.87 +/- 1.0843827560421808 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 196.87   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 525000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.3642   | \n",
      " |    critic_loss     | 0.5911   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 524899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=526000, episode_reward=208.88 +/- 1.2613450081592932 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 208.88   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 526000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.1436   | \n",
      " |    critic_loss     | 0.7859   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 525899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=527000, episode_reward=202.93 +/- 1.9434048092279093 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 202.93   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 527000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.6130   | \n",
      " |    critic_loss     | 0.6532   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 526899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=528000, episode_reward=196.98 +/- 1.2631790620877066 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 196.98   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 528000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.6198   | \n",
      " |    critic_loss     | 1.1549   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 527899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=529000, episode_reward=191.03 +/- 1.7429903526087027 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 191.03   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 529000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.2527   | \n",
      " |    critic_loss     | 0.5632   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 528899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=530000, episode_reward=185.07 +/- 1.7729142490231729 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 185.07   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 530000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.0490   | \n",
      " |    critic_loss     | 1.2623   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 529899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=531000, episode_reward=179.12 +/- 1.222820203811692 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 179.12   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 531000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.0708   | \n",
      " |    critic_loss     | 0.7677   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 530899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=532000, episode_reward=177.49 +/- 1.5738156440119744 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 177.49   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 532000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.0602   | \n",
      " |    critic_loss     | 0.5151   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 531899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=533000, episode_reward=175.87 +/- 1.3253069214642759 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 175.87   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 533000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.1829   | \n",
      " |    critic_loss     | 0.5457   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 532899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=534000, episode_reward=174.24 +/- 1.7340886334819021 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 174.24   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 534000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.8292   | \n",
      " |    critic_loss     | 0.9573   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 533899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=535000, episode_reward=172.6 +/- 1.1090094711809715 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 172.6    | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 535000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.2382   | \n",
      " |    critic_loss     | 0.5596   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 534899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=536000, episode_reward=170.97 +/- 1.8712227035093825 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 170.97   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 536000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.1718   | \n",
      " |    critic_loss     | 1.0430   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 535899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=537000, episode_reward=169.52 +/- 1.1204583081038115 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 169.52   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 537000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.8937   | \n",
      " |    critic_loss     | 0.7234   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 536899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=538000, episode_reward=168.07 +/- 1.9912549484456863 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 168.07   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 538000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.7820   | \n",
      " |    critic_loss     | 0.6955   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 537899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=539000, episode_reward=166.61 +/- 1.697655097091705 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 166.61   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 539000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.0526   | \n",
      " |    critic_loss     | 0.5131   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 538899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=540000, episode_reward=165.16 +/- 1.6693267939087353 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 165.16   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 540000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.6417   | \n",
      " |    critic_loss     | 1.4104   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 539899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=541000, episode_reward=163.7 +/- 1.0448305971460656 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 163.7    | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 541000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.0341   | \n",
      " |    critic_loss     | 1.0085   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 540899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=542000, episode_reward=167.97 +/- 1.1191972178679004 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 167.97   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 542000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.0851   | \n",
      " |    critic_loss     | 0.5213   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 541899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=543000, episode_reward=172.23 +/- 1.9936652490190547 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 172.23   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 543000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.3464   | \n",
      " |    critic_loss     | 0.8366   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 542899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=544000, episode_reward=176.49 +/- 1.0751712096022263 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 176.49   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 544000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.4757   | \n",
      " |    critic_loss     | 0.6189   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 543899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=545000, episode_reward=180.75 +/- 1.4752661874676807 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 180.75   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 545000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.0047   | \n",
      " |    critic_loss     | 0.5012   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 544899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=546000, episode_reward=185.01 +/- 1.2813837597153068 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 185.01   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 546000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.6106   | \n",
      " |    critic_loss     | 0.9027   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 545899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=547000, episode_reward=188.0 +/- 1.1963021323312706 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 188.0    | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 547000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.2545   | \n",
      " |    critic_loss     | 1.3136   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 546899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=548000, episode_reward=190.98 +/- 1.7135859786359848 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 190.98   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 548000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.7524   | \n",
      " |    critic_loss     | 1.4381   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 547899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=549000, episode_reward=193.96 +/- 1.2464495820133723 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 193.96   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 549000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.2523   | \n",
      " |    critic_loss     | 1.3131   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 548899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=550000, episode_reward=196.94 +/- 1.6188537381579038 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 196.94   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 550000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.2347   | \n",
      " |    critic_loss     | 1.3087   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 549899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=551000, episode_reward=199.91 +/- 1.457611864086351 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 199.91   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 551000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.8124   | \n",
      " |    critic_loss     | 0.7031   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 550899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=552000, episode_reward=203.34 +/- 1.7012532712960975 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 203.34   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 552000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.3950   | \n",
      " |    critic_loss     | 0.5988   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 551899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=553000, episode_reward=206.77 +/- 1.8482385722418377 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 206.77   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 553000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.8546   | \n",
      " |    critic_loss     | 1.2137   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 552899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=554000, episode_reward=210.19 +/- 1.548387972464913 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 210.19   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 554000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.2400   | \n",
      " |    critic_loss     | 0.5600   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 553899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=555000, episode_reward=213.61 +/- 1.0452578025758652 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 213.61   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 555000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.1249   | \n",
      " |    critic_loss     | 1.2812   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 554899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=556000, episode_reward=217.03 +/- 1.7410176918692901 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 217.03   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 556000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.0826   | \n",
      " |    critic_loss     | 0.7707   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 555899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=557000, episode_reward=216.34 +/- 1.5707921570911294 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 216.34   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 557000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.8514   | \n",
      " |    critic_loss     | 0.7128   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 556899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=558000, episode_reward=215.65 +/- 1.7906966592768385 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 215.65   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 558000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.0829   | \n",
      " |    critic_loss     | 1.2707   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 557899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=559000, episode_reward=214.96 +/- 1.1393392875301824 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 214.96   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 559000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.6407   | \n",
      " |    critic_loss     | 1.1602   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 558899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=560000, episode_reward=214.27 +/- 1.444011321197473 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 214.27   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 560000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.1824   | \n",
      " |    critic_loss     | 1.0456   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 559899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=561000, episode_reward=213.57 +/- 1.9358061974453094 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 213.57   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 561000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.2388   | \n",
      " |    critic_loss     | 0.8097   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 560899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=562000, episode_reward=212.92 +/- 1.4688398125467925 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 212.92   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 562000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.5046   | \n",
      " |    critic_loss     | 0.8762   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 561899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=563000, episode_reward=212.28 +/- 1.8014732500319477 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 212.28   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 563000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.2625   | \n",
      " |    critic_loss     | 0.8156   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 562899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=564000, episode_reward=211.63 +/- 1.404417055453977 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 211.63   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 564000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.2874   | \n",
      " |    critic_loss     | 1.0719   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 563899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=565000, episode_reward=210.97 +/- 1.3306243372347684 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 210.97   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 565000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.5632   | \n",
      " |    critic_loss     | 0.8908   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 564899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=566000, episode_reward=210.32 +/- 1.623265242616192 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 210.32   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 566000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.3893   | \n",
      " |    critic_loss     | 1.0973   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 565899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=567000, episode_reward=212.63 +/- 1.6023434183096668 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 212.63   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 567000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.8267   | \n",
      " |    critic_loss     | 0.7067   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 566899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=568000, episode_reward=214.95 +/- 1.992685615301998 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 214.95   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 568000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.6527   | \n",
      " |    critic_loss     | 1.1632   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 567899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=569000, episode_reward=217.26 +/- 1.6298315990098824 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 217.26   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 569000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.6052   | \n",
      " |    critic_loss     | 0.6513   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 568899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=570000, episode_reward=219.56 +/- 1.6671971411842845 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 219.56   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 570000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.7385   | \n",
      " |    critic_loss     | 1.1846   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 569899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=571000, episode_reward=221.86 +/- 1.4879686634157316 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 221.86   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 571000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.7673   | \n",
      " |    critic_loss     | 1.1918   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 570899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=572000, episode_reward=225.75 +/- 1.808787057739503 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 225.75   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 572000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.0829   | \n",
      " |    critic_loss     | 0.5207   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 571899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=573000, episode_reward=229.63 +/- 1.08156975129888 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 229.63   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 573000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.6381   | \n",
      " |    critic_loss     | 1.4095   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 572899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=574000, episode_reward=233.51 +/- 1.2091334960302302 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 233.51   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 574000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.6590   | \n",
      " |    critic_loss     | 0.6648   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 573899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=575000, episode_reward=237.39 +/- 1.5030187088010512 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 237.39   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 575000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.5701   | \n",
      " |    critic_loss     | 0.6425   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 574899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=576000, episode_reward=241.26 +/- 1.6845151255379402 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 241.26   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 576000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.3786   | \n",
      " |    critic_loss     | 1.3447   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 575899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=577000, episode_reward=234.3 +/- 1.8781792647743165 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 234.3    | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 577000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.5703   | \n",
      " |    critic_loss     | 0.6426   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 576899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=578000, episode_reward=227.33 +/- 1.8233918256136843 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 227.33   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 578000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.1099   | \n",
      " |    critic_loss     | 1.2775   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 577899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=579000, episode_reward=220.36 +/- 1.7936892074062354 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 220.36   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 579000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.3227   | \n",
      " |    critic_loss     | 1.3307   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 578899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=580000, episode_reward=213.39 +/- 1.6313339481098716 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 213.39   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 580000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.2875   | \n",
      " |    critic_loss     | 1.0719   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 579899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=581000, episode_reward=206.41 +/- 1.8608551074352289 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 206.41   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 581000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.1303   | \n",
      " |    critic_loss     | 0.5326   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 580899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=582000, episode_reward=209.35 +/- 1.2317936885376182 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 209.35   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 582000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.4356   | \n",
      " |    critic_loss     | 0.6089   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 581899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=583000, episode_reward=212.29 +/- 1.4374760151958177 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 212.29   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 583000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.4224   | \n",
      " |    critic_loss     | 1.1056   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 582899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=584000, episode_reward=215.22 +/- 1.2566956106641678 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 215.22   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 584000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.6283   | \n",
      " |    critic_loss     | 0.9071   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 583899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=585000, episode_reward=218.15 +/- 1.7065654380955593 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 218.15   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 585000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.0890   | \n",
      " |    critic_loss     | 1.0222   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 584899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=586000, episode_reward=221.08 +/- 1.5648641195193658 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 221.08   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 586000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.7377   | \n",
      " |    critic_loss     | 0.4344   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 585899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=587000, episode_reward=223.61 +/- 1.250087701339349 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 223.61   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 587000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.7009   | \n",
      " |    critic_loss     | 0.6752   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 586899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=588000, episode_reward=226.13 +/- 1.4403781767982093 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 226.13   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 588000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.1997   | \n",
      " |    critic_loss     | 1.2999   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 587899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=589000, episode_reward=228.66 +/- 1.8150721360044817 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 228.66   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 589000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.3250   | \n",
      " |    critic_loss     | 0.5812   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 588899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=590000, episode_reward=231.17 +/- 1.8956347224950258 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 231.17   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 590000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.7654   | \n",
      " |    critic_loss     | 0.6913   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 589899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=591000, episode_reward=233.69 +/- 1.2216029099560766 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 233.69   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 591000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.1936   | \n",
      " |    critic_loss     | 0.5484   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 590899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=592000, episode_reward=212.97 +/- 1.9889912672474896 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 212.97   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 592000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.9732   | \n",
      " |    critic_loss     | 0.7433   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 591899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=593000, episode_reward=192.25 +/- 1.9010596808388547 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 192.25   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 593000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.8052   | \n",
      " |    critic_loss     | 1.2013   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 592899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=594000, episode_reward=171.52 +/- 1.0370006854863263 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 171.52   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 594000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.3242   | \n",
      " |    critic_loss     | 1.0810   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 593899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=595000, episode_reward=150.79 +/- 1.4658729497123535 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 150.79   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 595000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.3045   | \n",
      " |    critic_loss     | 1.3261   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 594899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=596000, episode_reward=130.05 +/- 1.3734893701342692 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 130.05   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 596000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.6558   | \n",
      " |    critic_loss     | 1.1640   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 595899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=597000, episode_reward=154.54 +/- 1.274170075649705 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 154.54   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 597000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.3301   | \n",
      " |    critic_loss     | 1.3325   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 596899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=598000, episode_reward=179.02 +/- 1.9628072789090791 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 179.02   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 598000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.7959   | \n",
      " |    critic_loss     | 0.4490   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 597899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=599000, episode_reward=203.5 +/- 1.7196316555533346 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 203.5    | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 599000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.2916   | \n",
      " |    critic_loss     | 0.8229   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 598899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=600000, episode_reward=227.98 +/- 1.5375378705853777 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 227.98   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 600000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.0157   | \n",
      " |    critic_loss     | 0.5039   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 599899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=601000, episode_reward=252.45 +/- 1.7748598034609044 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 252.45   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 601000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.7586   | \n",
      " |    critic_loss     | 0.4396   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 600899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=602000, episode_reward=253.04 +/- 1.4863664045627498 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 253.04   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 602000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.9583   | \n",
      " |    critic_loss     | 0.4896   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 601899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=603000, episode_reward=253.63 +/- 1.8390440835695248 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 253.63   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 603000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.2837   | \n",
      " |    critic_loss     | 0.5709   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 602899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=604000, episode_reward=254.21 +/- 1.9974582571702444 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 254.21   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 604000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.0766   | \n",
      " |    critic_loss     | 1.2692   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 603899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=605000, episode_reward=254.79 +/- 1.524059762195038 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 254.79   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 605000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.0781   | \n",
      " |    critic_loss     | 0.7695   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 604899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=606000, episode_reward=255.37 +/- 1.3250489450175404 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 255.37   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 606000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.3068   | \n",
      " |    critic_loss     | 1.0767   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 605899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=607000, episode_reward=260.13 +/- 1.182759389950164 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 260.13   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 607000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.1375   | \n",
      " |    critic_loss     | 1.2844   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 606899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=608000, episode_reward=264.88 +/- 1.1505167594427714 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 264.88   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 608000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.2342   | \n",
      " |    critic_loss     | 0.8086   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 607899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=609000, episode_reward=269.63 +/- 1.0537566739926247 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 269.63   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 609000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.1851   | \n",
      " |    critic_loss     | 1.0463   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 608899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=610000, episode_reward=274.37 +/- 1.1419948892439673 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 274.37   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 610000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.1442   | \n",
      " |    critic_loss     | 0.5360   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 609899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=611000, episode_reward=279.11 +/- 1.416952260770638 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 279.11   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 611000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.5224   | \n",
      " |    critic_loss     | 0.6306   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 610899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=612000, episode_reward=271.95 +/- 1.349452248471397 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 271.95   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 612000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.8850   | \n",
      " |    critic_loss     | 0.7212   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 611899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=613000, episode_reward=264.8 +/- 1.8810400087401642 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 264.8    | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 613000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.5133   | \n",
      " |    critic_loss     | 0.6283   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 612899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=614000, episode_reward=257.64 +/- 1.0210551547426427 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 257.64   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 614000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.4261   | \n",
      " |    critic_loss     | 1.1065   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 613899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=615000, episode_reward=250.47 +/- 1.0046796820449844 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 250.47   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 615000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.2959   | \n",
      " |    critic_loss     | 1.3240   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 614899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=616000, episode_reward=243.3 +/- 1.5066241377884426 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 243.3    | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 616000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.1895   | \n",
      " |    critic_loss     | 0.7974   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 615899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=617000, episode_reward=252.6 +/- 1.2408504833396685 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 252.6    | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 617000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.5088   | \n",
      " |    critic_loss     | 0.8772   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 616899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=618000, episode_reward=261.89 +/- 1.7596509189898788 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 261.89   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 618000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.0891   | \n",
      " |    critic_loss     | 0.5223   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 617899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=619000, episode_reward=271.18 +/- 1.8687332732615278 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 271.18   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 619000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.3966   | \n",
      " |    critic_loss     | 0.5992   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 618899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=620000, episode_reward=280.46 +/- 1.7827888130728429 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 280.46   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 620000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.2377   | \n",
      " |    critic_loss     | 0.8094   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 619899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=621000, episode_reward=289.74 +/- 1.5820220838030834 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 289.74   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 621000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.0030   | \n",
      " |    critic_loss     | 0.5008   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 620899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=622000, episode_reward=286.35 +/- 1.1032804780385597 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 286.35   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 622000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.6485   | \n",
      " |    critic_loss     | 0.9121   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 621899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=623000, episode_reward=282.95 +/- 1.5141726462332854 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 282.95   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 623000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.5955   | \n",
      " |    critic_loss     | 0.8989   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 622899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=624000, episode_reward=279.54 +/- 1.6093003652156699 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 279.54   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 624000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.2711   | \n",
      " |    critic_loss     | 0.5678   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 623899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=625000, episode_reward=276.13 +/- 1.8486960843219244 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 276.13   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 625000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.1620   | \n",
      " |    critic_loss     | 1.2905   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 624899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=626000, episode_reward=272.71 +/- 1.0593536412688471 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 272.71   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 626000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.7634   | \n",
      " |    critic_loss     | 0.4408   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 625899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=627000, episode_reward=276.16 +/- 1.9184109277426646 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 276.16   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 627000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.8609   | \n",
      " |    critic_loss     | 0.7152   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 626899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=628000, episode_reward=279.6 +/- 1.9176862716567509 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 279.6    | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 628000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.7074   | \n",
      " |    critic_loss     | 0.9268   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 627899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=629000, episode_reward=283.03 +/- 1.712547787227783 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 283.03   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 629000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.8736   | \n",
      " |    critic_loss     | 0.9684   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 628899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=630000, episode_reward=286.46 +/- 1.1501468215519615 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 286.46   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 630000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.1907   | \n",
      " |    critic_loss     | 1.2977   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 629899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=631000, episode_reward=289.88 +/- 1.6076479705837978 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 289.88   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 631000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.4505   | \n",
      " |    critic_loss     | 1.1126   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 630899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=632000, episode_reward=267.22 +/- 1.2799353207228563 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 267.22   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 632000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.2053   | \n",
      " |    critic_loss     | 0.8013   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 631899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=633000, episode_reward=244.55 +/- 1.9215856071188435 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 244.55   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 633000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.4165   | \n",
      " |    critic_loss     | 0.6041   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 632899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=634000, episode_reward=221.87 +/- 1.4723715442000722 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 221.87   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 634000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.6789   | \n",
      " |    critic_loss     | 0.4197   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 633899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=635000, episode_reward=199.19 +/- 1.8023465067305737 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 199.19   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 635000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.7386   | \n",
      " |    critic_loss     | 1.1847   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 634899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=636000, episode_reward=176.51 +/- 1.936073039713748 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 176.51   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 636000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.7018   | \n",
      " |    critic_loss     | 0.9255   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 635899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=637000, episode_reward=194.44 +/- 1.4560004890370164 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 194.44   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 637000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.9278   | \n",
      " |    critic_loss     | 0.9820   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 636899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=638000, episode_reward=212.36 +/- 1.5073541266858235 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 212.36   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 638000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.7784   | \n",
      " |    critic_loss     | 0.6946   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 637899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=639000, episode_reward=230.28 +/- 1.4781851920928806 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 230.28   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 639000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.9962   | \n",
      " |    critic_loss     | 0.4991   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 638899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=640000, episode_reward=248.19 +/- 1.4622254563514834 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 248.19   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 640000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.2664   | \n",
      " |    critic_loss     | 0.8166   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 639899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=641000, episode_reward=266.1 +/- 1.422463332734534 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 266.1    | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 641000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.5265   | \n",
      " |    critic_loss     | 0.3816   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 640899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=642000, episode_reward=265.03 +/- 1.9273937111051977 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 265.03   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 642000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.7455   | \n",
      " |    critic_loss     | 0.9364   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 641899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=643000, episode_reward=263.95 +/- 1.7945933036588964 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 263.95   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 643000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.1466   | \n",
      " |    critic_loss     | 1.2867   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 642899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=644000, episode_reward=262.87 +/- 1.465152882678046 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 262.87   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 644000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.8055   | \n",
      " |    critic_loss     | 0.4514   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 643899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=645000, episode_reward=261.79 +/- 1.3949711192338627 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 261.79   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 645000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.5624   | \n",
      " |    critic_loss     | 0.6406   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 644899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=646000, episode_reward=260.69 +/- 1.8612718532242576 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 260.69   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 646000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.8385   | \n",
      " |    critic_loss     | 0.7096   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 645899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=647000, episode_reward=257.42 +/- 1.8762444343440206 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 257.42   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 647000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.8794   | \n",
      " |    critic_loss     | 0.9699   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 646899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=648000, episode_reward=254.15 +/- 1.6582551624261663 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 254.15   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 648000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.0075   | \n",
      " |    critic_loss     | 1.2519   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 647899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=649000, episode_reward=250.87 +/- 1.5051895632801446 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 250.87   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 649000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.9203   | \n",
      " |    critic_loss     | 0.4801   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 648899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=650000, episode_reward=247.58 +/- 1.6749214070506713 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 247.58   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 650000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.6862   | \n",
      " |    critic_loss     | 0.6716   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 649899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=651000, episode_reward=244.29 +/- 1.6444916087571466 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 244.29   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 651000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.4617   | \n",
      " |    critic_loss     | 0.3654   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 650899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=652000, episode_reward=246.48 +/- 1.6977212584515335 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 246.48   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 652000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.5282   | \n",
      " |    critic_loss     | 0.8821   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 651899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=653000, episode_reward=248.67 +/- 1.8450656177867217 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 248.67   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 653000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.9884   | \n",
      " |    critic_loss     | 0.7471   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 652899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=654000, episode_reward=250.85 +/- 1.1024209924022883 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 250.85   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 654000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.1105   | \n",
      " |    critic_loss     | 1.0276   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 653899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=655000, episode_reward=253.03 +/- 1.1034356880951819 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 253.03   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 655000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.8926   | \n",
      " |    critic_loss     | 0.9731   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 654899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=656000, episode_reward=255.2 +/- 1.1862494292327859 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 255.2    | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 656000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.2356   | \n",
      " |    critic_loss     | 1.0589   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 655899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=657000, episode_reward=256.09 +/- 1.5887453784529073 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 256.09   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 657000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.3162   | \n",
      " |    critic_loss     | 1.0791   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 656899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=658000, episode_reward=256.97 +/- 1.6783170065883348 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 256.97   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 658000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.9142   | \n",
      " |    critic_loss     | 1.2286   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 657899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=659000, episode_reward=257.85 +/- 1.4033132737102658 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 257.85   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 659000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.1137   | \n",
      " |    critic_loss     | 1.0284   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 658899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=660000, episode_reward=258.72 +/- 1.2556597510809238 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 258.72   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 660000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.3309   | \n",
      " |    critic_loss     | 0.8327   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 659899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=661000, episode_reward=259.59 +/- 1.8241254751844402 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 259.59   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 661000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.7017   | \n",
      " |    critic_loss     | 0.4254   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 660899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=662000, episode_reward=252.3 +/- 1.4326247812191601 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 252.3    | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 662000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.4433   | \n",
      " |    critic_loss     | 0.8608   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 661899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=663000, episode_reward=244.99 +/- 1.8819437040491624 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 244.99   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 663000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.6117   | \n",
      " |    critic_loss     | 0.9029   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 662899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=664000, episode_reward=237.69 +/- 1.9618545197693615 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 237.69   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 664000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.4732   | \n",
      " |    critic_loss     | 1.1183   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 663899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=665000, episode_reward=230.38 +/- 1.7182520448767349 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 230.38   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 665000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.2634   | \n",
      " |    critic_loss     | 0.5658   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 664899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=666000, episode_reward=223.06 +/- 1.3455594039516716 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 223.06   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 666000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.5827   | \n",
      " |    critic_loss     | 1.1457   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 665899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=667000, episode_reward=238.76 +/- 1.27488309235481 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 238.76   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 667000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.1180   | \n",
      " |    critic_loss     | 1.2795   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 666899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=668000, episode_reward=254.44 +/- 1.95327330491412 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 254.44   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 668000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.2248   | \n",
      " |    critic_loss     | 1.0562   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 667899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=669000, episode_reward=270.13 +/- 1.2651564265291828 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 270.13   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 669000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.3921   | \n",
      " |    critic_loss     | 0.3480   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 668899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=670000, episode_reward=285.8 +/- 1.5783214603665687 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 285.8    | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 670000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.6700   | \n",
      " |    critic_loss     | 0.6675   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 669899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=671000, episode_reward=301.48 +/- 1.6992472878507676 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 301.48   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 671000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.5732   | \n",
      " |    critic_loss     | 0.8933   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 670899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=672000, episode_reward=306.7 +/- 1.3396742413447418 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 306.7    | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 672000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.9045   | \n",
      " |    critic_loss     | 0.4761   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 671899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=673000, episode_reward=311.92 +/- 1.081779123567473 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 311.92   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 673000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.1806   | \n",
      " |    critic_loss     | 0.7951   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 672899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=674000, episode_reward=317.14 +/- 1.6247744991526454 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 317.14   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 674000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.7892   | \n",
      " |    critic_loss     | 0.4473   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 673899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=675000, episode_reward=322.35 +/- 1.670956887924865 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 322.35   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 675000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.8085   | \n",
      " |    critic_loss     | 0.9521   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 674899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=676000, episode_reward=327.56 +/- 1.834987749796237 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 327.56   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 676000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.5879   | \n",
      " |    critic_loss     | 1.1470   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 675899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=677000, episode_reward=318.51 +/- 1.1765167105351235 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 318.51   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 677000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.1220   | \n",
      " |    critic_loss     | 0.7805   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 676899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=678000, episode_reward=309.45 +/- 1.5522026131562172 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 309.45   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 678000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.0023   | \n",
      " |    critic_loss     | 0.7506   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 677899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=679000, episode_reward=300.39 +/- 1.3073155567243218 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 300.39   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 679000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.2644   | \n",
      " |    critic_loss     | 0.8161   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 678899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=680000, episode_reward=291.33 +/- 1.2996104774416581 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 291.33   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 680000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.5697   | \n",
      " |    critic_loss     | 0.6424   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 679899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=681000, episode_reward=282.26 +/- 1.3616995215410912 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 282.26   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 681000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.5121   | \n",
      " |    critic_loss     | 0.3780   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 680899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=682000, episode_reward=286.88 +/- 1.8845790665514435 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 286.88   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 682000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.0431   | \n",
      " |    critic_loss     | 0.7608   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 681899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=683000, episode_reward=291.49 +/- 1.1951819253920761 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 291.49   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 683000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.5825   | \n",
      " |    critic_loss     | 0.3956   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 682899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=684000, episode_reward=296.1 +/- 1.0119790158155575 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 296.1    | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 684000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.2956   | \n",
      " |    critic_loss     | 0.8239   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 683899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=685000, episode_reward=300.7 +/- 1.1273289426759066 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 300.7    | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 685000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.3930   | \n",
      " |    critic_loss     | 0.5983   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 684899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=686000, episode_reward=305.3 +/- 1.288832478235701 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 305.3    | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 686000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.1944   | \n",
      " |    critic_loss     | 0.5486   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 685899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=687000, episode_reward=312.8 +/- 1.6208339448595805 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 312.8    | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 687000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.2394   | \n",
      " |    critic_loss     | 1.0598   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 686899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=688000, episode_reward=320.3 +/- 1.74744760591238 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 320.3    | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 688000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.8235   | \n",
      " |    critic_loss     | 0.4559   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 687899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=689000, episode_reward=327.8 +/- 1.2934369697357662 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 327.8    | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 689000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.9907   | \n",
      " |    critic_loss     | 0.4977   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 688899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=690000, episode_reward=335.29 +/- 1.1647780851664287 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 335.29   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 690000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.5938   | \n",
      " |    critic_loss     | 0.6485   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 689899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=691000, episode_reward=342.78 +/- 1.177981458570477 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 342.78   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 691000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.3528   | \n",
      " |    critic_loss     | 1.0882   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 690899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=692000, episode_reward=345.08 +/- 1.6236643204630474 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 345.08   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 692000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.0016   | \n",
      " |    critic_loss     | 0.5004   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 691899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=693000, episode_reward=347.38 +/- 1.5124217076331221 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 347.38   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 693000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.8756   | \n",
      " |    critic_loss     | 1.2189   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 692899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=694000, episode_reward=349.68 +/- 1.6528314548041723 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 349.68   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 694000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.3544   | \n",
      " |    critic_loss     | 0.5886   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 693899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=695000, episode_reward=351.97 +/- 1.385075823067114 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 351.97   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 695000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.6296   | \n",
      " |    critic_loss     | 1.1574   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 694899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=696000, episode_reward=354.26 +/- 1.6401179576087999 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 354.26   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 696000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.0955   | \n",
      " |    critic_loss     | 1.0239   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 695899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=697000, episode_reward=350.33 +/- 1.6418313826981947 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 350.33   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 697000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.5731   | \n",
      " |    critic_loss     | 0.6433   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 696899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=698000, episode_reward=346.39 +/- 1.3227360555201213 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 346.39   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 698000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.2876   | \n",
      " |    critic_loss     | 0.8219   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 697899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=699000, episode_reward=342.45 +/- 1.3218591768688692 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 342.45   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 699000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.9405   | \n",
      " |    critic_loss     | 0.9851   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 698899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=700000, episode_reward=338.51 +/- 1.259881339478933 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 338.51   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 700000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.8386   | \n",
      " |    critic_loss     | 0.4596   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 699899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=701000, episode_reward=334.56 +/- 1.364222492174626 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 334.56   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 701000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.8289   | \n",
      " |    critic_loss     | 0.9572   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 700899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=702000, episode_reward=338.23 +/- 1.1986100234894794 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 338.23   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 702000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.9131   | \n",
      " |    critic_loss     | 0.9783   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 701899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=703000, episode_reward=341.9 +/- 1.526801126331587 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 341.9    | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 703000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.8944   | \n",
      " |    critic_loss     | 0.7236   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 702899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=704000, episode_reward=345.56 +/- 1.7925164886902822 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 345.56   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 704000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.2190   | \n",
      " |    critic_loss     | 1.0548   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 703899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=705000, episode_reward=349.22 +/- 1.8947359057717341 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 349.22   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 705000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.5110   | \n",
      " |    critic_loss     | 0.6278   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 704899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=706000, episode_reward=352.88 +/- 1.647307336967148 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 352.88   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 706000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.5421   | \n",
      " |    critic_loss     | 0.8855   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 705899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=707000, episode_reward=350.04 +/- 1.705329911999061 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 350.04   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 707000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.4377   | \n",
      " |    critic_loss     | 0.3594   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 706899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=708000, episode_reward=347.21 +/- 1.5065412945329166 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 347.21   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 708000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.3402   | \n",
      " |    critic_loss     | 0.5850   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 707899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=709000, episode_reward=344.37 +/- 1.320713576053786 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 344.37   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 709000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.3519   | \n",
      " |    critic_loss     | 0.3380   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 708899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=710000, episode_reward=341.53 +/- 1.9205132083328469 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 341.53   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 710000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.1839   | \n",
      " |    critic_loss     | 0.5460   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 709899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=711000, episode_reward=338.68 +/- 1.6746424270246623 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 338.68   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 711000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.7180   | \n",
      " |    critic_loss     | 0.4295   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 710899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=712000, episode_reward=339.97 +/- 1.9138852810857416 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 339.97   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 712000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.7167   | \n",
      " |    critic_loss     | 0.6792   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 711899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=713000, episode_reward=341.26 +/- 1.5678351948116973 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 341.26   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 713000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.2788   | \n",
      " |    critic_loss     | 0.3197   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 712899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=714000, episode_reward=342.55 +/- 1.3547859808052167 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 342.55   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 714000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.6563   | \n",
      " |    critic_loss     | 0.6641   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 713899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=715000, episode_reward=343.83 +/- 1.1726904610842728 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 343.83   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 715000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.1355   | \n",
      " |    critic_loss     | 1.2839   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 714899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=716000, episode_reward=345.11 +/- 1.4726900984653803 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 345.11   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 716000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.3562   | \n",
      " |    critic_loss     | 0.5891   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 715899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=717000, episode_reward=349.51 +/- 1.6799828990559265 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 349.51   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 717000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.1878   | \n",
      " |    critic_loss     | 0.7970   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 716899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=718000, episode_reward=353.92 +/- 1.7806974450069888 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 353.92   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 718000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.1742   | \n",
      " |    critic_loss     | 0.5436   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 717899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=719000, episode_reward=358.31 +/- 1.4423792271497413 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 358.31   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 719000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.9144   | \n",
      " |    critic_loss     | 0.7286   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 718899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=720000, episode_reward=362.71 +/- 1.5684188949576536 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 362.71   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 720000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.3720   | \n",
      " |    critic_loss     | 1.0930   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 719899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=721000, episode_reward=367.1 +/- 1.7214131117790759 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 367.1    | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 721000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.8120   | \n",
      " |    critic_loss     | 0.4530   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 720899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=722000, episode_reward=370.67 +/- 1.6072654191649065 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 370.67   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 722000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.3050   | \n",
      " |    critic_loss     | 0.8263   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 721899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=723000, episode_reward=374.24 +/- 1.4580658514986948 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 374.24   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 723000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.3685   | \n",
      " |    critic_loss     | 0.5921   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 722899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=724000, episode_reward=377.8 +/- 1.4913906455616295 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 377.8    | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 724000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.7519   | \n",
      " |    critic_loss     | 0.6880   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 723899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=725000, episode_reward=381.37 +/- 1.6559981095652443 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 381.37   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 725000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.3462   | \n",
      " |    critic_loss     | 0.3365   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 724899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=726000, episode_reward=384.93 +/- 1.8432693888068368 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 384.93   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 726000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.2467   | \n",
      " |    critic_loss     | 0.5617   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 725899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=727000, episode_reward=380.64 +/- 1.422648776200453 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 380.64   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 727000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.3527   | \n",
      " |    critic_loss     | 0.5882   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 726899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=728000, episode_reward=376.36 +/- 1.2413495874794944 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 376.36   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 728000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.1992   | \n",
      " |    critic_loss     | 0.7998   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 727899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=729000, episode_reward=372.07 +/- 1.549375779979072 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 372.07   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 729000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.8892   | \n",
      " |    critic_loss     | 1.2223   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 728899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=730000, episode_reward=367.78 +/- 1.9424442352795155 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 367.78   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 730000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.6827   | \n",
      " |    critic_loss     | 0.9207   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 729899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=731000, episode_reward=363.48 +/- 1.995679013527638 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 363.48   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 731000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.8644   | \n",
      " |    critic_loss     | 1.2161   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 730899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=732000, episode_reward=367.92 +/- 1.820844563728111 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 367.92   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 732000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.9784   | \n",
      " |    critic_loss     | 0.7446   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 731899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=733000, episode_reward=372.35 +/- 1.5178161313032048 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 372.35   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 733000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.7790   | \n",
      " |    critic_loss     | 1.1948   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 732899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=734000, episode_reward=376.78 +/- 1.1601770986985112 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 376.78   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 734000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.5587   | \n",
      " |    critic_loss     | 1.1397   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 733899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=735000, episode_reward=381.21 +/- 1.24957056206875 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 381.21   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 735000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.0574   | \n",
      " |    critic_loss     | 0.7644   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 734899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=736000, episode_reward=385.63 +/- 1.6787987105115256 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 385.63   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 736000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.1418   | \n",
      " |    critic_loss     | 0.5355   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 735899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=737000, episode_reward=378.55 +/- 1.1257129161117296 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 378.55   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 737000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.7274   | \n",
      " |    critic_loss     | 0.6818   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 736899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=738000, episode_reward=371.46 +/- 1.4993832641933489 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 371.46   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 738000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.3932   | \n",
      " |    critic_loss     | 0.3483   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 737899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=739000, episode_reward=364.38 +/- 1.7407721655755357 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 364.38   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 739000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.6738   | \n",
      " |    critic_loss     | 1.1684   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 738899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=740000, episode_reward=357.29 +/- 1.1417958457379678 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 357.29   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 740000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.4194   | \n",
      " |    critic_loss     | 0.8549   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 739899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=741000, episode_reward=350.19 +/- 1.8778093285123338 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 350.19   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 741000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.1489   | \n",
      " |    critic_loss     | 0.2872   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 740899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=742000, episode_reward=358.0 +/- 1.008026224429257 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 358.0    | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 742000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.5092   | \n",
      " |    critic_loss     | 0.8773   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 741899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=743000, episode_reward=365.8 +/- 1.1393465149205642 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 365.8    | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 743000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.5627   | \n",
      " |    critic_loss     | 0.8907   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 742899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=744000, episode_reward=373.6 +/- 1.2039594715900428 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 373.6    | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 744000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.1859   | \n",
      " |    critic_loss     | 0.7965   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 743899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=745000, episode_reward=381.4 +/- 1.311224992025367 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 381.4    | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 745000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.8352   | \n",
      " |    critic_loss     | 0.9588   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 744899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=746000, episode_reward=389.2 +/- 1.1432172323889305 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 389.2    | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 746000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.1666   | \n",
      " |    critic_loss     | 0.7916   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 745899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=747000, episode_reward=383.18 +/- 1.3342419147278095 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 383.18   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 747000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.7445   | \n",
      " |    critic_loss     | 1.1861   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 746899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=748000, episode_reward=377.16 +/- 1.4403433843582218 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 377.16   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 748000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.0302   | \n",
      " |    critic_loss     | 0.5075   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 747899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=749000, episode_reward=371.14 +/- 1.9959289622383287 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 371.14   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 749000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.5456   | \n",
      " |    critic_loss     | 0.8864   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 748899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=750000, episode_reward=365.12 +/- 1.0909607737918936 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 365.12   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 750000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.1633   | \n",
      " |    critic_loss     | 0.7908   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 749899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=751000, episode_reward=359.09 +/- 1.1380515864068022 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 359.09   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 751000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.0355   | \n",
      " |    critic_loss     | 0.2589   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 750899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=752000, episode_reward=360.53 +/- 1.3071699562801244 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 360.53   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 752000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.9582   | \n",
      " |    critic_loss     | 0.7395   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 751899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=753000, episode_reward=361.97 +/- 1.3479289647112407 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 361.97   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 753000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.2771   | \n",
      " |    critic_loss     | 0.5693   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 752899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=754000, episode_reward=363.41 +/- 1.9637990271109607 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 363.41   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 754000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.1534   | \n",
      " |    critic_loss     | 0.2884   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 753899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=755000, episode_reward=364.85 +/- 1.0860338129772868 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 364.85   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 755000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.4527   | \n",
      " |    critic_loss     | 0.6132   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 754899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=756000, episode_reward=366.28 +/- 1.8525847514209621 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 366.28   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 756000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.4243   | \n",
      " |    critic_loss     | 0.8561   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 755899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=757000, episode_reward=352.28 +/- 1.2730709033619991 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 352.28   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 757000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.1366   | \n",
      " |    critic_loss     | 1.0341   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 756899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=758000, episode_reward=338.27 +/- 1.719204003390809 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 338.27   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 758000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.5905   | \n",
      " |    critic_loss     | 0.8976   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 757899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=759000, episode_reward=324.26 +/- 1.8424832272521008 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 324.26   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 759000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.1037   | \n",
      " |    critic_loss     | 1.0259   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 758899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=760000, episode_reward=310.25 +/- 1.454332983314206 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 310.25   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 760000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.6236   | \n",
      " |    critic_loss     | 0.6559   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 759899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=761000, episode_reward=296.24 +/- 1.2660758281769087 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 296.24   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 761000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.1850   | \n",
      " |    critic_loss     | 0.2962   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 760899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=762000, episode_reward=298.33 +/- 1.7103686772277125 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 298.33   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 762000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.8989   | \n",
      " |    critic_loss     | 0.9747   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 761899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=763000, episode_reward=300.42 +/- 1.6128742124233169 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 300.42   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 763000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.8878   | \n",
      " |    critic_loss     | 0.9720   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 762899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=764000, episode_reward=302.51 +/- 1.8926851344770217 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 302.51   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 764000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.2698   | \n",
      " |    critic_loss     | 0.3174   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 763899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=765000, episode_reward=304.59 +/- 1.1551165574418127 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 304.59   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 765000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.2194   | \n",
      " |    critic_loss     | 0.3049   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 764899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=766000, episode_reward=306.68 +/- 1.9109395383047647 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 306.68   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 766000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.6753   | \n",
      " |    critic_loss     | 0.9188   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 765899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=767000, episode_reward=322.79 +/- 1.7778382519490346 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 322.79   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 767000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.8233   | \n",
      " |    critic_loss     | 0.4558   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 766899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=768000, episode_reward=338.91 +/- 1.3079129825152411 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 338.91   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 768000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.0186   | \n",
      " |    critic_loss     | 1.0046   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 767899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=769000, episode_reward=355.02 +/- 1.1296529846320018 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 355.02   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 769000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.5949   | \n",
      " |    critic_loss     | 0.6487   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 768899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=770000, episode_reward=371.13 +/- 1.5427860972724026 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 371.13   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 770000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.9342   | \n",
      " |    critic_loss     | 0.4835   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 769899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=771000, episode_reward=387.24 +/- 1.8630409139985358 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 387.24   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 771000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.3111   | \n",
      " |    critic_loss     | 0.5778   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 770899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=772000, episode_reward=384.11 +/- 1.9958959510182495 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 384.11   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 772000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.8183   | \n",
      " |    critic_loss     | 0.7046   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 771899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=773000, episode_reward=380.99 +/- 1.8040031546506785 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 380.99   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 773000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.3767   | \n",
      " |    critic_loss     | 0.3442   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 772899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=774000, episode_reward=377.86 +/- 1.418404569370725 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 377.86   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 774000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.4176   | \n",
      " |    critic_loss     | 0.8544   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 773899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=775000, episode_reward=374.73 +/- 1.4040637073289122 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 374.73   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 775000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.0026   | \n",
      " |    critic_loss     | 0.5006   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 774899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=776000, episode_reward=371.6 +/- 1.8550021704950441 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 371.6    | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 776000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.7868   | \n",
      " |    critic_loss     | 0.6967   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 775899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=777000, episode_reward=371.77 +/- 1.4828817271870478 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 371.77   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 777000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.6513   | \n",
      " |    critic_loss     | 0.4128   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 776899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=778000, episode_reward=371.93 +/- 1.4202621119114005 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 371.93   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 778000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.4742   | \n",
      " |    critic_loss     | 1.1186   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 777899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=779000, episode_reward=372.09 +/- 1.9632607321689135 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 372.09   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 779000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.7723   | \n",
      " |    critic_loss     | 0.6931   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 778899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=780000, episode_reward=372.25 +/- 1.3583015991654577 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 372.25   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 780000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.3989   | \n",
      " |    critic_loss     | 0.8497   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 779899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=781000, episode_reward=372.41 +/- 1.273710398777515 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 372.41   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 781000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.5179   | \n",
      " |    critic_loss     | 0.3795   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 780899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=782000, episode_reward=378.95 +/- 1.876691941276031 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 378.95   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 782000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 0.9468   | \n",
      " |    critic_loss     | 0.2367   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 781899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=783000, episode_reward=385.5 +/- 1.0865886200608192 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 385.5    | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 783000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.9616   | \n",
      " |    critic_loss     | 0.4904   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 782899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=784000, episode_reward=392.04 +/- 1.1791179603800042 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 392.04   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 784000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.6467   | \n",
      " |    critic_loss     | 0.9117   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 783899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=785000, episode_reward=398.58 +/- 1.925675400572252 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 398.58   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 785000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.3556   | \n",
      " |    critic_loss     | 0.3389   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 784899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=786000, episode_reward=405.11 +/- 1.769569338362929 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 405.11   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 786000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.5612   | \n",
      " |    critic_loss     | 1.1403   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 785899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=787000, episode_reward=399.27 +/- 1.175044953326678 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 399.27   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 787000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.4738   | \n",
      " |    critic_loss     | 0.6184   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 786899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=788000, episode_reward=393.43 +/- 1.2773445678483117 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 393.43   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 788000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.6487   | \n",
      " |    critic_loss     | 1.1622   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 787899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=789000, episode_reward=387.59 +/- 1.3196349747545546 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 387.59   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 789000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.7719   | \n",
      " |    critic_loss     | 1.1930   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 788899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=790000, episode_reward=381.74 +/- 1.5542213759620498 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 381.74   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 790000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.1604   | \n",
      " |    critic_loss     | 0.5401   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 789899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=791000, episode_reward=375.89 +/- 1.2681165362567548 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 375.89   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 791000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.1227   | \n",
      " |    critic_loss     | 0.5307   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 790899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=792000, episode_reward=372.09 +/- 1.8954491015446788 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 372.09   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 792000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.5400   | \n",
      " |    critic_loss     | 0.6350   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 791899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=793000, episode_reward=368.28 +/- 1.720228234345527 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 368.28   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 793000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.2149   | \n",
      " |    critic_loss     | 0.3037   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 792899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=794000, episode_reward=364.48 +/- 1.1786806580029134 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 364.48   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 794000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.5817   | \n",
      " |    critic_loss     | 0.3954   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 793899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=795000, episode_reward=360.67 +/- 1.0092019571771476 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 360.67   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 795000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.4039   | \n",
      " |    critic_loss     | 0.6010   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 794899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=796000, episode_reward=356.86 +/- 1.0321016105493377 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 356.86   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 796000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.1101   | \n",
      " |    critic_loss     | 0.5275   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 795899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=797000, episode_reward=353.48 +/- 1.0994209355149143 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 353.48   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 797000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.7067   | \n",
      " |    critic_loss     | 0.4267   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 796899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=798000, episode_reward=350.11 +/- 1.7090692881156377 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 350.11   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 798000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.5651   | \n",
      " |    critic_loss     | 0.6413   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 797899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=799000, episode_reward=346.73 +/- 1.7126890547994826 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 346.73   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 799000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 0.8287   | \n",
      " |    critic_loss     | 0.2072   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 798899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=800000, episode_reward=343.35 +/- 1.695043379183446 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 343.35   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 800000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.8537   | \n",
      " |    critic_loss     | 0.7134   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 799899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=801000, episode_reward=339.97 +/- 1.5582470353988631 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 339.97   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 801000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 0.9682   | \n",
      " |    critic_loss     | 0.2421   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 800899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=802000, episode_reward=347.88 +/- 1.2770157052387967 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 347.88   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 802000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.5298   | \n",
      " |    critic_loss     | 1.1324   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 801899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=803000, episode_reward=355.8 +/- 1.7320632412285426 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 355.8    | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 803000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.1153   | \n",
      " |    critic_loss     | 0.7788   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 802899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=804000, episode_reward=363.71 +/- 1.8260810322801149 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 363.71   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 804000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.1409   | \n",
      " |    critic_loss     | 0.7852   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 803899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=805000, episode_reward=371.62 +/- 1.5516338365520823 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 371.62   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 805000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.4657   | \n",
      " |    critic_loss     | 0.6164   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 804899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=806000, episode_reward=379.52 +/- 1.3451523164688486 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 379.52   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 806000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.4758   | \n",
      " |    critic_loss     | 1.1190   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 805899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=807000, episode_reward=379.72 +/- 1.0351914267964082 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 379.72   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 807000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.7662   | \n",
      " |    critic_loss     | 0.6915   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 806899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=808000, episode_reward=379.91 +/- 1.384264145709293 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 379.91   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 808000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.0437   | \n",
      " |    critic_loss     | 0.2609   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 807899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=809000, episode_reward=380.1 +/- 1.8205718554782835 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 380.1    | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 809000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.1742   | \n",
      " |    critic_loss     | 0.7936   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 808899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=810000, episode_reward=380.29 +/- 1.7387823702035816 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 380.29   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 810000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.3001   | \n",
      " |    critic_loss     | 0.3250   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 809899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=811000, episode_reward=380.47 +/- 1.4432602556408507 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 380.47   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 811000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.1392   | \n",
      " |    critic_loss     | 1.0348   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 810899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=812000, episode_reward=371.76 +/- 1.0533615078622476 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 371.76   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 812000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.5420   | \n",
      " |    critic_loss     | 1.1355   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 811899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=813000, episode_reward=363.03 +/- 1.644081907550851 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 363.03   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 813000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.4115   | \n",
      " |    critic_loss     | 1.1029   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 812899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=814000, episode_reward=354.31 +/- 1.4021060332121424 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 354.31   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 814000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.2717   | \n",
      " |    critic_loss     | 0.8179   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 813899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=815000, episode_reward=345.59 +/- 1.6446915462902403 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 345.59   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 815000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.5827   | \n",
      " |    critic_loss     | 0.3957   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 814899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=816000, episode_reward=336.86 +/- 1.279269392270832 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 336.86   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 816000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.1567   | \n",
      " |    critic_loss     | 0.2892   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 815899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=817000, episode_reward=350.84 +/- 1.993995575993403 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 350.84   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 817000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.5052   | \n",
      " |    critic_loss     | 1.1263   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 816899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=818000, episode_reward=364.82 +/- 1.392818097490951 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 364.82   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 818000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.9257   | \n",
      " |    critic_loss     | 0.7314   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 817899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=819000, episode_reward=378.8 +/- 1.0866164102847098 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 378.8    | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 819000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.3425   | \n",
      " |    critic_loss     | 1.0856   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 818899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=820000, episode_reward=392.78 +/- 1.4969674979197074 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 392.78   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 820000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.7904   | \n",
      " |    critic_loss     | 0.9476   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 819899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=821000, episode_reward=406.75 +/- 1.8115269537662813 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 406.75   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 821000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.2340   | \n",
      " |    critic_loss     | 0.5585   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 820899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=822000, episode_reward=403.24 +/- 1.4600331837104894 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 403.24   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 822000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.6924   | \n",
      " |    critic_loss     | 0.4231   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 821899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=823000, episode_reward=399.72 +/- 1.661965436455643 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 399.72   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 823000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.5933   | \n",
      " |    critic_loss     | 0.6483   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 822899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=824000, episode_reward=396.2 +/- 1.7233808891835332 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 396.2    | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 824000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.5417   | \n",
      " |    critic_loss     | 1.1354   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 823899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=825000, episode_reward=392.68 +/- 1.7234876715478595 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 392.68   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 825000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.5869   | \n",
      " |    critic_loss     | 0.3967   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 824899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=826000, episode_reward=389.16 +/- 1.7764632067818724 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 389.16   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 826000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.6524   | \n",
      " |    critic_loss     | 0.9131   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 825899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=827000, episode_reward=390.4 +/- 1.130074349278234 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 390.4    | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 827000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.5737   | \n",
      " |    critic_loss     | 0.6434   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 826899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=828000, episode_reward=391.63 +/- 1.7022647760225038 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 391.63   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 828000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.0493   | \n",
      " |    critic_loss     | 0.5123   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 827899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=829000, episode_reward=392.86 +/- 1.618763371566022 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 392.86   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 829000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.3440   | \n",
      " |    critic_loss     | 0.5860   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 828899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=830000, episode_reward=394.1 +/- 1.4981686103334717 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 394.1    | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 830000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.8446   | \n",
      " |    critic_loss     | 0.7111   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 829899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=831000, episode_reward=395.33 +/- 1.5498270407357397 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 395.33   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 831000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.1467   | \n",
      " |    critic_loss     | 0.5367   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 830899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=832000, episode_reward=397.99 +/- 1.445642036880781 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 397.99   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 832000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.5347   | \n",
      " |    critic_loss     | 0.6337   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 831899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=833000, episode_reward=400.66 +/- 1.9433823302532174 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 400.66   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 833000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.0585   | \n",
      " |    critic_loss     | 0.5146   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 832899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=834000, episode_reward=403.32 +/- 1.0047752272151549 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 403.32   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 834000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.0636   | \n",
      " |    critic_loss     | 0.7659   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 833899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=835000, episode_reward=405.98 +/- 1.3499458794028427 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 405.98   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 835000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.0703   | \n",
      " |    critic_loss     | 1.0176   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 834899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=836000, episode_reward=408.64 +/- 1.721006585317158 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 408.64   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 836000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.0088   | \n",
      " |    critic_loss     | 0.5022   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 835899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=837000, episode_reward=403.32 +/- 1.3385644836103703 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 403.32   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 837000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.9668   | \n",
      " |    critic_loss     | 0.9917   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 836899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=838000, episode_reward=398.0 +/- 1.8567028842016051 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 398.0    | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 838000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.4593   | \n",
      " |    critic_loss     | 0.3648   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 837899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=839000, episode_reward=392.68 +/- 1.737557127972222 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 392.68   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 839000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.7547   | \n",
      " |    critic_loss     | 0.4387   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 838899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=840000, episode_reward=387.36 +/- 1.6992335235827567 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 387.36   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 840000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.0378   | \n",
      " |    critic_loss     | 0.5094   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 839899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=841000, episode_reward=382.04 +/- 1.7339839332742426 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 382.04   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 841000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.2225   | \n",
      " |    critic_loss     | 0.5556   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 840899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=842000, episode_reward=392.99 +/- 1.7975258864321346 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 392.99   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 842000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.7037   | \n",
      " |    critic_loss     | 0.6759   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 841899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=843000, episode_reward=403.95 +/- 1.1159394782254002 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 403.95   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 843000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 0.8849   | \n",
      " |    critic_loss     | 0.2212   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 842899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=844000, episode_reward=414.9 +/- 1.2430282450668768 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 414.9    | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 844000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.6107   | \n",
      " |    critic_loss     | 0.9027   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 843899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=845000, episode_reward=425.86 +/- 1.4294295137301765 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 425.86   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 845000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.7699   | \n",
      " |    critic_loss     | 0.9425   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 844899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=846000, episode_reward=436.81 +/- 1.4038729100650216 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 436.81   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 846000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.1399   | \n",
      " |    critic_loss     | 0.5350   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 845899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=847000, episode_reward=438.32 +/- 1.6395788920107757 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 438.32   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 847000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.0188   | \n",
      " |    critic_loss     | 1.0047   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 846899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=848000, episode_reward=439.83 +/- 1.5627290152772826 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 439.83   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 848000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.9531   | \n",
      " |    critic_loss     | 0.9883   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 847899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=849000, episode_reward=441.34 +/- 1.4003957750890637 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 441.34   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 849000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.6759   | \n",
      " |    critic_loss     | 0.9190   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 848899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=850000, episode_reward=442.85 +/- 1.8847701727438215 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 442.85   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 850000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.2903   | \n",
      " |    critic_loss     | 0.5726   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 849899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=851000, episode_reward=444.36 +/- 1.6771101426114763 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 444.36   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 851000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.1012   | \n",
      " |    critic_loss     | 0.7753   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 850899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=852000, episode_reward=435.81 +/- 1.3912101749618202 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 435.81   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 852000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.2100   | \n",
      " |    critic_loss     | 0.8025   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 851899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=853000, episode_reward=427.25 +/- 1.8953773201680368 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 427.25   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 853000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.0551   | \n",
      " |    critic_loss     | 1.0138   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 852899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=854000, episode_reward=418.69 +/- 1.692633055586876 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 418.69   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 854000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 0.6667   | \n",
      " |    critic_loss     | 0.1667   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 853899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=855000, episode_reward=410.13 +/- 1.2839979205076228 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 410.13   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 855000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.8348   | \n",
      " |    critic_loss     | 0.4587   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 854899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=856000, episode_reward=401.57 +/- 1.0038591606783953 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 401.57   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 856000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.8036   | \n",
      " |    critic_loss     | 0.4509   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 855899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=857000, episode_reward=404.2 +/- 1.9574234810583109 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 404.2    | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 857000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.8611   | \n",
      " |    critic_loss     | 0.7153   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 856899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=858000, episode_reward=406.82 +/- 1.9869123279382244 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 406.82   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 858000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.8744   | \n",
      " |    critic_loss     | 0.9686   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 857899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=859000, episode_reward=409.45 +/- 1.8263613970425356 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 409.45   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 859000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.1910   | \n",
      " |    critic_loss     | 0.5478   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 858899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=860000, episode_reward=412.07 +/- 1.2111863933737728 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 412.07   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 860000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.8109   | \n",
      " |    critic_loss     | 0.7027   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 859899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=861000, episode_reward=414.69 +/- 1.4432095350990926 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 414.69   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 861000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.8167   | \n",
      " |    critic_loss     | 0.4542   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 860899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=862000, episode_reward=420.62 +/- 1.011734666906042 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 420.62   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 862000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.5179   | \n",
      " |    critic_loss     | 0.3795   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 861899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=863000, episode_reward=426.55 +/- 1.1136429897165283 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 426.55   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 863000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.2382   | \n",
      " |    critic_loss     | 0.8095   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 862899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=864000, episode_reward=432.47 +/- 1.6730593756750318 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 432.47   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 864000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.8327   | \n",
      " |    critic_loss     | 0.4582   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 863899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=865000, episode_reward=438.39 +/- 1.7589565770444144 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 438.39   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 865000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.1179   | \n",
      " |    critic_loss     | 1.0295   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 864899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=866000, episode_reward=444.32 +/- 1.1362159686287348 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 444.32   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 866000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.8162   | \n",
      " |    critic_loss     | 0.9540   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 865899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=867000, episode_reward=436.21 +/- 1.2981494140163543 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 436.21   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 867000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.7954   | \n",
      " |    critic_loss     | 0.4488   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 866899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=868000, episode_reward=428.11 +/- 1.2990553458838558 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 428.11   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 868000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.9930   | \n",
      " |    critic_loss     | 0.9982   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 867899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=869000, episode_reward=420.0 +/- 1.5926920752298712 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 420.0    | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 869000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.0169   | \n",
      " |    critic_loss     | 0.7542   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 868899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=870000, episode_reward=411.89 +/- 1.1958974474437076 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 411.89   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 870000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.9130   | \n",
      " |    critic_loss     | 0.4782   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 869899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=871000, episode_reward=403.78 +/- 1.1606613673163575 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 403.78   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 871000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.5770   | \n",
      " |    critic_loss     | 0.8943   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 870899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=872000, episode_reward=409.73 +/- 1.5841207313113213 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 409.73   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 872000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.0290   | \n",
      " |    critic_loss     | 1.0072   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 871899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=873000, episode_reward=415.67 +/- 1.974445747816672 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 415.67   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 873000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.2733   | \n",
      " |    critic_loss     | 0.3183   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 872899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=874000, episode_reward=421.62 +/- 1.4700619613639083 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 421.62   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 874000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.7587   | \n",
      " |    critic_loss     | 0.6897   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 873899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=875000, episode_reward=427.56 +/- 1.2611069547489082 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 427.56   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 875000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.6667   | \n",
      " |    critic_loss     | 0.6667   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 874899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=876000, episode_reward=433.5 +/- 1.1841462464559582 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 433.5    | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 876000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 0.7173   | \n",
      " |    critic_loss     | 0.1793   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 875899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=877000, episode_reward=432.47 +/- 1.195427165559372 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 432.47   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 877000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.9480   | \n",
      " |    critic_loss     | 0.9870   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 876899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=878000, episode_reward=431.44 +/- 1.7254173449474222 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 431.44   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 878000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.1152   | \n",
      " |    critic_loss     | 0.7788   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 877899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=879000, episode_reward=430.41 +/- 1.720583141512694 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 430.41   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 879000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.0957   | \n",
      " |    critic_loss     | 0.2739   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 878899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=880000, episode_reward=429.37 +/- 1.5549359554954294 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 429.37   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 880000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.2643   | \n",
      " |    critic_loss     | 1.0661   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 879899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=881000, episode_reward=428.34 +/- 1.6923386339573618 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 428.34   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 881000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.0569   | \n",
      " |    critic_loss     | 1.0142   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 880899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=882000, episode_reward=416.22 +/- 1.189131899857017 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 416.22   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 882000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 0.8181   | \n",
      " |    critic_loss     | 0.2045   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 881899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=883000, episode_reward=404.11 +/- 1.216868051020111 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 404.11   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 883000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.8086   | \n",
      " |    critic_loss     | 0.9522   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 882899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=884000, episode_reward=391.99 +/- 1.6076218285681 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 391.99   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 884000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 0.7802   | \n",
      " |    critic_loss     | 0.1950   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 883899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=885000, episode_reward=379.87 +/- 1.3483965320955253 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 379.87   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 885000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.4106   | \n",
      " |    critic_loss     | 0.8526   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 884899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=886000, episode_reward=367.75 +/- 1.6174091859741364 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 367.75   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 886000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.1616   | \n",
      " |    critic_loss     | 0.7904   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 885899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=887000, episode_reward=374.84 +/- 1.8280948284036782 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 374.84   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 887000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.0478   | \n",
      " |    critic_loss     | 1.0119   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 886899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=888000, episode_reward=381.92 +/- 1.0803687112502847 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 381.92   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 888000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.9597   | \n",
      " |    critic_loss     | 0.7399   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 887899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=889000, episode_reward=389.01 +/- 1.8512616335076881 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 389.01   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 889000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 0.4995   | \n",
      " |    critic_loss     | 0.1249   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 888899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=890000, episode_reward=396.09 +/- 1.380862557791013 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 396.09   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 890000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.6485   | \n",
      " |    critic_loss     | 0.9121   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 889899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=891000, episode_reward=403.17 +/- 1.1679702834892702 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 403.17   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 891000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.8899   | \n",
      " |    critic_loss     | 0.7225   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 890899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=892000, episode_reward=400.45 +/- 1.8620512269814564 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 400.45   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 892000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.8724   | \n",
      " |    critic_loss     | 0.4681   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 891899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=893000, episode_reward=397.73 +/- 1.1248845809407788 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 397.73   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 893000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.3528   | \n",
      " |    critic_loss     | 1.0882   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 892899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=894000, episode_reward=395.0 +/- 1.9796129404898557 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 395.0    | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 894000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.0957   | \n",
      " |    critic_loss     | 0.7739   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 893899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=895000, episode_reward=392.28 +/- 1.8474759299162478 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 392.28   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 895000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.1878   | \n",
      " |    critic_loss     | 0.5470   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 894899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=896000, episode_reward=389.55 +/- 1.476598193628754 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 389.55   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 896000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.1643   | \n",
      " |    critic_loss     | 0.5411   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 895899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=897000, episode_reward=392.44 +/- 1.042993387531343 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 392.44   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 897000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.5498   | \n",
      " |    critic_loss     | 0.3874   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 896899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=898000, episode_reward=395.32 +/- 1.2240589934514692 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 395.32   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 898000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.3308   | \n",
      " |    critic_loss     | 1.0827   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 897899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=899000, episode_reward=398.2 +/- 1.1327266600666246 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 398.2    | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 899000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.0665   | \n",
      " |    critic_loss     | 0.5166   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 898899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=900000, episode_reward=401.08 +/- 1.5060923017789318 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 401.08   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 900000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.9446   | \n",
      " |    critic_loss     | 0.7362   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 899899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=901000, episode_reward=403.96 +/- 1.1534468857049403 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 403.96   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 901000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.0554   | \n",
      " |    critic_loss     | 1.0138   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 900899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=902000, episode_reward=403.57 +/- 1.5426819579510247 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 403.57   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 902000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.3179   | \n",
      " |    critic_loss     | 0.8295   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 901899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=903000, episode_reward=403.17 +/- 1.2273454760710083 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 403.17   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 903000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.9910   | \n",
      " |    critic_loss     | 0.9977   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 902899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=904000, episode_reward=402.77 +/- 1.4857755633184706 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 402.77   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 904000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.1938   | \n",
      " |    critic_loss     | 0.5484   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 903899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=905000, episode_reward=402.36 +/- 1.114593784189336 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 402.36   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 905000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.5942   | \n",
      " |    critic_loss     | 0.8986   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 904899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=906000, episode_reward=401.96 +/- 1.1393436099864194 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 401.96   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 906000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.5958   | \n",
      " |    critic_loss     | 0.3989   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 905899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=907000, episode_reward=395.31 +/- 1.5442333628551252 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 395.31   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 907000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.6299   | \n",
      " |    critic_loss     | 0.9075   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 906899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=908000, episode_reward=388.66 +/- 1.6005281021770639 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 388.66   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 908000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.4355   | \n",
      " |    critic_loss     | 0.6089   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 907899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=909000, episode_reward=382.01 +/- 1.8829575091298385 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 382.01   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 909000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.5255   | \n",
      " |    critic_loss     | 0.6314   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 908899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=910000, episode_reward=375.36 +/- 1.426246090379281 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 375.36   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 910000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.0981   | \n",
      " |    critic_loss     | 1.0245   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 909899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=911000, episode_reward=368.7 +/- 1.5929754407904917 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 368.7    | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 911000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.9366   | \n",
      " |    critic_loss     | 0.4841   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 910899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=912000, episode_reward=373.67 +/- 1.5422838021144372 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 373.67   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 912000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.2889   | \n",
      " |    critic_loss     | 1.0722   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 911899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=913000, episode_reward=378.63 +/- 1.5120669588029156 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 378.63   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 913000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.3866   | \n",
      " |    critic_loss     | 0.8466   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 912899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=914000, episode_reward=383.59 +/- 1.6488615956070571 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 383.59   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 914000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.5948   | \n",
      " |    critic_loss     | 0.8987   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 913899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=915000, episode_reward=388.55 +/- 1.4582096139879996 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 388.55   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 915000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.8058   | \n",
      " |    critic_loss     | 0.9514   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 914899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=916000, episode_reward=393.51 +/- 1.382678362116453 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 393.51   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 916000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.7753   | \n",
      " |    critic_loss     | 0.6938   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 915899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=917000, episode_reward=391.94 +/- 1.3481182177346351 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 391.94   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 917000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.5472   | \n",
      " |    critic_loss     | 0.8868   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 916899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=918000, episode_reward=390.38 +/- 1.0580261811392742 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 390.38   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 918000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.1272   | \n",
      " |    critic_loss     | 1.0318   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 917899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=919000, episode_reward=388.81 +/- 1.585269030430124 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 388.81   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 919000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.1820   | \n",
      " |    critic_loss     | 1.0455   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 918899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=920000, episode_reward=387.25 +/- 1.3720397241031592 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 387.25   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 920000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.1183   | \n",
      " |    critic_loss     | 0.5296   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 919899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=921000, episode_reward=385.68 +/- 1.3522797583914432 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 385.68   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 921000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.0908   | \n",
      " |    critic_loss     | 0.2727   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 920899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=922000, episode_reward=381.73 +/- 1.267902189644677 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 381.73   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 922000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 0.6879   | \n",
      " |    critic_loss     | 0.1720   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 921899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=923000, episode_reward=377.78 +/- 1.3103150511402148 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 377.78   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 923000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.9780   | \n",
      " |    critic_loss     | 0.9945   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 922899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=924000, episode_reward=373.82 +/- 1.3888865532214063 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 373.82   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 924000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 0.9961   | \n",
      " |    critic_loss     | 0.2490   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 923899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=925000, episode_reward=369.87 +/- 1.4552684642750653 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 369.87   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 925000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.0852   | \n",
      " |    critic_loss     | 0.2713   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 924899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=926000, episode_reward=365.91 +/- 1.7573880917454947 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 365.91   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 926000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.9220   | \n",
      " |    critic_loss     | 0.9805   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 925899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=927000, episode_reward=371.78 +/- 1.568075965852147 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 371.78   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 927000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.8834   | \n",
      " |    critic_loss     | 0.9709   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 926899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=928000, episode_reward=377.64 +/- 1.6823633627963266 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 377.64   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 928000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.1034   | \n",
      " |    critic_loss     | 0.5259   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 927899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=929000, episode_reward=383.51 +/- 1.9174797353080917 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 383.51   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 929000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.0990   | \n",
      " |    critic_loss     | 1.0247   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 928899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=930000, episode_reward=389.38 +/- 1.887364473377232 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 389.38   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 930000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.5088   | \n",
      " |    critic_loss     | 0.3772   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 929899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=931000, episode_reward=395.24 +/- 1.024940797784617 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 395.24   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 931000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 0.6922   | \n",
      " |    critic_loss     | 0.1731   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 930899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=932000, episode_reward=393.9 +/- 1.02250109370675 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 393.9    | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 932000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.5769   | \n",
      " |    critic_loss     | 0.6442   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 931899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=933000, episode_reward=392.55 +/- 1.252480643852288 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 392.55   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 933000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.3423   | \n",
      " |    critic_loss     | 0.5856   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 932899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=934000, episode_reward=391.2 +/- 1.1520911267443728 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 391.2    | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 934000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 0.9542   | \n",
      " |    critic_loss     | 0.2386   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 933899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=935000, episode_reward=389.86 +/- 1.462097555122841 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 389.86   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 935000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.4009   | \n",
      " |    critic_loss     | 0.3502   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 934899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=936000, episode_reward=388.51 +/- 1.534534001896406 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 388.51   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 936000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.9239   | \n",
      " |    critic_loss     | 0.7310   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 935899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=937000, episode_reward=382.78 +/- 1.608740776998172 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 382.78   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 937000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.7949   | \n",
      " |    critic_loss     | 0.9487   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 936899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=938000, episode_reward=377.05 +/- 1.659501635824106 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 377.05   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 938000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 0.3314   | \n",
      " |    critic_loss     | 0.0829   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 937899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=939000, episode_reward=371.31 +/- 1.7434013105666304 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 371.31   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 939000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.0851   | \n",
      " |    critic_loss     | 0.2713   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 938899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=940000, episode_reward=365.58 +/- 1.4073221876385862 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 365.58   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 940000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.4318   | \n",
      " |    critic_loss     | 0.3580   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 939899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=941000, episode_reward=359.85 +/- 1.5082698768092224 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 359.85   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 941000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.3383   | \n",
      " |    critic_loss     | 0.3346   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 940899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=942000, episode_reward=365.89 +/- 1.9974340300946012 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 365.89   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 942000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.0141   | \n",
      " |    critic_loss     | 1.0035   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 941899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=943000, episode_reward=371.94 +/- 1.0121827815422684 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 371.94   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 943000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.6129   | \n",
      " |    critic_loss     | 0.6532   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 942899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=944000, episode_reward=377.99 +/- 1.7152179507495835 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 377.99   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 944000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.5819   | \n",
      " |    critic_loss     | 0.8955   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 943899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=945000, episode_reward=384.03 +/- 1.7179807570916092 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 384.03   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 945000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.6864   | \n",
      " |    critic_loss     | 0.9216   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 944899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=946000, episode_reward=390.08 +/- 1.247161693929645 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 390.08   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 946000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.8627   | \n",
      " |    critic_loss     | 0.7157   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 945899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=947000, episode_reward=390.35 +/- 1.1373441909630206 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 390.35   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 947000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.3773   | \n",
      " |    critic_loss     | 0.3443   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 946899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=948000, episode_reward=390.62 +/- 1.464844281813991 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 390.62   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 948000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.5239   | \n",
      " |    critic_loss     | 0.8810   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 947899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=949000, episode_reward=390.9 +/- 1.043596915100776 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 390.9    | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 949000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 0.6776   | \n",
      " |    critic_loss     | 0.1694   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 948899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=950000, episode_reward=391.17 +/- 1.1086127563820538 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 391.17   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 950000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.5053   | \n",
      " |    critic_loss     | 0.3763   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 949899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=951000, episode_reward=391.44 +/- 1.4894939215595098 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 391.44   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 951000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.7146   | \n",
      " |    critic_loss     | 0.9286   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 950899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=952000, episode_reward=392.38 +/- 1.7012193642458686 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 392.38   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 952000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.7729   | \n",
      " |    critic_loss     | 0.4432   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 951899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=953000, episode_reward=393.31 +/- 1.2948963631181818 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 393.31   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 953000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.1679   | \n",
      " |    critic_loss     | 0.2920   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 952899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=954000, episode_reward=394.25 +/- 1.3658005415898713 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 394.25   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 954000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.0967   | \n",
      " |    critic_loss     | 0.5242   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 953899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=955000, episode_reward=395.19 +/- 1.4493278947321238 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 395.19   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 955000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.0552   | \n",
      " |    critic_loss     | 0.7638   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 954899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=956000, episode_reward=396.12 +/- 1.6443629665443278 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 396.12   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 956000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.2112   | \n",
      " |    critic_loss     | 0.3028   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 955899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=957000, episode_reward=394.6 +/- 1.560469320014744 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 394.6    | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 957000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.1301   | \n",
      " |    critic_loss     | 0.7825   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 956899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=958000, episode_reward=393.07 +/- 1.9690220957564382 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 393.07   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 958000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 0.3804   | \n",
      " |    critic_loss     | 0.0951   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 957899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=959000, episode_reward=391.55 +/- 1.8382151460857556 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 391.55   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 959000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.8052   | \n",
      " |    critic_loss     | 0.9513   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 958899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=960000, episode_reward=390.02 +/- 1.469283672014857 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 390.02   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 960000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.5555   | \n",
      " |    critic_loss     | 0.3889   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 959899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=961000, episode_reward=388.49 +/- 1.6689797878306083 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 388.49   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 961000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.7962   | \n",
      " |    critic_loss     | 0.4491   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 960899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=962000, episode_reward=379.62 +/- 1.0875778649226606 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 379.62   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 962000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.7504   | \n",
      " |    critic_loss     | 0.6876   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 961899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=963000, episode_reward=370.75 +/- 1.4133233662216544 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 370.75   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 963000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 0.4617   | \n",
      " |    critic_loss     | 0.1154   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 962899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=964000, episode_reward=361.88 +/- 1.6811232968828123 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 361.88   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 964000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 0.7333   | \n",
      " |    critic_loss     | 0.1833   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 963899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=965000, episode_reward=353.0 +/- 1.7377010470201513 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 353.0    | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 965000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.4285   | \n",
      " |    critic_loss     | 0.6071   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 964899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=966000, episode_reward=344.13 +/- 1.7230293614024033 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 344.13   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 966000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.3669   | \n",
      " |    critic_loss     | 0.5917   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 965899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=967000, episode_reward=353.82 +/- 1.3992725505985404 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 353.82   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 967000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.4457   | \n",
      " |    critic_loss     | 0.3614   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 966899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=968000, episode_reward=363.5 +/- 1.1824909520699498 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 363.5    | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 968000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.3154   | \n",
      " |    critic_loss     | 0.5789   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 967899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=969000, episode_reward=373.19 +/- 1.2673874062625745 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 373.19   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 969000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 0.9628   | \n",
      " |    critic_loss     | 0.2407   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 968899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=970000, episode_reward=382.87 +/- 1.5017811817328024 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 382.87   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 970000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.0316   | \n",
      " |    critic_loss     | 0.7579   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 969899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=971000, episode_reward=392.55 +/- 1.4323228551334977 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 392.55   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 971000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.7279   | \n",
      " |    critic_loss     | 0.6820   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 970899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=972000, episode_reward=386.13 +/- 1.488908360722827 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 386.13   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 972000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 0.4000   | \n",
      " |    critic_loss     | 0.1000   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 971899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=973000, episode_reward=379.7 +/- 1.954961075994278 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 379.7    | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 973000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.5494   | \n",
      " |    critic_loss     | 0.8874   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 972899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=974000, episode_reward=373.28 +/- 1.6628147957362778 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 373.28   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 974000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.8469   | \n",
      " |    critic_loss     | 0.9617   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 973899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=975000, episode_reward=366.85 +/- 1.4021693206091799 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 366.85   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 975000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.3988   | \n",
      " |    critic_loss     | 0.8497   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 974899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=976000, episode_reward=360.43 +/- 1.1132783761844092 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 360.43   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 976000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.4919   | \n",
      " |    critic_loss     | 0.3730   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 975899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=977000, episode_reward=373.04 +/- 1.9043747608900934 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 373.04   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 977000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.6908   | \n",
      " |    critic_loss     | 0.4227   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 976899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=978000, episode_reward=385.65 +/- 1.6308567254796769 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 385.65   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 978000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.4889   | \n",
      " |    critic_loss     | 0.8722   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 977899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=979000, episode_reward=398.26 +/- 1.4562466340308453 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 398.26   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 979000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 0.6527   | \n",
      " |    critic_loss     | 0.1632   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 978899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=980000, episode_reward=410.87 +/- 1.8682342291587402 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 410.87   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 980000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.9131   | \n",
      " |    critic_loss     | 0.7283   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 979899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=981000, episode_reward=423.48 +/- 1.6823139658827018 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 423.48   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 981000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.3692   | \n",
      " |    critic_loss     | 0.3423   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 980899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=982000, episode_reward=413.19 +/- 1.630318666761188 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 413.19   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 982000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 0.6480   | \n",
      " |    critic_loss     | 0.1620   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 981899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=983000, episode_reward=402.9 +/- 1.9852419615681336 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 402.9    | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 983000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 0.3351   | \n",
      " |    critic_loss     | 0.0838   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 982899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=984000, episode_reward=392.61 +/- 1.8606839313462722 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 392.61   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 984000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.4976   | \n",
      " |    critic_loss     | 0.6244   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 983899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=985000, episode_reward=382.32 +/- 1.6967721744483155 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 382.32   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 985000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.2678   | \n",
      " |    critic_loss     | 0.3169   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 984899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=986000, episode_reward=372.02 +/- 1.4619482908744272 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 372.02   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 986000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.2068   | \n",
      " |    critic_loss     | 0.5517   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 985899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=987000, episode_reward=378.22 +/- 1.9219105157482903 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 378.22   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 987000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.7984   | \n",
      " |    critic_loss     | 0.4496   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 986899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=988000, episode_reward=384.43 +/- 1.618725134435671 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 384.43   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 988000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.4534   | \n",
      " |    critic_loss     | 0.6134   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 987899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=989000, episode_reward=390.63 +/- 1.868094287831326 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 390.63   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 989000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 0.4528   | \n",
      " |    critic_loss     | 0.1132   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 988899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=990000, episode_reward=396.83 +/- 1.8173797671046463 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 396.83   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 990000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 0.4990   | \n",
      " |    critic_loss     | 0.1248   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 989899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=991000, episode_reward=403.03 +/- 1.9945447474289821 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 403.03   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 991000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.1067   | \n",
      " |    critic_loss     | 0.2767   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 990899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=992000, episode_reward=401.98 +/- 1.1372318365452114 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 401.98   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 992000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 0.6424   | \n",
      " |    critic_loss     | 0.1606   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 991899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=993000, episode_reward=400.93 +/- 1.3790646213082753 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 400.93   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 993000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.3222   | \n",
      " |    critic_loss     | 0.5806   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 992899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=994000, episode_reward=399.89 +/- 1.009894992714286 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 399.89   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 994000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.5559   | \n",
      " |    critic_loss     | 0.6390   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 993899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=995000, episode_reward=398.84 +/- 1.8412823534769767 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 398.84   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 995000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.7068   | \n",
      " |    critic_loss     | 0.9267   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 994899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=996000, episode_reward=397.79 +/- 1.126059143338745 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 397.79   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 996000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 0.2923   | \n",
      " |    critic_loss     | 0.0731   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 995899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=997000, episode_reward=397.83 +/- 1.4850183557568404 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 397.83   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 997000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 0.7807   | \n",
      " |    critic_loss     | 0.1952   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 996899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=998000, episode_reward=397.86 +/- 1.6075265822965794 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 397.86   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 998000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.2213   | \n",
      " |    critic_loss     | 0.8053   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 997899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=999000, episode_reward=397.89 +/- 1.0070337802277929 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 397.89   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 999000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.7402   | \n",
      " |    critic_loss     | 0.4351   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 998899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=1000000, episode_reward=397.93 +/- 1.951403022951164 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     | 397.93   | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 1000000  | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.9209   | \n",
      " |    critic_loss     | 0.7302   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 999899   | \n",
      " --------------------------------- \n"
     ]
    },
    {
     "data": {},
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Start training\n",
    "time_steps = 1000000\n",
    "model.learn(\n",
    "    total_timesteps=time_steps, callback=eval_callback, log_interval=10\n",
    ")\n",
    "model.save(\"artifacts/model/td3_car_racing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This last section of the notebook contains a visualization of the mean reward for each episode during training and information on the best performing model checkpoint.\n",
    "\n",
    "Finally, we created a visualization tool that generates an animated sample interaction of the agent with the environment which is saved locally in `artifacts/animation/td3_car_racer.gif`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from stable_baselines3.common import results_plotter\n",
    "from stable_baselines3.common.monitor import load_results\n",
    "from utils.visualization import render_example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the plot below we can see that after initial stagnation, during which we assume that the agent learns establishes a fundamental representation of the environment we see a relatively stable increase in the mean episode reward. This reward does not seem to be fully converged and could potentially be improved by training longer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results_plotter.plot_results(\n",
    "    dirs=[log_dir],\n",
    "    num_timesteps=time_steps,\n",
    "    x_axis=results_plotter.X_EPISODES,\n",
    "    task_name=\"TD3 Training on Car Racing V2\",\n",
    "    figsize=(10, 4),\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model performance:\n",
      "- Best mean_reward is  444.361\n",
      "- Reached at episode  850\n",
      "- Checkpoint:  ./artifacts/model/best_model.pkl\n"
     ]
    }
   ],
   "source": [
    "results_df = load_results(path=log_dir)\n",
    "best_episode = results_df[\"r\"].idxmax()\n",
    "print(\"Best model performance:\")\n",
    "print(\"- Best mean_reward is \", results_df.loc[best_episode].r)\n",
    "print(\"- Reached at episode \", best_episode)\n",
    "print(\"- Checkpoint: \", best_path + \"best_model.pks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved evaluation GIF to \"artifacts/td3_car_racer.gif\""
     ]
    }
   ],
   "source": [
    "# Save as gif\n",
    "render_example(\n",
    "    model=model,\n",
    "    env_id=env_id,\n",
    "    continuous=True,\n",
    "    render_frames=True,\n",
    "    output_file=\"artifacts/animation/td3_car_racer.gif\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
