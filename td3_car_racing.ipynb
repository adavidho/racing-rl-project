{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TD3 on Car Racing V2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains all the code required to train a Twin Delayed Deep Deterministic Policy Gradient (TD3) algorithm on the CarRacing-v2 Box2D environment. The TD3 algorithm is an extension of the Deep Deterministic Policy Gradient (DDPG) method, designed to address the overestimation bias in Q-learning, which can lead to more stable training and improved performance. To further ensure convergence we conduct hyperparameter tuning using the syne-tune package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # General\n",
    "import platform\n",
    "assert platform.python_version() == \"3.10.14\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convergence and stability are common challenges in reinforcement learning. To address this, we employ the Syne Tune package for hyperparameter tuning. Syne Tune is an efficient and flexible tool for automatic hyperparameter optimization, which helps us identify the optimal set of parameters for our TD3 algorithm on the CarRacing-v2 environment. \n",
    "\n",
    "Specifically, we utilize the Asynchronous Successive Halving Algorithm (ASHA), which is designed to allocate resources to promising configurations early and terminate underperforming ones, thereby speeding up the hyperparameter search process. By fine-tuning these hyperparameters, we aim to achieve robust and consistent training results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/adavidho/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from syne_tune import Tuner\n",
    "from syne_tune.backend import PythonBackend\n",
    "from syne_tune.experiments import load_experiment\n",
    "from syne_tune.config_space import loguniform, uniform\n",
    "from syne_tune.optimizer.baselines import ASHA\n",
    "from syne_tune.stopping_criterion import StoppingCriterion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we have to define the search space. Here we choose the learning rate, the Polyak update coefficient ($\\tau$) and the discount factor ($\\gamma$) to be the most important hyperparameters to ensure stable training. The ranges are set based on intuition and defaults provided by the stable_baselines3 package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the hyperparameter search space\n",
    "config_space = {\n",
    "    \"learning_rate\": loguniform(1e-8, 0.1),\n",
    "    \"tau\":  loguniform(1e-8, 1),\n",
    "    \"gamma\": uniform(0.9, 0.999),    \n",
    "    \"steps\": 100000 \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we define the tuning function `train_hpo_model()` which will be called inside worker processes spawned by the Syne-Tune scheduler. As `train_hpo_model()` will run inside its on process we need to define all relevant imports inside this function. \n",
    "\n",
    "The Syne-Tune scheduler will then pass a sampled hyperparameter configuration to the function, which will create an instance of a stable-baselines3 TD3 model and start training it with a reporter callback which communicates with the main tuning process. Note that we use the 'CnnPolicy' as the observed states are images of the environment and that we add noise to actions to facilitate exploration during training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the tuning function\n",
    "def train_hpo_model(learning_rate: float, tau: float, gamma: float, steps: int):\n",
    "    # Worker imports\n",
    "    import numpy as np\n",
    "    from stable_baselines3.common.env_util import make_vec_env\n",
    "    from stable_baselines3.common.callbacks import BaseCallback\n",
    "    from stable_baselines3.common.noise import NormalActionNoise\n",
    "    from stable_baselines3 import TD3\n",
    "    \n",
    "    from syne_tune import Reporter\n",
    "\n",
    "    # Create the vectorized environment\n",
    "    env_id = \"CarRacing-v2\"\n",
    "    vec_env = make_vec_env(env_id, n_envs=4)\n",
    "    \n",
    "    # Initialize the PPO agent with the given hyperparameters\n",
    "    n_actions = vec_env.action_space.shape[-1]\n",
    "    action_noise = NormalActionNoise(mean=np.zeros(n_actions), sigma=0.1 * np.ones(n_actions))  \n",
    "    model = TD3(\"CnnPolicy\", vec_env,  \n",
    "                action_noise=action_noise,\n",
    "                learning_rate=learning_rate,\n",
    "                tau=tau,\n",
    "                gamma=gamma,\n",
    "                batch_size=32,\n",
    "                verbose=1)\n",
    "\n",
    "    report = Reporter()\n",
    "    class WorkerCallback(BaseCallback):\n",
    "        def _on_step(self) -> bool:\n",
    "            # Log the mean reward\n",
    "            mean_reward = sum(self.locals[\"rewards\"]) / len(self.locals[\"rewards\"])\n",
    "            step = self.locals[\"num_collected_steps\"]\n",
    "            report(step=step, mean_reward=mean_reward)\n",
    "            return True \n",
    "    \n",
    "    # Train the agent\n",
    "    worker_callback = WorkerCallback()\n",
    "    model.learn(total_timesteps=steps, callback=worker_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = \"mean_reward\"\n",
    "scheduler = ASHA(\n",
    "    config_space,\n",
    "    metric=metric,\n",
    "    max_resource_attr=\"steps\",\n",
    "    resource_attr=\"step\",\n",
    "    mode=\"max\",\n",
    ")\n",
    "trial_backend = PythonBackend(\n",
    "    tune_function=train_hpo_model, config_space=config_space\n",
    ")\n",
    "stop_criterion = StoppingCriterion(\n",
    "    max_wallclock_time=61200, \n",
    ")\n",
    "tuner = Tuner(\n",
    "    trial_backend=trial_backend,\n",
    "    scheduler=scheduler,\n",
    "    stop_criterion=stop_criterion,\n",
    "    n_workers=8,\n",
    "    save_tuner=False,\n",
    "    wait_trial_completion_when_stopping=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above cell all necessary tuning objects are created and tuning is started in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "Resource summary (last result is reported):\n",
      " trial_id     status  iter  learning_rate          tau        gamma      steps  step  mean_reward worker-time\n",
      "        0  Completed   722    6.24041e-06  6.47157e-03  1.24004e-01    3613.77     1      -12.272    7227.53\n",
      "        1    Stopped  1414    3.56044e-07  4.32651e-03  8.32033e-01    5717.71     1       27.673    11435.43\n",
      "        2    Stopped   835    6.50236e-06  5.85811e-03  8.92359e-01     4393.6     1        9.095    8787.20\n",
      "        3    Stopped  2467    7.73109e-06  6.45598e-03  6.62475e-01    2084.03     1       -3.816    4168.07\n",
      "        4  Completed   862    4.02646e-06  5.81391e-03  3.68519e-01   10883.68     1       34.372    21767.37\n",
      "        5    Stopped  1325    5.97696e-06  4.04024e-03  4.86856e-01    4499.14     1      -19.657    8998.27\n",
      "        6  Completed  1821    9.98881e-06  9.98381e-03  8.54684e-01    6858.61     1       45.120    13717.22\n",
      "        7  Completed  1427    2.78481e-06  8.89039e-03  4.18345e-01    2884.93     1      -19.307    5769.86\n",
      "        8    Stopped  1817    4.73605e-06  3.67964e-03  4.64164e-01   12049.32     1       11.912    24098.64\n",
      "        9    Stopped  1091    5.28496e-07  1.66823e-03  1.26986e-01    2773.15     1      -23.989    5546.30\n",
      "       10    Stopped  2188    4.87616e-06  5.50376e-04  2.44002e-01   10578.73     1       -1.707    21157.46\n",
      "       11  Completed  2341    8.83730e-06  4.98778e-04  3.30698e-01    6791.18     1      -28.360    13582.37\n",
      "       12    Stopped  2044    1.22936e-06  3.67262e-03  9.75777e-01     837.31     1      -62.583    1674.62\n",
      "       13    Stopped  1316    6.29075e-07  5.69225e-04  2.49476e-01    2805.57     1       -3.496    5611.14\n",
      "       14    Stopped  1365    3.95296e-06  2.18375e-03  4.77990e-01    7439.09     1       -2.113    14878.19\n",
      "       15  Completed   973    8.10159e-06  1.44072e-03  9.95417e-01      805.8     1       12.824    1611.61\n",
      "       16  Completed   157    5.68705e-06  3.62683e-03  8.97050e-01    7237.77     1       16.703    14475.55\n",
      "       17    Stopped  1604    4.90286e-06  7.05757e-03  7.56047e-01   10804.81     1      -13.266    21609.62\n",
      "       18  Completed   898    1.17702e-06  7.98079e-03  8.52879e-01    4430.82     1      -58.269    8861.63\n",
      "       19    Stopped  1912    5.60174e-07  5.02038e-03  1.69449e-01   12145.83     1       44.858    24291.66\n",
      "       20    Stopped  2225    5.99133e-06  1.16868e-03  1.19305e-01    7280.14     1       22.413    14560.28\n",
      "       21    Stopped  2050    8.98671e-06  2.96221e-03  4.46012e-01   11073.59     1      -41.339    22147.19\n",
      "       22  Completed  2443    9.03882e-06  6.60901e-03  7.70986e-01     7309.4     1       48.967    14618.80\n",
      "       23    Stopped   930    2.92222e-06  2.40113e-03  7.53456e-01    7023.36     1       56.578    14046.72\n",
      "       24  Completed  2315    1.14592e-06  9.36184e-03  3.23604e-01     539.66     1        7.392    1079.33\n",
      "       25  Completed   386    5.79498e-06  8.53660e-03  4.63410e-01    5108.61     1      -12.577    10217.21\n",
      "       26    Stopped  1184    2.88652e-06  3.93405e-03  6.43194e-01    4239.29     1       13.501    8478.59\n",
      "       27  Completed  2295    5.43618e-06  8.38760e-03  7.29776e-01   11011.15     1        8.937    22022.30\n",
      "       28  Completed  1871    7.21641e-06  8.44071e-03  8.15263e-01    5845.21     1      -40.691    11690.41\n",
      "       29  Completed  1693    5.66721e-05  7.25243e-06  9.75882e-01    7622.53     1       70.810    15245.06\n",
      "       30    Stopped  2146    1.35898e-06  1.09638e-03  7.01068e-01     1058.6     1       23.348    2117.20\n",
      "       31  Completed  1716    7.68527e-06  3.65469e-03  5.54270e-01    6265.96     1       31.636    12531.93\n",
      "       32  Completed  1381    7.75148e-06  3.35904e-03  9.57527e-01    3317.16     1       48.246    6634.31\n",
      "       33  Completed  2153    7.71645e-06  9.61875e-03  9.89196e-01     944.05     1      -15.844    1888.09\n",
      "       34    Stopped  2163    8.64061e-06  6.59363e-04  6.83768e-01    3818.51     1      -34.560    7637.02\n",
      "       35  Completed  2135    7.82837e-06  6.68097e-03  2.52126e-01   10362.55     1       -1.491    20725.10\n",
      "       36    Stopped   660    9.90288e-06  5.08074e-03  9.99809e-01    7922.68     1      -23.132    15845.35\n",
      "       37  Completed   168    7.30991e-06  8.27546e-03  8.37721e-01    8478.52     1       35.710    16957.04\n",
      "       38  Completed  1044    1.95589e-06  4.35802e-03  4.35839e-01    8305.24     1       14.938    16610.49\n",
      "       39  Completed   166    6.57174e-06  7.92010e-03  6.08174e-01    6015.98     1      -17.968    12031.97\n",
      "       40  Completed    93    2.29203e-06  4.88890e-03  4.47251e-01    8248.37     1       -8.484    16496.73\n",
      "       41    Stopped   946    1.36808e-06  9.65646e-03  9.37398e-01    9509.53     1       10.868    19019.06\n",
      "       42    Stopped  1178    2.23001e-06  6.69421e-03  2.59999e-01    6508.06     1      -13.456    13016.12\n",
      "       43  Completed   376    8.82731e-06  9.01022e-06  5.47279e-01   10397.43     1        1.803    20794.85\n",
      "       44    Stopped   765    5.05626e-06  8.31930e-03  7.05706e-01    4762.76     1      -36.976    9525.52\n",
      "       45  Completed   389    5.42170e-06  2.65336e-03  3.98716e-01   11037.47     1       -0.312    22074.94\n",
      "       46    Stopped   798    3.78248e-06  9.79895e-03  8.66797e-01    4290.21     1       -3.765    8580.43\n",
      "       47    Stopped  1280    1.69824e-06  6.03534e-03  9.15037e-01    5898.94     1        0.646    11797.87\n",
      "       48    Stopped  1106    7.06350e-06  5.90139e-03  7.15573e-01    8641.07     1      -10.870    17282.15\n",
      "       49  Completed   185    8.28315e-06  7.96023e-03  3.91741e-01    5907.87     1       -8.686    11815.75\n",
      "       50  Completed  1078    6.66377e-06  8.82797e-03  1.50330e-01   12174.83     1       23.607    24349.66\n",
      "       51  Completed   721    7.64305e-06  4.10406e-03  3.49520e-01    7794.82     1      -29.200    15589.65\n",
      "       52  Completed   658    2.39349e-08  7.60775e-03  6.29480e-01   10052.34     1      -18.558    20104.69\n",
      "       53    Stopped  2038    7.10998e-06  6.47657e-03  4.83918e-01    3907.78     1       14.892    7815.56\n",
      "       54  Completed  2451    4.46613e-06  7.42828e-03  2.54152e-01   12052.21     1        2.208    24104.41\n",
      "       55  Completed  1154    1.02517e-06  9.33944e-03  7.58541e-01    8633.65     1      -19.547    17267.29\n",
      "       56  Completed  2497    5.98805e-06  4.20944e-03  9.18328e-01    10327.7     1       -4.255    20655.41\n",
      "       57    Stopped   660    5.18629e-06  4.66962e-03  3.87648e-01    1851.78     1      -14.722    3703.57\n",
      "       58    Stopped  1880    7.82141e-06  2.62296e-03  8.17927e-01   11797.07     1       53.178    23594.13\n",
      "       59  Completed  2205    2.19620e-06  7.29238e-03  5.36631e-01    7982.04     1       34.374    15964.07\n",
      "       60    Stopped   195    3.48713e-06  9.05772e-03  2.80232e-01    5639.68     1       16.890    11279.37\n",
      "       61    Stopped   498    1.13174e-06  3.81085e-03  3.45347e-01     1604.2     1       31.666    3208.41\n",
      "       62  Completed   963    3.83988e-06  1.86261e-03  8.87258e-01    5884.16     1       -0.899    11768.32\n",
      "       63    Stopped  1316    2.14188e-07  9.11084e-04  1.82960e-01    2043.03     1        7.876    4086.06\n",
      "       64  Completed  2127    6.82210e-06  9.63086e-03  7.14384e-01    4034.53     1      -19.230    8069.07\n",
      "       65    Stopped  1332    5.23143e-06  1.89841e-03  9.75487e-01    6617.07     1       24.431    13234.15\n",
      "       66  Completed  1995    3.22210e-06  1.47419e-03  8.04626e-01     8106.5     1        5.870    16213.00\n",
      "       67  Completed   260    1.05623e-06  7.57757e-03  7.34053e-01   10119.99     1       25.544    20239.98\n",
      "       68  Completed    75    7.30907e-08  5.92081e-03  9.39390e-01    6627.35     1      -24.581    13254.70\n",
      "       69    Stopped   315    7.54915e-06  2.66015e-03  3.40738e-01    6164.66     1      -56.964    12329.32\n",
      "       70    Stopped  1577    5.25673e-06  6.43938e-03  5.62476e-01   10538.51     1       47.905    21077.02\n",
      "       71    Stopped   184    2.67247e-06  1.51880e-03  6.64949e-01    9272.16     1        4.506    18544.32\n",
      "       72    Stopped  2041    4.10355e-06  8.97587e-03  8.30404e-01     6628.0     1      -21.178    13255.99\n",
      "       73  Completed  2154    3.29460e-06  5.22712e-03  2.91164e-01    4342.37     1       21.778    8684.73\n",
      "       74    Stopped   716    3.75059e-06  4.12764e-03  1.72498e-01    8402.39     1        7.212    16804.78\n",
      "       75    Stopped  2075    8.81555e-06  2.83335e-03  5.64329e-01    6416.72     1      -16.633    12833.44\n",
      "       76    Stopped  2187    8.10893e-06  9.45539e-03  6.72852e-01    1539.49     1        1.953    3078.98\n",
      "       77  Completed   196    7.62780e-06  3.37322e-03  7.86878e-01    1497.01     1      -16.263    2994.03\n",
      "       78  Completed  1040    8.00630e-06  5.23254e-03  3.27868e-01    9344.18     1      -28.163    18688.36\n",
      "       79    Stopped  2010    4.38024e-06  4.60631e-03  4.04183e-01   11939.77     1        0.257    23879.53\n",
      "       80    Stopped   891    4.76952e-07  5.44874e-03  3.85446e-01    9930.02     1      -15.789    19860.05\n",
      "       81  Completed  1123    2.94033e-06  1.59129e-03  9.54175e-01     351.89     1       20.855     703.77\n",
      "       82  Completed  1135    2.67643e-06  7.20623e-03  1.96423e-01    6020.36     1       -6.465    12040.73\n",
      "       83    Stopped   126    4.72605e-06  1.84490e-03  5.92360e-01    5978.89     1      -11.149    11957.78\n",
      "       84    Stopped  2169    4.93882e-07  6.65270e-04  3.57140e-01    11300.1     1       21.021    22600.19\n",
      "       85    Stopped   728    9.51760e-06  4.03714e-03  9.84795e-01    3660.87     1      -47.157    7321.73\n",
      "       86    Stopped   617    8.22877e-06  3.51245e-03  8.47793e-01    4480.94     1      -47.326    8961.88\n",
      "       87  Completed  1210    4.19116e-06  3.96142e-03  2.81338e-01    1477.53     1      -10.139    2955.07\n",
      "       88    Stopped  2062    2.27542e-06  7.68064e-03  2.56151e-01    7451.87     1      -32.665    14903.74\n",
      "       89  Completed  1919    5.96660e-06  3.66109e-03  2.78094e-01    1842.45     1        4.508    3684.90\n",
      "       90    Stopped  1325    4.07436e-06  3.32888e-03  4.00887e-01    4826.87     1       24.426    9653.74\n",
      "       91    Stopped  1153    2.67457e-06  6.89339e-03  1.89058e-01    3939.67     1        1.087    7879.35\n",
      "       92    Stopped   599    6.03634e-06  3.12219e-04  7.15713e-01    6560.01     1       33.254    13120.01\n",
      "       93  Completed   452    3.08889e-06  6.50895e-03  5.14751e-01     9804.4     1       32.151    19608.79\n",
      "       94    Stopped  1570    4.55246e-06  7.81224e-03  6.58062e-01    2115.27     1      -21.213    4230.53\n",
      "       95  Completed  1180    1.81890e-06  4.94529e-03  7.84817e-01     3712.9     1      -38.903    7425.80\n",
      "       96    Stopped  1126    7.47998e-06  9.45431e-03  9.53332e-01   10536.59     1       38.820    21073.18\n",
      "       97    Stopped   162    1.24253e-06  5.40505e-03  9.86645e-01    8839.39     1       19.633    17678.78\n",
      "       98  Completed  2113    2.23151e-06  1.77526e-03  7.17017e-01     2022.3     1       31.130    4044.59\n",
      "       99  Completed  1808    4.22603e-06  3.74989e-03  8.04304e-01    9527.22     1        5.092    19054.44\n",
      "      100    Stopped  1904    4.33020e-06  7.24163e-03  1.72503e-01    1493.17     1      -11.636    2986.33\n",
      "      101  Completed  2358    9.33789e-07  8.47270e-03  5.95539e-01    5169.52     1       59.163    10339.05\n",
      "      102    Stopped   404    7.97700e-06  2.26759e-03  1.29408e-01     8415.5     1       -6.721    16831.00\n",
      "      103  Completed  1336    8.57787e-06  7.76135e-03  3.13973e-01    6792.74     1       32.334    13585.48\n",
      "      104    Stopped  1243    9.42773e-06  9.29325e-03  2.18457e-01   11506.17     1       24.271    23012.35\n",
      "      105  Completed   663    9.14366e-06  8.68843e-03  4.57212e-01    8586.78     1       25.088    17173.55\n",
      "      106  Completed    95    3.91180e-07  5.42608e-03  2.60070e-01   10912.03     1      -26.770    21824.07\n",
      "      107    Stopped   538    2.70731e-06  3.05229e-03  7.76281e-01    8590.68     1        9.175    17181.36\n",
      "      108    Stopped  2165    7.63232e-06  8.94317e-03  2.07004e-01    1520.98     1        6.209    3041.96\n",
      "      109    Stopped  1502    7.92494e-06  3.29496e-03  3.07185e-01    6788.46     1      -59.335    13576.92\n",
      "      110  Completed  2095    7.34799e-06  9.65943e-04  4.64656e-01     7640.8     1      -69.054    15281.60\n",
      "      111  Completed  1404    1.53823e-06  5.88221e-03  1.67584e-01    8077.51     1      -57.966    16155.02\n",
      "      112  Completed   277    9.01184e-06  3.35425e-03  2.97300e-01    9022.94     1       12.157    18045.88\n",
      "      113  Completed  1349    8.62970e-06  2.81547e-03  8.62521e-01    1758.27     1       -1.799    3516.54\n",
      "      114  Completed   250    2.61279e-06  4.12311e-03  7.87423e-01   11208.59     1      -30.630    22417.17\n",
      "      115    Stopped  2067    6.19471e-07  5.67365e-03  9.47399e-01    9807.02     1        0.503    19614.04\n",
      "      116    Stopped   268    1.85809e-06  7.19060e-03  3.67389e-01    7242.07     1      -23.598    14484.14\n",
      "      117    Stopped   995    2.35152e-07  9.22740e-03  1.37424e-01    5353.04     1      -24.070    10706.09\n",
      "      118    Stopped   540    6.91571e-06  7.50103e-05  3.79314e-01     632.35     1       60.324    1264.71\n",
      "      119  Completed   697    9.18093e-07  6.73316e-03  6.22306e-01     8832.2     1       50.370    17664.41\n",
      "      120    Stopped   431    9.54817e-06  3.05182e-03  5.73283e-01    6912.94     1      -31.778    13825.88\n",
      "      121  Completed    30    1.67312e-06  2.44387e-03  5.00613e-01    8868.15     1      -29.782    17736.29\n",
      "      122    Stopped  1702    9.12985e-07  1.41097e-03  1.80224e-01   10621.53     1       53.257    21243.07\n",
      "      123  Completed   160    8.32075e-06  1.32838e-03  8.53003e-01    7448.78     1      -10.373    14897.55\n",
      "      124  Completed  2409    7.86685e-06  8.20195e-03  5.47356e-01    6301.12     1      -26.833    12602.24\n",
      "      125    Stopped  1700    3.67572e-07  6.06962e-03  6.57033e-01    1895.71     1       23.384    3791.43\n",
      "      126  Completed  1480    9.68481e-06  8.86388e-03  6.47007e-01     2074.9     1       -0.322    4149.79\n",
      "      127    Stopped   743    7.90591e-06  4.47473e-03  1.70229e-01    3718.93     1      -32.801    7437.86\n",
      "      128    Stopped  2276    8.13653e-06  5.08668e-03  2.76730e-01    4813.69     1        7.853    9627.38\n",
      "      129  Completed  1420    9.16193e-06  2.18278e-03  6.57766e-01    4100.22     1      -17.723    8200.43\n",
      "      130    Stopped   755    2.97659e-06  3.76758e-03  1.86631e-01    4524.26     1       12.988    9048.51\n",
      "      131  Completed  1607    9.13485e-06  8.48091e-04  3.63070e-01    1066.48     1       10.488    2132.95\n",
      "      132    Stopped   232    8.63857e-06  5.80999e-03  9.14507e-01    3677.14     1      -12.312    7354.28\n",
      "      133    Stopped   905    8.29899e-07  7.58721e-03  8.80749e-01   10516.23     1      -12.498    21032.45\n",
      "      134    Stopped   561    2.44171e-06  2.60526e-03  3.71141e-01   11759.55     1      -40.719    23519.11\n",
      "      135  Completed  1722    5.35142e-06  1.10330e-03  7.15397e-01    6568.47     1       21.446    13136.93\n",
      "      136  Completed  2446    4.42568e-06  3.34307e-03  2.74323e-01    1220.11     1       22.156    2440.21\n",
      "      137    Stopped  1157    1.98385e-06  8.71310e-03  5.35181e-01    8407.72     1      -22.296    16815.44\n",
      "      138  Completed   583    3.37668e-06  2.83202e-03  4.08472e-01    5991.31     1       -4.887    11982.61\n",
      "      139  Completed   577    6.97295e-06  5.17885e-03  5.68773e-01    2264.31     1        1.358    4528.61\n",
      "      140  Completed  1969    9.50635e-06  3.64028e-03  5.24552e-01    6488.16     1      -53.790    12976.31\n",
      "      141    Stopped   538    8.29141e-07  8.67879e-03  4.05277e-01   12204.47     1      -30.910    24408.94\n",
      "      142    Stopped   556    6.72550e-06  5.08169e-03  1.00243e-01    1956.11     1       -2.933    3912.23\n",
      "      143    Stopped  1261    2.52661e-06  5.18996e-03  9.36971e-01     4429.1     1      -36.563    8858.19\n",
      "      144  Completed  1353    2.73115e-06  4.21235e-03  3.97836e-01   11946.29     1      -10.306    23892.58\n",
      "      145    Stopped   898    1.03911e-07  2.02930e-03  8.77179e-01   10635.03     1       -2.554    21270.06\n",
      "      146  Completed  1138    9.95287e-06  9.62555e-03  2.42661e-01    8129.26     1       36.389    16258.52\n",
      "      147  Completed   449    6.44338e-06  2.83240e-03  5.28439e-01    6529.82     1       -6.671    13059.64\n",
      "      148    Stopped  1290    6.62039e-06  4.86297e-03  8.21816e-01   11318.79     1      -49.283    22637.59\n",
      "      149    Stopped   434    6.73864e-06  9.19746e-03  5.04882e-01    9920.36     1        7.589    19840.72\n",
      "      150    Stopped  1835    8.90677e-06  6.75702e-03  8.20689e-01    8905.62     1      -36.099    17811.25\n",
      "      151  Completed  1895    2.02345e-06  3.79221e-03  5.74107e-01    6843.13     1       11.417    13686.25\n",
      "      152    Stopped  1849    1.18913e-06  7.45149e-03  9.98928e-01    5812.93     1      -11.774    11625.87\n",
      "      153    Stopped  2091    5.60829e-06  6.22188e-03  6.41073e-01    4032.08     1       26.543    8064.17\n",
      "      154    Stopped   915    7.52932e-06  3.32213e-03  1.50135e-01    1642.55     1      -40.153    3285.10\n",
      "      155    Stopped  1336    2.33540e-06  8.20847e-03  2.00541e-01    6224.03     1       47.961    12448.07\n",
      "      156  Completed  1026    6.50126e-07  8.76300e-04  6.01368e-01    4530.41     1       -1.129    9060.83\n",
      "      157    Stopped   867    7.29355e-06  8.51265e-04  1.31333e-01    3423.39     1      -21.167    6846.78\n",
      "      158    Stopped  1949    8.71928e-06  6.76137e-03  7.75058e-01    1517.84     1       -0.529    3035.67\n",
      "      159    Stopped  2047    7.79838e-06  2.85620e-03  3.16363e-01    8018.41     1      -44.135    16036.82\n",
      "      160    Stopped  1490    2.10331e-06  1.41195e-03  4.41081e-01    7140.23     1      -38.929    14280.45\n",
      "      161    Stopped    79    9.26865e-06  4.84408e-03  8.02573e-01   12130.52     1      -26.700    24261.03\n",
      "      162  Completed  1665    1.09926e-06  5.46290e-03  8.64458e-01    2366.64     1        6.159    4733.29\n",
      "      163    Stopped  1692    7.21963e-07  5.90912e-03  3.63093e-01    9258.24     1        5.433    18516.47\n",
      "      164  Completed  2251    9.09227e-06  7.16805e-03  7.24615e-01    10277.1     1        4.377    20554.20\n",
      "      165    Stopped  1726    2.07366e-06  5.33543e-05  1.21956e-01    7923.33     1      -38.509    15846.65\n",
      "      166    Stopped  1666    8.87345e-06  7.69484e-03  5.21077e-01    1379.32     1      -29.065    2758.64\n",
      "      167  Completed  2304    3.11492e-06  6.99606e-03  8.98044e-01    5558.43     1      -41.209    11116.87\n",
      "      168    Stopped  2047    1.75371e-06  1.69017e-03  9.45635e-01   10063.77     1        9.140    20127.55\n",
      "      169  Completed  1831    4.14023e-06  6.25268e-03  5.51446e-01     4019.8     1      -59.168    8039.61\n",
      "      170    Stopped   771    5.48606e-06  9.85191e-03  7.32674e-01    1245.04     1      -20.297    2490.08\n",
      "      171    Stopped  1133    7.41520e-06  2.36396e-03  6.08717e-01     670.34     1       -6.609    1340.67\n",
      "      172    Stopped  2052    1.04732e-06  1.41839e-03  7.59304e-01    5709.18     1        4.479    11418.37\n",
      "      173  Completed  2051    8.58197e-06  6.44530e-03  8.00382e-01    6193.61     1        0.169    12387.23\n",
      "      174  Completed  1580    8.09552e-06  4.45908e-03  8.91297e-01    9885.45     1      -27.128    19770.91\n",
      "      175    Stopped  1712    2.04756e-06  2.10894e-03  4.68353e-01    3641.48     1        8.063    7282.96\n",
      "      176  Completed  1874    4.76007e-06  1.08988e-03  8.35472e-01    6764.68     1       18.389    13529.37\n",
      "      177    Stopped  2270    2.32294e-06  2.60994e-03  9.11894e-01    2793.66     1      -24.174    5587.33\n",
      "      178    Stopped   552    4.54968e-06  1.54174e-03  2.41972e-01   12195.15     1      -30.990    24390.31\n",
      "      179    Stopped  1588    2.01095e-06  6.59825e-03  4.03878e-01    2404.04     1       31.872    4808.08\n",
      "      180  Completed  2312    7.70748e-06  9.68286e-03  7.47744e-01     9522.3     1        5.010    19044.59\n",
      "      181    Stopped  2323    5.03721e-06  8.74130e-03  2.13264e-01    2657.35     1       52.390    5314.70\n",
      "      182  Completed  1356    8.44378e-06  4.73303e-03  1.64795e-01   11043.93     1      -18.557    22087.87\n",
      "      183    Stopped  1750    9.32770e-06  9.48609e-04  6.99474e-01    1617.74     1       -4.179    3235.48\n",
      "      184    Stopped   537    6.71667e-06  6.57741e-03  9.96689e-01     6190.9     1       52.469    12381.79\n",
      "      185  Completed   160    3.40244e-06  1.73203e-03  2.86889e-01     8609.1     1        8.020    17218.20\n",
      "      186    Stopped  1851    4.13553e-06  1.79214e-03  1.19154e-01    6175.57     1       33.922    12351.14\n",
      "      187    Stopped   467    8.31047e-06  8.59668e-03  7.57647e-01     868.64     1       19.937    1737.28\n",
      "      188  Completed  2100    9.96434e-06  8.29562e-03  3.71859e-01    4113.49     1      -10.587    8226.98\n",
      "      189    Stopped   239    1.04386e-06  5.37521e-03  6.05058e-01    6072.31     1       20.965    12144.62\n",
      "      190  Completed   246    7.27842e-06  6.24082e-03  6.87265e-01    5155.45     1      -55.962    10310.91\n",
      "      191    Stopped  1353    2.56742e-06  8.03380e-03  2.85851e-01    9498.01     1       45.168    18996.02\n",
      "      192  Completed  1990    5.23011e-06  6.11715e-03  6.12968e-01   10407.84     1      -20.927    20815.69\n",
      "      193  Completed  1698    6.63541e-06  9.41759e-03  5.57668e-01    9810.18     1       30.478    19620.35\n",
      "      194    Stopped   995    3.01474e-06  7.79134e-03  9.43152e-01     1054.4     1       -0.559    2108.81\n",
      "      195  Completed  1729    4.65000e-07  3.04267e-03  9.63281e-01    5275.54     1       -0.146    10551.09\n",
      "      196    Stopped  1714    8.40312e-06  2.69227e-03  4.25801e-01     6787.2     1        3.630    13574.39\n",
      "      197  Completed    77    5.46466e-06  4.51708e-03  7.04096e-01     9523.2     1        8.704    19046.41\n",
      "      198    Stopped  1737    2.30305e-07  4.70876e-04  2.12032e-01    3462.11     1      -15.243    6924.21\n",
      "      199    Stopped  2223    2.10443e-06  3.00367e-03  3.77241e-01    4444.19     1       21.979    8888.37\n",
      "      200  Completed  1800    2.33201e-06  2.19815e-03  7.79221e-01     471.96     1      -14.435     943.92\n",
      "      201    Stopped  2102    5.76446e-06  9.63759e-04  9.98875e-01    5821.15     1       22.137    11642.30\n",
      "      202    Stopped  1267    6.37254e-06  8.99614e-03  6.97910e-01   10943.03     1       23.410    21886.07\n",
      "      203  Completed   759    2.84662e-06  4.63911e-03  4.63261e-01   10899.55     1      -45.987    21799.09\n",
      "      204    Stopped   681    9.54177e-06  1.45141e-03  7.08519e-01   12020.96     1       -6.416    24041.92\n",
      "      205    Stopped  1632    9.48011e-06  7.24315e-03  3.71475e-01    2775.44     1        3.884    5550.88\n",
      "      206    Stopped   302    8.03922e-06  4.98939e-03  8.30212e-01    5644.66     1      -15.378    11289.33\n",
      "      207    Stopped   663    5.56265e-06  5.29038e-03  8.93862e-01    2146.64     1       -2.413    4293.29\n",
      "      208    Stopped    40    5.97699e-06  4.18374e-03  7.01460e-01    2245.98     1       20.961    4491.97\n",
      "      209  Completed  1461    1.84652e-07  8.96124e-03  5.65591e-01   10190.31     1      -33.199    20380.63\n",
      "      210  Completed   826    8.26749e-06  7.08389e-03  1.20048e-01    7262.39     1       13.058    14524.78\n",
      "      211  Completed   698    6.03217e-06  4.49591e-03  9.56657e-01     762.57     1        1.299    1525.13\n",
      "      212  Completed  1908    2.70019e-06  7.90573e-03  9.74306e-01    3339.52     1       24.559    6679.04\n",
      "      213  Completed   587    5.45356e-06  7.99876e-03  4.33041e-01    9100.64     1      -37.981    18201.28\n",
      "      214    Stopped   434    6.83329e-06  6.36320e-03  6.85080e-01    5341.33     1      -44.323    10682.66\n",
      "      215  Completed  1699    1.62305e-06  9.77713e-03  7.90530e-01    2790.23     1      -12.001    5580.45\n",
      "      216    Stopped  1997    5.75154e-06  6.12347e-03  6.17784e-01    2871.84     1      -16.882    5743.69\n",
      "      217  Completed  1207    3.19050e-06  7.31629e-05  6.27881e-01   10280.23     1       26.839    20560.46\n",
      "      218  Completed  1975    1.00478e-06  6.24452e-03  5.77634e-01   11794.98     1       43.307    23589.95\n",
      "      219  Completed  1804    2.17689e-06  2.75731e-03  6.21034e-01   12188.86     1       20.833    24377.72\n",
      "      220  Completed  1969    1.17498e-06  7.61240e-03  7.37391e-01    5108.66     1      -16.771    10217.31\n",
      "      221  Completed   760    2.70686e-06  8.25983e-03  8.26922e-01     5039.1     1       -3.336    10078.19\n",
      "      222    Stopped  2017    9.01870e-06  3.56148e-03  6.23819e-01    1878.82     1      -30.676    3757.65\n",
      "      223    Stopped  1327    7.98600e-07  1.25530e-03  5.93835e-01     4091.6     1       18.604    8183.20\n",
      "      224    Stopped  2059    7.39383e-06  3.74279e-03  5.97298e-01   12018.58     1       17.768    24037.17\n",
      "      225  Completed  2468    3.26998e-06  3.95117e-03  9.58208e-01   10170.85     1       20.609    20341.70\n",
      "      226    Stopped   700    8.24437e-07  3.68942e-03  1.41177e-01    4323.93     1       15.787    8647.86\n",
      "      227  Completed  2032    1.49280e-06  1.93353e-03  1.50436e-01    6279.11     1       -4.234    12558.21\n",
      "      228  Completed   607    4.82023e-06  4.21004e-03  6.97032e-01    4055.62     1       31.963    8111.24\n",
      "      229  Completed   902    9.62996e-06  8.80865e-03  8.15675e-01    7283.55     1       -0.064    14567.10\n",
      "      230  Completed   370    9.48233e-06  4.84497e-03  7.96677e-01    1268.18     1       51.130    2536.37\n",
      "      231    Stopped  2131    3.79997e-06  6.89360e-03  6.60577e-01    7626.65     1      -42.710    15253.29\n",
      "      232  Completed  1391    5.25724e-06  5.08395e-03  1.40141e-01    6792.26     1      -20.998    13584.51\n",
      "      233    Stopped  2037    3.29754e-06  3.85976e-03  7.72383e-01     4141.4     1       21.237    8282.81\n",
      "      234    Stopped    56    6.43532e-06  4.69690e-03  5.13081e-01   10127.29     1      -27.177    20254.58\n",
      "      235  Completed  2126    5.04408e-06  2.10847e-03  3.57215e-01    1417.49     1      -39.882    2834.98\n",
      "      236    Stopped   365    1.04463e-06  3.06706e-03  9.40047e-01    6627.76     1      -48.505    13255.52\n",
      "      237    Stopped  1008    9.13622e-06  8.99929e-03  1.19876e-01    7029.45     1       22.812    14058.90\n",
      "      238  Completed   634    6.91284e-06  4.08797e-03  8.13070e-01    1953.35     1       -1.039    3906.70\n",
      "      239  Completed  1637    8.89611e-06  4.31143e-03  8.20458e-01    3678.83     1       27.047    7357.66\n",
      "      240    Stopped   516    5.63956e-06  7.70190e-04  6.84840e-01    1247.84     1        2.849    2495.68\n",
      "      241    Stopped   551    6.75380e-06  6.52752e-03  1.61932e-01    3555.13     1       42.518    7110.26\n",
      "      242  Completed   880    2.24335e-06  4.36274e-03  6.01247e-01    8831.99     1       13.569    17663.97\n",
      "      243    Stopped   408    1.92191e-07  9.57210e-03  2.25929e-01    8808.45     1      -29.841    17616.90\n",
      "      244    Stopped  1025    9.76674e-06  2.95389e-03  2.96890e-01    3808.87     1      -12.482    7617.75\n",
      "      245  Completed  2372    4.95645e-06  7.82979e-03  1.00492e-01      444.1     1       -4.297     888.19\n",
      "      246    Stopped  1776    8.83781e-06  5.23053e-03  6.48848e-01   10324.32     1        7.169    20648.65\n",
      "      247  Completed   186    1.73195e-07  9.06641e-03  7.37848e-01    4405.99     1       49.723    8811.99\n",
      "      248    Stopped  1331    7.48422e-06  9.30914e-03  9.99116e-01    1653.04     1        9.853    3306.08\n",
      "      249    Stopped   732    9.48101e-06  7.61969e-03  4.00835e-01   12168.26     1      -23.073    24336.53\n",
      "      250    Stopped  1663    7.52407e-06  8.08980e-03  9.55066e-01     7681.0     1      -37.454    15362.00\n",
      "      251  Completed  1572    6.89713e-06  3.60414e-04  3.45385e-01    3873.16     1      -39.402    7746.32\n",
      "      252  Completed   773    2.43994e-06  7.18935e-03  3.89240e-01    4293.94     1      -32.272    8587.87\n",
      "      253  Completed  1754    2.66408e-06  6.39652e-03  3.36373e-01     1273.5     1      -11.409    2547.00\n",
      "      254  Completed    86    2.75389e-06  2.42563e-03  2.86085e-01    1960.69     1       31.639    3921.37\n",
      "      255  Completed  1470    6.52620e-06  5.37327e-03  7.37246e-01   10911.78     1      -18.499    21823.56\n",
      "      256  Completed  1729    9.88597e-06  7.64097e-03  2.95514e-01     993.86     1       52.853    1987.71\n",
      "      257  Completed  2155    5.11854e-06  8.97129e-04  9.90279e-01    6634.59     1       23.641    13269.19\n",
      "      258    Stopped  1093    1.84750e-06  3.20633e-03  7.23537e-01    9113.35     1        1.914    18226.70\n",
      "      259    Stopped  1277    9.64989e-06  7.04090e-04  1.10679e-01    8064.97     1        2.803    16129.94\n",
      "      260  Completed  1405    6.43605e-06  2.28653e-03  5.63854e-01    9419.86     1      -14.597    18839.73\n",
      "      261    Stopped   802    2.73871e-06  4.76174e-03  6.07723e-01    4641.83     1       26.939    9283.65\n",
      "      262    Stopped  1132    4.08687e-06  9.13043e-03  6.91392e-01    4996.89     1       31.740    9993.78\n",
      "      263  Completed  1648    9.73269e-06  2.44447e-04  9.56044e-01   11501.93     1      -37.360    23003.87\n",
      "      264    Stopped  1176    5.27858e-06  1.22741e-03  7.96423e-01    7729.06     1        8.659    15458.12\n",
      "      265    Stopped   581    2.09339e-06  2.17680e-03  8.09880e-01    3579.66     1       57.209    7159.32\n",
      "      266  Completed  1706    8.79392e-06  1.99245e-03  7.40717e-01    4995.17     1      -13.016    9990.34\n",
      "      267    Stopped   793    5.51680e-06  7.88557e-04  3.69993e-01   11777.02     1       29.768    23554.03\n",
      "      268  Completed   560    9.36047e-06  1.67362e-03  7.74424e-01     7979.2     1      -13.472    15958.39\n",
      "      269    Stopped  1392    6.97623e-07  4.15293e-03  8.68196e-01    7527.63     1      -26.319    15055.26\n",
      "      270  Completed  1336    2.13318e-06  3.43283e-03  4.22351e-01    1712.64     1       27.869    3425.28\n",
      "      271    Stopped  2051    5.23034e-06  6.46522e-03  4.62936e-01   11066.56     1        2.187    22133.13\n",
      "      272  Completed  1431    8.32430e-06  8.19872e-03  6.12486e-01     1293.9     1       24.369    2587.80\n",
      "      273    Stopped   873    1.47476e-06  8.96466e-03  7.65837e-01    4013.94     1       20.007    8027.89\n",
      "      274    Stopped  1251    1.57770e-07  8.19364e-03  2.31945e-01     4220.7     1       24.346    8441.41\n",
      "      275    Stopped  2166    6.57598e-06  1.99330e-03  8.92277e-01    2717.22     1       27.454    5434.45\n",
      "      276  Completed  2482    7.22163e-06  9.20265e-03  4.19885e-01    3645.84     1      -21.089    7291.69\n",
      "      277  Completed   438    8.61930e-06  1.27202e-03  2.51194e-01   11177.09     1       -3.053    22354.18\n",
      "      278    Stopped  1182    5.80477e-06  7.29391e-04  1.52908e-01    3415.34     1      -26.383    6830.68\n",
      "      279  Completed  1386    1.71150e-06  2.37047e-03  4.39033e-01   10023.57     1        5.978    20047.13\n",
      "      280  Completed  1803    1.74526e-06  9.37928e-03  5.90945e-01    5504.45     1      -36.494    11008.91\n",
      "      281  Completed   762    1.86790e-06  7.65049e-03  6.28219e-01    1393.12     1       -0.534    2786.25\n",
      "      282  Completed   311    9.62336e-06  1.96833e-03  7.75207e-01    4883.13     1        7.821    9766.26\n",
      "      283  Completed  1366    3.39816e-06  1.47367e-03  4.10344e-01    2803.72     1      -14.658    5607.44\n",
      "      284  Completed  2261    3.38914e-06  4.25016e-04  5.31042e-01   11160.41     1       23.966    22320.83\n",
      "      285  Completed   170    9.33084e-06  6.26418e-03  2.86034e-01     3475.4     1      -40.752    6950.80\n",
      "      286    Stopped  2141    5.48057e-07  2.18969e-03  5.96937e-01     3044.7     1       30.368    6089.41\n",
      "      287  Completed   426    1.28101e-07  5.43423e-04  8.41353e-01    3216.66     1      -23.699    6433.33\n",
      "      288  Completed  1579    3.46279e-06  8.27633e-04  1.63786e-01   11881.88     1       44.055    23763.76\n",
      "      289  Completed  1043    1.11491e-07  3.87831e-04  8.32292e-01   10720.53     1       -9.239    21441.07\n",
      "      290    Stopped  1055    1.02418e-07  7.48403e-04  7.15343e-01    2941.95     1       34.997    5883.90\n",
      "      291  Completed  1946    8.49053e-06  6.87401e-03  1.26367e-01    4907.57     1       10.784    9815.14\n",
      "      292    Stopped  1978    9.83847e-06  7.52515e-03  5.04872e-01    6985.51     1        7.904    13971.03\n",
      "0 trials running, 293 finished (293 until the end), 61207.29s wallclock-time\n",
      "\n",
      "mean_reward: best 7.686551213264465 for trial-id 30\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "# Start hyperparameter tuning\n",
    "tuner.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get results\n",
    "tuner_path = tuner.tuner_path\n",
    "tuning_experiment = load_experiment(tuner_path)\n",
    "best_run = tuning_experiment.best_config()\n",
    "tuning_experiment.results.to_csv(\"artifacts/tune_logs/tuning_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tuning jobs: 293\n",
      "Best mean reward: 70.810\n",
      "Best tuning configuration:\n",
      " - config_learning_rate: 5.6672e-05\n",
      " - config_tau: 7.2524e-06\n",
      " - config_gamma: 9.7588e-01\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of tuning jobs:\", len(tuning_experiment.results))\n",
    "print(\"Best mean reward:\", round(best_run[\"mean_reward\"], 4))\n",
    "print(\"Best tuning configuration:\")\n",
    "for i in [\"config_learning_rate\", \"config_tau\", \"config_gamma\"]:\n",
    "    print(f\" - {i}: {best_run[i]:.4e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The approximation optimal hyperparameters fund in the tuning section above will now be used for training a final TD3 model. The training itself is very similar to the behavior of `train_hpo_model()`. However, instead of the reporter callback we now employ a `StopTrainingOnRewardThreshold` callback which determines an early stopping criterion in terms of a reward threshold. This callback is embedded inside the `EvalCallback` which is used to evaluate the model and create best model checkpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.noise import NormalActionNoise\n",
    "from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnRewardThreshold\n",
    "from stable_baselines3 import TD3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we create the environment for training which is wrapped `Monitor`. This serves the purpose of tracking the reward of environment interactions and logging it into `artifacts/train_logs/monitor.csv`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the vectorized environment\n",
    "env_id = \"CarRacing-v2\"\n",
    "log_dir = \"./artifacts/train_logs/\"\n",
    "env = gym.make(env_id, domain_randomize=False, render_mode=\"rgb_array\") \n",
    "env = Monitor(env, log_dir, allow_early_resets=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we create a new model instance with the optimal hyperparameters found in the tuning section above. The code for training (also the tuning worker above) is based on this [TD3 Pendulum-v1](https://stable-baselines3.readthedocs.io/en/master/modules/td3.html) example from the stable-baselines3 website. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n"
     ]
    }
   ],
   "source": [
    "# Initialize optimal hyperparameters\n",
    "n_actions = env.action_space.shape[-1]\n",
    "action_noise = NormalActionNoise(mean=np.zeros(n_actions), sigma=0.1 * np.ones(n_actions))  \n",
    "learning_rate = best_run[\"config_learning_rate\"]\n",
    "tau = best_run[\"config_tau\"]\n",
    "gamma = best_run[\"config_gamma\"]\n",
    "# Create the TD3 Agent\n",
    "model = TD3(\n",
    "    \"CnnPolicy\", \n",
    "    env,  \n",
    "    action_noise=action_noise,\n",
    "    learning_rate=learning_rate,\n",
    "    tau=tau,\n",
    "    gamma=gamma,\n",
    "    batch_size=32,\n",
    "    verbose=1,\n",
    "    device=\"cuda\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use DummyVecEnv to create a vectorized environment\n",
    "eval_env = Monitor(gym.make('CarRacing-v2'), log_dir)\n",
    "# Define callback for early stopping and include it in the eval callback\n",
    "callback_on_best = StopTrainingOnRewardThreshold(reward_threshold=500, verbose=1)\n",
    "eval_callback = EvalCallback(\n",
    "    eval_env, \n",
    "    callback_on_new_best=callback_on_best, \n",
    "    verbose=1, \n",
    "    best_model_save_path=\"./artifacts/model\",\n",
    "    log_path=\"./artifacts/logs/\", \n",
    "    eval_freq=1000\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of time_steps used for training is mostly based on resources limits. Configurations we found in other training examples of TD3 on the CarRacing-v2 environment typically used values two or three orders of magnitude larger which is outside the scope of this university project. \n",
    "- [Solving CarRacing with DDPG](https://github.com/lzhan144/Solving-CarRacing-with-DDPG/blob/master/car_racing.py)\n",
    "- [Torch CarRacing TD3](https://blog.csdn.net/Scc_hy/article/details/135179576)\n",
    "- [Control CartRacing-v2 environment using DQN from scratch](https://hiddenbeginner.github.io/study-notes/contents/tutorials/2023-04-20_CartRacing-v2_DQN.html)\n",
    "\n",
    "However, we found that the agent still converges with 1M time steps (1000 episodes) and our current hyperparameter configuration within around 10 hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adavidho/miniconda3/envs/rl/lib/python3.10/site-packages/stable_baselines3/common/callbacks.py:414: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.vec_transpose.VecTransposeImage object at 0x7effd97c34c0> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x7f00103b3b80>\n",
      "  warnings.warn(\"Training and eval env are not of the same type\" f\"{self.training_env} != {self.eval_env}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Eval num_timesteps=1000, episode_reward=--99.76 +/- 1.9368005538097308 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -99.76  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 1000     | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.5585   | \n",
      " |    critic_loss     | 1.6396   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 899      | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=2000, episode_reward=--99.07 +/- 1.9123654148820783 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -99.07  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 2000     | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.8429   | \n",
      " |    critic_loss     | 1.4607   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 1899     | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=3000, episode_reward=--98.48 +/- 1.591814676689446 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -98.48  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 3000     | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.6411   | \n",
      " |    critic_loss     | 1.6603   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 2899     | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=4000, episode_reward=--97.87 +/- 1.496370397995862 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -97.87  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 4000     | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.2218   | \n",
      " |    critic_loss     | 1.5555   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 3899     | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=5000, episode_reward=--97.26 +/- 1.0235264475766634 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -97.26  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 5000     | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.8858   | \n",
      " |    critic_loss     | 1.7214   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 4899     | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=6000, episode_reward=--96.61 +/- 1.2151798241627687 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -96.61  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 6000     | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.0910   | \n",
      " |    critic_loss     | 1.5227   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 5899     | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=7000, episode_reward=--96.4 +/- 1.2240342658068752 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   -96.4  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 7000     | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.7697   | \n",
      " |    critic_loss     | 1.4424   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 6899     | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=8000, episode_reward=--96.34 +/- 1.6184046360562592 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -96.34  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 8000     | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 7.7365   | \n",
      " |    critic_loss     | 1.9341   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 7899     | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=9000, episode_reward=--95.37 +/- 1.3059940958722098 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -95.37  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 9000     | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 7.7034   | \n",
      " |    critic_loss     | 1.9258   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 8899     | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=10000, episode_reward=--95.1 +/- 1.493506836452508 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   -95.1  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 10000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.8759   | \n",
      " |    critic_loss     | 1.4690   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 9899     | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=11000, episode_reward=--94.92 +/- 1.0421225604758593 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -94.92  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 11000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.5187   | \n",
      " |    critic_loss     | 1.6297   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 10899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=12000, episode_reward=--93.56 +/- 1.422161768977881 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -93.56  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 12000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.3426   | \n",
      " |    critic_loss     | 1.3356   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 11899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=13000, episode_reward=--93.94 +/- 1.7741001475900067 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -93.94  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 13000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.7749   | \n",
      " |    critic_loss     | 1.4437   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 12899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=14000, episode_reward=--92.59 +/- 1.8018875614445558 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -92.59  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 14000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.4010   | \n",
      " |    critic_loss     | 1.6002   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 13899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=15000, episode_reward=--92.66 +/- 1.62159476715939 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -92.66  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 15000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.5098   | \n",
      " |    critic_loss     | 1.3774   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 14899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=16000, episode_reward=--91.66 +/- 1.8904644690489456 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -91.66  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 16000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.1111   | \n",
      " |    critic_loss     | 1.0278   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 15899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=17000, episode_reward=--91.28 +/- 1.7258881823472993 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -91.28  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 17000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.8115   | \n",
      " |    critic_loss     | 1.7029   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 16899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=18000, episode_reward=--91.42 +/- 1.2779837962088112 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -91.42  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 18000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.5156   | \n",
      " |    critic_loss     | 1.1289   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 17899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=19000, episode_reward=--90.22 +/- 1.620348349646148 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -90.22  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 19000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.2566   | \n",
      " |    critic_loss     | 1.3141   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 18899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=20000, episode_reward=--89.75 +/- 1.5790504592295491 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -89.75  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 20000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.8171   | \n",
      " |    critic_loss     | 1.7043   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 19899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=21000, episode_reward=--89.56 +/- 1.4523047922413856 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -89.56  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 21000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 7.8783   | \n",
      " |    critic_loss     | 1.9696   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 20899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=22000, episode_reward=--88.71 +/- 1.8165512059891487 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -88.71  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 22000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.0049   | \n",
      " |    critic_loss     | 1.2512   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 21899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=23000, episode_reward=--88.79 +/- 1.2788238500474234 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -88.79  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 23000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 7.8167   | \n",
      " |    critic_loss     | 1.9542   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 22899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=24000, episode_reward=--87.7 +/- 1.456459471083075 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   -87.7  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 24000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.0816   | \n",
      " |    critic_loss     | 1.2704   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 23899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=25000, episode_reward=--87.41 +/- 1.2059946994902921 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -87.41  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 25000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.1099   | \n",
      " |    critic_loss     | 1.0275   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 24899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=26000, episode_reward=--87.47 +/- 1.2697775523900356 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -87.47  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 26000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.8431   | \n",
      " |    critic_loss     | 1.7108   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 25899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=27000, episode_reward=--86.97 +/- 1.786056107584788 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -86.97  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 27000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.9735   | \n",
      " |    critic_loss     | 1.4934   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 26899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=28000, episode_reward=--86.38 +/- 1.4714709185907973 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -86.38  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 28000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.3037   | \n",
      " |    critic_loss     | 1.3259   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 27899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=29000, episode_reward=--85.95 +/- 1.7061520075340595 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -85.95  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 29000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.0494   | \n",
      " |    critic_loss     | 1.2623   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 28899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=30000, episode_reward=--85.28 +/- 1.2248744685819029 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -85.28  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 30000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.1466   | \n",
      " |    critic_loss     | 1.2867   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 29899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=31000, episode_reward=--84.67 +/- 1.6581973934406204 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -84.67  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 31000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 7.7263   | \n",
      " |    critic_loss     | 1.9316   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 30899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=32000, episode_reward=--83.83 +/- 1.045767416527303 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -83.83  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 32000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.6616   | \n",
      " |    critic_loss     | 1.4154   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 31899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=33000, episode_reward=--83.32 +/- 1.9322389621020615 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -83.32  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 33000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 7.6898   | \n",
      " |    critic_loss     | 1.9225   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 32899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=34000, episode_reward=--83.43 +/- 1.7986418274865486 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -83.43  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 34000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 7.8087   | \n",
      " |    critic_loss     | 1.9522   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 33899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=35000, episode_reward=--82.09 +/- 1.3054745906928067 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -82.09  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 35000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 7.0104   | \n",
      " |    critic_loss     | 1.7526   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 34899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=36000, episode_reward=--81.91 +/- 1.9194715262912199 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -81.91  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 36000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.2459   | \n",
      " |    critic_loss     | 1.5615   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 35899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=37000, episode_reward=--81.74 +/- 1.1370893386458145 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -81.74  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 37000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.5659   | \n",
      " |    critic_loss     | 1.3915   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 36899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=38000, episode_reward=--81.16 +/- 1.6540975563072047 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -81.16  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 38000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.9152   | \n",
      " |    critic_loss     | 0.9788   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 37899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=39000, episode_reward=--80.11 +/- 1.4353412845271332 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -80.11  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 39000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.5918   | \n",
      " |    critic_loss     | 1.1480   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 38899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=40000, episode_reward=--80.34 +/- 1.2776412100105472 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -80.34  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 40000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.0961   | \n",
      " |    critic_loss     | 1.2740   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 39899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=41000, episode_reward=--79.98 +/- 1.3240688113145918 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -79.98  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 41000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.9913   | \n",
      " |    critic_loss     | 1.7478   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 40899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=42000, episode_reward=--78.72 +/- 1.7005686543721907 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -78.72  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 42000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.7515   | \n",
      " |    critic_loss     | 1.4379   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 41899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=43000, episode_reward=--78.38 +/- 1.5011720524438308 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -78.38  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 43000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.1642   | \n",
      " |    critic_loss     | 1.0411   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 42899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=44000, episode_reward=--77.8 +/- 1.0695898663562535 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   -77.8  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 44000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.7428   | \n",
      " |    critic_loss     | 1.1857   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 43899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=45000, episode_reward=--77.83 +/- 1.851512122983121 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -77.83  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 45000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.4205   | \n",
      " |    critic_loss     | 1.1051   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 44899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=46000, episode_reward=--76.63 +/- 1.250458813793636 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -76.63  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 46000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.4551   | \n",
      " |    critic_loss     | 1.3638   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 45899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=47000, episode_reward=--76.6 +/- 1.253385205252362 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   -76.6  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 47000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.9135   | \n",
      " |    critic_loss     | 1.7284   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 46899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=48000, episode_reward=--75.64 +/- 1.6972709248494589 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -75.64  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 48000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 7.3426   | \n",
      " |    critic_loss     | 1.8356   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 47899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=49000, episode_reward=--75.41 +/- 1.6675511555051865 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -75.41  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 49000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.1357   | \n",
      " |    critic_loss     | 1.2839   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 48899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=50000, episode_reward=--75.39 +/- 1.9936195736454136 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -75.39  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 50000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.8805   | \n",
      " |    critic_loss     | 1.7201   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 49899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=51000, episode_reward=--74.64 +/- 1.972667918097981 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -74.64  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 51000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.3153   | \n",
      " |    critic_loss     | 1.3288   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 50899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=52000, episode_reward=--74.03 +/- 1.5745349757149527 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -74.03  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 52000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.5908   | \n",
      " |    critic_loss     | 1.3977   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 51899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=53000, episode_reward=--73.01 +/- 1.14721096352633 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -73.01  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 53000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 7.7134   | \n",
      " |    critic_loss     | 1.9284   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 52899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=54000, episode_reward=--72.95 +/- 1.9135759563995545 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -72.95  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 54000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.3450   | \n",
      " |    critic_loss     | 1.0862   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 53899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=55000, episode_reward=--72.82 +/- 1.8607355443503897 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -72.82  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 55000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.9850   | \n",
      " |    critic_loss     | 1.4963   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 54899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=56000, episode_reward=--72.44 +/- 1.3606752328798475 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -72.44  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 56000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.1655   | \n",
      " |    critic_loss     | 1.0414   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 55899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=57000, episode_reward=--71.66 +/- 1.556633976896565 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -71.66  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 57000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.8484   | \n",
      " |    critic_loss     | 1.7121   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 56899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=58000, episode_reward=--70.98 +/- 1.6937959960859028 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -70.98  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 58000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.5084   | \n",
      " |    critic_loss     | 1.1271   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 57899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=59000, episode_reward=--70.09 +/- 1.9637429618038853 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -70.09  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 59000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.3381   | \n",
      " |    critic_loss     | 1.5845   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 58899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=60000, episode_reward=--70.02 +/- 1.3726226532627939 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -70.02  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 60000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 7.0628   | \n",
      " |    critic_loss     | 1.7657   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 59899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=61000, episode_reward=--69.01 +/- 1.3647193356853073 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -69.01  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 61000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 7.7348   | \n",
      " |    critic_loss     | 1.9337   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 60899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=62000, episode_reward=--69.25 +/- 1.9399578434248268 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -69.25  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 62000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 7.2138   | \n",
      " |    critic_loss     | 1.8035   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 61899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=63000, episode_reward=--68.7 +/- 1.285584256856477 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   -68.7  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 63000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.5184   | \n",
      " |    critic_loss     | 1.6296   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 62899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=64000, episode_reward=--68.23 +/- 1.0199502953795725 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -68.23  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 64000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.9594   | \n",
      " |    critic_loss     | 1.2398   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 63899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=65000, episode_reward=--67.66 +/- 1.161844457374401 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -67.66  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 65000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.9798   | \n",
      " |    critic_loss     | 0.9949   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 64899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=66000, episode_reward=--67.5 +/- 1.417028324675372 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   -67.5  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 66000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.3780   | \n",
      " |    critic_loss     | 1.5945   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 65899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=67000, episode_reward=--66.91 +/- 1.2819067565821993 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -66.91  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 67000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.8010   | \n",
      " |    critic_loss     | 1.7003   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 66899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=68000, episode_reward=--65.64 +/- 1.0456195367616647 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -65.64  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 68000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.6684   | \n",
      " |    critic_loss     | 1.4171   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 67899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=69000, episode_reward=--65.8 +/- 1.2473328211568409 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   -65.8  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 69000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.8642   | \n",
      " |    critic_loss     | 0.9661   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 68899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=70000, episode_reward=--65.21 +/- 1.733254946611332 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -65.21  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 70000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.0351   | \n",
      " |    critic_loss     | 1.5088   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 69899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=71000, episode_reward=--64.1 +/- 1.2362596106975756 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   -64.1  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 71000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.0984   | \n",
      " |    critic_loss     | 1.2746   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 70899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=72000, episode_reward=--64.05 +/- 1.273722440380555 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -64.05  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 72000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.0625   | \n",
      " |    critic_loss     | 1.5156   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 71899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=73000, episode_reward=--63.2 +/- 1.0803729232429213 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   -63.2  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 73000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 7.1339   | \n",
      " |    critic_loss     | 1.7835   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 72899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=74000, episode_reward=--63.42 +/- 1.3747154600047888 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -63.42  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 74000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.4974   | \n",
      " |    critic_loss     | 1.1243   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 73899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=75000, episode_reward=--62.85 +/- 1.2870129033438515 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -62.85  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 75000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.8065   | \n",
      " |    critic_loss     | 0.9516   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 74899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=76000, episode_reward=--61.76 +/- 1.4549122501503777 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -61.76  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 76000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 7.4301   | \n",
      " |    critic_loss     | 1.8575   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 75899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=77000, episode_reward=--61.14 +/- 1.9553685835094803 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -61.14  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 77000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.1575   | \n",
      " |    critic_loss     | 1.0394   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 76899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=78000, episode_reward=--61.17 +/- 1.7147401564582885 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -61.17  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 78000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.7945   | \n",
      " |    critic_loss     | 1.6986   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 77899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=79000, episode_reward=--60.84 +/- 1.0321815413603384 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -60.84  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 79000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.7568   | \n",
      " |    critic_loss     | 1.4392   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 78899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=80000, episode_reward=--60.07 +/- 1.6109612207466144 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -60.07  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 80000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 7.3568   | \n",
      " |    critic_loss     | 1.8392   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 79899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=81000, episode_reward=--59.16 +/- 1.8067321078948149 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -59.16  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 81000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.6522   | \n",
      " |    critic_loss     | 1.1631   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 80899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=82000, episode_reward=--58.69 +/- 1.9742777643013967 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -58.69  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 82000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.5597   | \n",
      " |    critic_loss     | 1.6399   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 81899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=83000, episode_reward=--58.32 +/- 1.1335024969321947 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -58.32  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 83000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.9502   | \n",
      " |    critic_loss     | 1.2376   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 82899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=84000, episode_reward=--57.79 +/- 1.6223674091133604 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -57.79  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 84000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.8639   | \n",
      " |    critic_loss     | 1.7160   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 83899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=85000, episode_reward=--57.77 +/- 1.461703046162973 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -57.77  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 85000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.7471   | \n",
      " |    critic_loss     | 1.4368   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 84899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=86000, episode_reward=--56.54 +/- 1.848479014112569 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -56.54  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 86000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.7323   | \n",
      " |    critic_loss     | 1.6831   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 85899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=87000, episode_reward=--56.97 +/- 1.8720033818090396 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -56.97  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 87000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.4447   | \n",
      " |    critic_loss     | 1.6112   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 86899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=88000, episode_reward=--55.91 +/- 1.0347739348736602 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -55.91  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 88000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 7.4676   | \n",
      " |    critic_loss     | 1.8669   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 87899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=89000, episode_reward=--55.12 +/- 1.779709579572783 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -55.12  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 89000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.3326   | \n",
      " |    critic_loss     | 1.0831   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 88899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=90000, episode_reward=--54.55 +/- 1.0528488555269462 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -54.55  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 90000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.7451   | \n",
      " |    critic_loss     | 1.4363   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 89899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=91000, episode_reward=--54.36 +/- 1.0366184136099639 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -54.36  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 91000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.6421   | \n",
      " |    critic_loss     | 1.1605   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 90899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=92000, episode_reward=--53.56 +/- 1.4661779419933696 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -53.56  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 92000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.1020   | \n",
      " |    critic_loss     | 1.2755   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 91899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=93000, episode_reward=--53.64 +/- 1.3783136878525934 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -53.64  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 93000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.2058   | \n",
      " |    critic_loss     | 1.3015   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 92899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=94000, episode_reward=--52.56 +/- 1.13302238990113 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -52.56  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 94000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 7.1425   | \n",
      " |    critic_loss     | 1.7856   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 93899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=95000, episode_reward=--52.15 +/- 1.230529191208602 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -52.15  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 95000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.6812   | \n",
      " |    critic_loss     | 0.9203   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 94899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=96000, episode_reward=--51.68 +/- 1.913131238203063 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -51.68  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 96000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.9233   | \n",
      " |    critic_loss     | 0.9808   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 95899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=97000, episode_reward=--51.62 +/- 1.6456964232425917 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -51.62  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 97000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.1508   | \n",
      " |    critic_loss     | 1.2877   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 96899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=98000, episode_reward=--51.09 +/- 1.2182457494893835 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -51.09  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 98000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.6293   | \n",
      " |    critic_loss     | 1.6573   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 97899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=99000, episode_reward=--50.05 +/- 1.463427162136408 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -50.05  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 99000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.9401   | \n",
      " |    critic_loss     | 1.2350   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 98899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=100000, episode_reward=--49.97 +/- 1.4745245648443066 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -49.97  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 100000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.2521   | \n",
      " |    critic_loss     | 1.5630   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 99899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=101000, episode_reward=--49.4 +/- 1.199192945754593 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   -49.4  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 101000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 7.2211   | \n",
      " |    critic_loss     | 1.8053   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 100899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=102000, episode_reward=--48.83 +/- 1.441736146232214 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -48.83  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 102000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.0063   | \n",
      " |    critic_loss     | 1.0016   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 101899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=103000, episode_reward=--48.83 +/- 1.992259080712865 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -48.83  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 103000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.4125   | \n",
      " |    critic_loss     | 1.3531   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 102899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=104000, episode_reward=--47.56 +/- 1.9412246653111658 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -47.56  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 104000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.7887   | \n",
      " |    critic_loss     | 0.9472   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 103899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=105000, episode_reward=--47.3 +/- 1.161311443220189 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   -47.3  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 105000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.4588   | \n",
      " |    critic_loss     | 1.1147   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 104899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=106000, episode_reward=--47.32 +/- 1.5569313895188253 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -47.32  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 106000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.1089   | \n",
      " |    critic_loss     | 1.5272   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 105899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=107000, episode_reward=--46.15 +/- 1.6523004252239017 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -46.15  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 107000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.5126   | \n",
      " |    critic_loss     | 1.6282   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 106899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=108000, episode_reward=--45.97 +/- 1.533275327492581 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -45.97  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 108000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.2320   | \n",
      " |    critic_loss     | 1.3080   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 107899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=109000, episode_reward=--45.05 +/- 1.354874349680388 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -45.05  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 109000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.4964   | \n",
      " |    critic_loss     | 1.1241   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 108899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=110000, episode_reward=--45.13 +/- 1.833362344643577 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -45.13  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 110000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 7.0813   | \n",
      " |    critic_loss     | 1.7703   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 109899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=111000, episode_reward=--44.94 +/- 1.7168736644526084 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -44.94  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 111000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 7.3872   | \n",
      " |    critic_loss     | 1.8468   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 110899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=112000, episode_reward=--44.17 +/- 1.877204345290199 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -44.17  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 112000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.8340   | \n",
      " |    critic_loss     | 1.7085   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 111899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=113000, episode_reward=--43.46 +/- 1.7646908926387557 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -43.46  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 113000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.7821   | \n",
      " |    critic_loss     | 1.4455   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 112899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=114000, episode_reward=--42.63 +/- 1.731455158166857 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -42.63  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 114000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.6423   | \n",
      " |    critic_loss     | 1.1606   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 113899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=115000, episode_reward=--42.32 +/- 1.7516007985020279 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -42.32  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 115000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.8522   | \n",
      " |    critic_loss     | 1.4631   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 114899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=116000, episode_reward=--41.6 +/- 1.473660298139011 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   -41.6  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 116000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.2914   | \n",
      " |    critic_loss     | 1.5729   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 115899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=117000, episode_reward=--41.95 +/- 1.383646160363074 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -41.95  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 117000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.8997   | \n",
      " |    critic_loss     | 0.9749   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 116899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=118000, episode_reward=--40.77 +/- 1.267605217026901 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -40.77  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 118000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.0773   | \n",
      " |    critic_loss     | 1.0193   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 117899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=119000, episode_reward=--40.38 +/- 1.0202305979751172 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -40.38  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 119000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 7.0480   | \n",
      " |    critic_loss     | 1.7620   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 118899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=120000, episode_reward=--39.75 +/- 1.135571939154922 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -39.75  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 120000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 7.0066   | \n",
      " |    critic_loss     | 1.7516   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 119899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=121000, episode_reward=--39.06 +/- 1.2212816037705185 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -39.06  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 121000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.7988   | \n",
      " |    critic_loss     | 0.9497   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 120899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=122000, episode_reward=--39.19 +/- 1.2991313414485717 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -39.19  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 122000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.9314   | \n",
      " |    critic_loss     | 1.2329   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 121899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=123000, episode_reward=--38.92 +/- 1.9251601103658302 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -38.92  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 123000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.5708   | \n",
      " |    critic_loss     | 0.8927   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 122899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=124000, episode_reward=--38.27 +/- 1.0247014148043774 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -38.27  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 124000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.4570   | \n",
      " |    critic_loss     | 1.3642   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 123899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=125000, episode_reward=--37.58 +/- 1.2553447717867625 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -37.58  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 125000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.9124   | \n",
      " |    critic_loss     | 0.9781   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 124899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=126000, episode_reward=--36.81 +/- 1.0556035346955905 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -36.81  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 126000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.5034   | \n",
      " |    critic_loss     | 1.3759   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 125899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=127000, episode_reward=--36.1 +/- 1.7415322996225773 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   -36.1  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 127000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.5087   | \n",
      " |    critic_loss     | 1.3772   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 126899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=128000, episode_reward=--36.49 +/- 1.2231807769216902 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -36.49  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 128000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.4525   | \n",
      " |    critic_loss     | 1.1131   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 127899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=129000, episode_reward=--35.5 +/- 1.6029339801964777 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   -35.5  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 129000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.2005   | \n",
      " |    critic_loss     | 1.3001   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 128899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=130000, episode_reward=--35.1 +/- 1.2878608902526032 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   -35.1  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 130000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.0518   | \n",
      " |    critic_loss     | 1.0130   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 129899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=131000, episode_reward=--34.91 +/- 1.3161120325917937 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -34.91  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 131000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.8781   | \n",
      " |    critic_loss     | 1.4695   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 130899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=132000, episode_reward=--34.04 +/- 1.2380450370682288 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -34.04  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 132000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.3917   | \n",
      " |    critic_loss     | 1.0979   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 131899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=133000, episode_reward=--33.22 +/- 1.8583797370959503 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -33.22  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 133000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.6056   | \n",
      " |    critic_loss     | 1.4014   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 132899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=134000, episode_reward=--32.55 +/- 1.3385791344228068 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -32.55  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 134000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.9097   | \n",
      " |    critic_loss     | 1.2274   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 133899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=135000, episode_reward=--32.19 +/- 1.597408447362785 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -32.19  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 135000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.1891   | \n",
      " |    critic_loss     | 1.0473   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 134899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=136000, episode_reward=--32.19 +/- 1.671618369267407 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -32.19  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 136000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.0327   | \n",
      " |    critic_loss     | 1.2582   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 135899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=137000, episode_reward=--31.6 +/- 1.5089455743559874 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   -31.6  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 137000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.1117   | \n",
      " |    critic_loss     | 1.2779   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 136899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=138000, episode_reward=--31.42 +/- 1.0222243847114503 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -31.42  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 138000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.5485   | \n",
      " |    critic_loss     | 1.6371   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 137899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=139000, episode_reward=--30.98 +/- 1.1831748474524244 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -30.98  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 139000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.5576   | \n",
      " |    critic_loss     | 1.3894   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 138899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=140000, episode_reward=--30.28 +/- 1.96806637338701 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -30.28  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 140000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 7.0783   | \n",
      " |    critic_loss     | 1.7696   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 139899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=141000, episode_reward=--29.36 +/- 1.1729203686104528 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -29.36  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 141000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.5123   | \n",
      " |    critic_loss     | 1.6281   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 140899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=142000, episode_reward=--28.84 +/- 1.9073184929018552 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -28.84  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 142000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.6028   | \n",
      " |    critic_loss     | 1.6507   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 141899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=143000, episode_reward=--28.74 +/- 1.126088501875956 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -28.74  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 143000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.3923   | \n",
      " |    critic_loss     | 1.5981   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 142899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=144000, episode_reward=--27.74 +/- 1.897903498059333 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -27.74  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 144000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.5732   | \n",
      " |    critic_loss     | 1.1433   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 143899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=145000, episode_reward=--27.22 +/- 1.6266599722935504 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -27.22  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 145000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.1549   | \n",
      " |    critic_loss     | 1.2887   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 144899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=146000, episode_reward=--26.9 +/- 1.0398472675796717 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   -26.9  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 146000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.8290   | \n",
      " |    critic_loss     | 0.9572   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 145899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=147000, episode_reward=--26.29 +/- 1.1518258984644527 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -26.29  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 147000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.0080   | \n",
      " |    critic_loss     | 1.2520   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 146899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=148000, episode_reward=--25.69 +/- 1.7990955511645008 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -25.69  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 148000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.6409   | \n",
      " |    critic_loss     | 1.4102   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 147899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=149000, episode_reward=--25.6 +/- 1.5723858373863848 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   -25.6  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 149000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.1995   | \n",
      " |    critic_loss     | 1.2999   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 148899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=150000, episode_reward=--24.9 +/- 1.399048691001363 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   -24.9  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 150000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 7.3917   | \n",
      " |    critic_loss     | 1.8479   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 149899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=151000, episode_reward=--24.83 +/- 1.7665024486452203 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -24.83  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 151000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.9408   | \n",
      " |    critic_loss     | 0.9852   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 150899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=152000, episode_reward=--24.38 +/- 1.2880898120501718 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -24.38  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 152000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.9608   | \n",
      " |    critic_loss     | 1.4902   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 151899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=153000, episode_reward=--23.53 +/- 1.0790552996215168 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -23.53  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 153000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.5250   | \n",
      " |    critic_loss     | 0.8812   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 152899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=154000, episode_reward=--22.96 +/- 1.1747991913844427 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -22.96  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 154000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.0744   | \n",
      " |    critic_loss     | 1.0186   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 153899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=155000, episode_reward=--23.0 +/- 1.3904802689397284 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   -23.0  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 155000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.5522   | \n",
      " |    critic_loss     | 1.3880   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 154899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=156000, episode_reward=--21.64 +/- 1.938401833100194 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -21.64  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 156000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.8485   | \n",
      " |    critic_loss     | 0.9621   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 155899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=157000, episode_reward=--21.3 +/- 1.9578273991404935 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   -21.3  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 157000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.6302   | \n",
      " |    critic_loss     | 1.4075   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 156899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=158000, episode_reward=--21.03 +/- 1.3905463745165538 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -21.03  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 158000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.3629   | \n",
      " |    critic_loss     | 1.5907   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 157899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=159000, episode_reward=--20.18 +/- 1.5859624861009414 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -20.18  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 159000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.9960   | \n",
      " |    critic_loss     | 1.2490   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 158899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=160000, episode_reward=--20.4 +/- 1.7638287017856764 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   -20.4  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 160000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.8112   | \n",
      " |    critic_loss     | 1.4528   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 159899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=161000, episode_reward=--19.71 +/- 1.7022117428787666 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -19.71  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 161000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.3819   | \n",
      " |    critic_loss     | 1.5955   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 160899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=162000, episode_reward=--18.55 +/- 1.2570834361154417 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -18.55  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 162000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.2916   | \n",
      " |    critic_loss     | 1.0729   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 161899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=163000, episode_reward=--18.21 +/- 1.3476113572265307 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -18.21  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 163000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.9181   | \n",
      " |    critic_loss     | 1.7295   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 162899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=164000, episode_reward=--17.86 +/- 1.0639649479073228 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -17.86  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 164000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.9110   | \n",
      " |    critic_loss     | 1.2278   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 163899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=165000, episode_reward=--17.2 +/- 1.9119635559672985 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   -17.2  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 165000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.9032   | \n",
      " |    critic_loss     | 1.7258   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 164899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=166000, episode_reward=--16.73 +/- 1.9300280260990759 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -16.73  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 166000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.5257   | \n",
      " |    critic_loss     | 1.6314   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 165899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=167000, episode_reward=--16.82 +/- 1.049727352307338 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -16.82  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 167000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.8010   | \n",
      " |    critic_loss     | 1.7003   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 166899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=168000, episode_reward=--16.25 +/- 1.1579337183429388 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -16.25  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 168000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.9022   | \n",
      " |    critic_loss     | 1.7256   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 167899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=169000, episode_reward=--15.19 +/- 1.7593120008322045 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -15.19  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 169000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.0201   | \n",
      " |    critic_loss     | 1.0050   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 168899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=170000, episode_reward=--14.74 +/- 1.2446785489572827 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -14.74  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 170000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.8463   | \n",
      " |    critic_loss     | 1.2116   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 169899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=171000, episode_reward=--14.61 +/- 1.31222957756456 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -14.61  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 171000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.1653   | \n",
      " |    critic_loss     | 1.2913   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 170899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=172000, episode_reward=--13.76 +/- 1.7414394623913245 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -13.76  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 172000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.3182   | \n",
      " |    critic_loss     | 1.0795   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 171899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=173000, episode_reward=--13.99 +/- 1.141202352258062 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -13.99  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 173000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.1214   | \n",
      " |    critic_loss     | 1.0303   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 172899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=174000, episode_reward=--12.64 +/- 1.1066684441490215 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -12.64  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 174000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.2061   | \n",
      " |    critic_loss     | 1.5515   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 173899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=175000, episode_reward=--12.84 +/- 1.7748870797626917 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -12.84  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 175000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.2409   | \n",
      " |    critic_loss     | 1.5602   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 174899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=176000, episode_reward=--12.44 +/- 1.9626742024203394 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -12.44  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 176000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 7.0868   | \n",
      " |    critic_loss     | 1.7717   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 175899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=177000, episode_reward=--11.51 +/- 1.8043901474921178 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -11.51  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 177000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.3272   | \n",
      " |    critic_loss     | 1.0818   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 176899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=178000, episode_reward=--10.7 +/- 1.3131570360810612 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   -10.7  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 178000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.5413   | \n",
      " |    critic_loss     | 0.8853   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 177899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=179000, episode_reward=--10.83 +/- 1.3337226755124276 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -10.83  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 179000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.5618   | \n",
      " |    critic_loss     | 1.1404   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 178899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=180000, episode_reward=--9.69 +/- 1.9225163547232054 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   -9.69  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 180000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.3771   | \n",
      " |    critic_loss     | 1.0943   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 179899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=181000, episode_reward=--9.74 +/- 1.8104719250068189 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   -9.74  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 181000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.4459   | \n",
      " |    critic_loss     | 1.3615   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 180899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=182000, episode_reward=--8.69 +/- 1.4288550754522555 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   -8.69  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 182000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.1572   | \n",
      " |    critic_loss     | 1.0393   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 181899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=183000, episode_reward=--8.44 +/- 1.9130536515699386 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   -8.44  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 183000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.9065   | \n",
      " |    critic_loss     | 1.2266   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 182899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=184000, episode_reward=--7.87 +/- 1.7034701500006473 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   -7.87  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 184000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.5803   | \n",
      " |    critic_loss     | 1.1451   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 183899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=185000, episode_reward=--7.37 +/- 1.1679913858964261 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   -7.37  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 185000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.1714   | \n",
      " |    critic_loss     | 1.0429   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 184899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=186000, episode_reward=--7.12 +/- 1.3828804643254335 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   -7.12  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 186000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.7149   | \n",
      " |    critic_loss     | 1.1787   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 185899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=187000, episode_reward=--6.01 +/- 1.1380465746966857 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   -6.01  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 187000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.6888   | \n",
      " |    critic_loss     | 1.1722   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 186899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=188000, episode_reward=--6.0 +/- 1.6693150969718054 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |    -6.0  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 188000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.6640   | \n",
      " |    critic_loss     | 1.4160   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 187899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=189000, episode_reward=--5.53 +/- 1.1868580017644632 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   -5.53  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 189000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.7203   | \n",
      " |    critic_loss     | 1.4301   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 188899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=190000, episode_reward=--5.18 +/- 1.635145807130125 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   -5.18  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 190000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.4427   | \n",
      " |    critic_loss     | 1.1107   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 189899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=191000, episode_reward=--4.61 +/- 1.8650621137010446 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   -4.61  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 191000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.8717   | \n",
      " |    critic_loss     | 1.7179   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 190899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=192000, episode_reward=--4.16 +/- 1.9653005146440465 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   -4.16  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 192000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.4335   | \n",
      " |    critic_loss     | 1.3584   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 191899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=193000, episode_reward=--3.38 +/- 1.6647467677628693 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   -3.38  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 193000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.7099   | \n",
      " |    critic_loss     | 1.1775   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 192899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=194000, episode_reward=--2.62 +/- 1.9098850088836599 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   -2.62  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 194000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.4364   | \n",
      " |    critic_loss     | 1.6091   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 193899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=195000, episode_reward=--2.87 +/- 1.6836278679466792 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   -2.87  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 195000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.5211   | \n",
      " |    critic_loss     | 0.8803   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 194899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=196000, episode_reward=--1.51 +/- 1.8902053452422718 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   -1.51  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 196000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.4551   | \n",
      " |    critic_loss     | 1.6138   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 195899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=197000, episode_reward=--1.7 +/- 1.310673186620015 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |    -1.7  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 197000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 7.1644   | \n",
      " |    critic_loss     | 1.7911   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 196899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=198000, episode_reward=--0.73 +/- 1.2361551147278385 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   -0.73  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 198000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.7038   | \n",
      " |    critic_loss     | 1.1760   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 197899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=199000, episode_reward=--0.97 +/- 1.0135645145642054 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   -0.97  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 199000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 7.0135   | \n",
      " |    critic_loss     | 1.7534   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 198899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=200000, episode_reward=-0.03 +/- 1.6274070876761217 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |    0.03  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 200000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.3203   | \n",
      " |    critic_loss     | 1.5801   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 199899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=201000, episode_reward=-0.34 +/- 1.767969925201736 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |    0.34  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 201000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.1401   | \n",
      " |    critic_loss     | 1.0350   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 200899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=202000, episode_reward=-0.99 +/- 1.7966723991552538 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |    0.99  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 202000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.4352   | \n",
      " |    critic_loss     | 1.6088   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 201899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=203000, episode_reward=-1.6 +/- 1.6816026208083739 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |     1.6  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 203000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.6474   | \n",
      " |    critic_loss     | 1.4119   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 202899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=204000, episode_reward=-2.17 +/- 1.8473343643617932 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |    2.17  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 204000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.5628   | \n",
      " |    critic_loss     | 0.8907   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 203899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=205000, episode_reward=-2.19 +/- 1.9402370747583921 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |    2.19  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 205000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.0655   | \n",
      " |    critic_loss     | 1.5164   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 204899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=206000, episode_reward=-2.74 +/- 1.4224031826550263 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |    2.74  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 206000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.8129   | \n",
      " |    critic_loss     | 1.7032   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 205899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=207000, episode_reward=-3.4 +/- 1.1838012775749354 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |     3.4  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 207000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.5702   | \n",
      " |    critic_loss     | 1.3926   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 206899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=208000, episode_reward=-4.33 +/- 1.8567235088753957 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |    4.33  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 208000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.2085   | \n",
      " |    critic_loss     | 1.0521   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 207899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=209000, episode_reward=-4.25 +/- 1.0584681898277817 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |    4.25  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 209000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.8815   | \n",
      " |    critic_loss     | 1.2204   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 208899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=210000, episode_reward=-4.73 +/- 1.9738139052436132 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |    4.73  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 210000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.0264   | \n",
      " |    critic_loss     | 1.2566   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 209899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=211000, episode_reward=-5.28 +/- 1.8138355655128164 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |    5.28  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 211000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.7608   | \n",
      " |    critic_loss     | 1.1902   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 210899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=212000, episode_reward=-5.71 +/- 1.8915717799675829 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |    5.71  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 212000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.2389   | \n",
      " |    critic_loss     | 0.8097   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 211899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=213000, episode_reward=-6.84 +/- 1.5421062038364892 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |    6.84  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 213000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.8393   | \n",
      " |    critic_loss     | 0.9598   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 212899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=214000, episode_reward=-6.84 +/- 1.4330329522408563 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |    6.84  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 214000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.5158   | \n",
      " |    critic_loss     | 0.8789   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 213899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=215000, episode_reward=-7.43 +/- 1.2699200984092642 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |    7.43  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 215000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.1605   | \n",
      " |    critic_loss     | 0.7901   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 214899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=216000, episode_reward=-8.21 +/- 1.9178725810253638 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |    8.21  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 216000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.8100   | \n",
      " |    critic_loss     | 0.9525   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 215899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=217000, episode_reward=-8.54 +/- 1.4514748994340614 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |    8.54  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 217000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.8459   | \n",
      " |    critic_loss     | 1.2115   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 216899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=218000, episode_reward=-9.4 +/- 1.5940624353414343 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |     9.4  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 218000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.4898   | \n",
      " |    critic_loss     | 1.3725   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 217899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=219000, episode_reward=-9.78 +/- 1.3580133310407074 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |    9.78  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 219000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.9763   | \n",
      " |    critic_loss     | 1.7441   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 218899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=220000, episode_reward=-9.76 +/- 1.8014575233311558 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |    9.76  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 220000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.7697   | \n",
      " |    critic_loss     | 1.4424   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 219899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=221000, episode_reward=-10.48 +/- 1.6229887673740597 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   10.48  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 221000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.6791   | \n",
      " |    critic_loss     | 1.4198   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 220899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=222000, episode_reward=-11.17 +/- 1.286146641405101 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   11.17  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 222000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.2262   | \n",
      " |    critic_loss     | 1.5565   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 221899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=223000, episode_reward=-11.07 +/- 1.8682877810834508 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   11.07  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 223000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.3334   | \n",
      " |    critic_loss     | 1.3333   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 222899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=224000, episode_reward=-11.88 +/- 1.031513880914033 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   11.88  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 224000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.2645   | \n",
      " |    critic_loss     | 1.0661   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 223899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=225000, episode_reward=-12.79 +/- 1.5540984457413638 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   12.79  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 225000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 7.0772   | \n",
      " |    critic_loss     | 1.7693   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 224899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=226000, episode_reward=-13.35 +/- 1.0123027227960137 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   13.35  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 226000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.1626   | \n",
      " |    critic_loss     | 1.0406   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 225899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=227000, episode_reward=-13.88 +/- 1.9266358766874114 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   13.88  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 227000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.5501   | \n",
      " |    critic_loss     | 0.8875   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 226899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=228000, episode_reward=-13.6 +/- 1.034049702194697 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |    13.6  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 228000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.5318   | \n",
      " |    critic_loss     | 1.3830   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 227899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=229000, episode_reward=-14.56 +/- 1.0582273400074005 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   14.56  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 229000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.8721   | \n",
      " |    critic_loss     | 0.9680   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 228899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=230000, episode_reward=-15.06 +/- 1.1597522910102942 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   15.06  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 230000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.6584   | \n",
      " |    critic_loss     | 1.1646   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 229899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=231000, episode_reward=-15.78 +/- 1.5695913004500477 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   15.78  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 231000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.3959   | \n",
      " |    critic_loss     | 1.0990   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 230899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=232000, episode_reward=-16.48 +/- 1.5819606750754822 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   16.48  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 232000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.4821   | \n",
      " |    critic_loss     | 1.6205   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 231899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=233000, episode_reward=-16.57 +/- 1.051798546587885 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   16.57  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 233000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.5366   | \n",
      " |    critic_loss     | 1.3841   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 232899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=234000, episode_reward=-16.64 +/- 1.9961963568036465 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   16.64  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 234000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.9984   | \n",
      " |    critic_loss     | 0.9996   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 233899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=235000, episode_reward=-17.02 +/- 1.5271899798841 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   17.02  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 235000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.9644   | \n",
      " |    critic_loss     | 1.4911   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 234899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=236000, episode_reward=-18.47 +/- 1.1225362031881831 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   18.47  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 236000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.5751   | \n",
      " |    critic_loss     | 1.6438   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 235899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=237000, episode_reward=-18.25 +/- 1.0176523341174462 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   18.25  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 237000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.4903   | \n",
      " |    critic_loss     | 1.1226   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 236899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=238000, episode_reward=-19.22 +/- 1.4378417639719268 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   19.22  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 238000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.4889   | \n",
      " |    critic_loss     | 1.1222   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 237899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=239000, episode_reward=-19.44 +/- 1.7882414836152587 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   19.44  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 239000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.8655   | \n",
      " |    critic_loss     | 1.2164   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 238899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=240000, episode_reward=-19.74 +/- 1.4379456936866828 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   19.74  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 240000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.2040   | \n",
      " |    critic_loss     | 1.5510   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 239899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=241000, episode_reward=-20.82 +/- 1.869360098936546 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   20.82  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 241000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 7.0189   | \n",
      " |    critic_loss     | 1.7547   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 240899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=242000, episode_reward=-20.96 +/- 1.6230898271709238 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   20.96  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 242000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.2429   | \n",
      " |    critic_loss     | 0.8107   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 241899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=243000, episode_reward=-21.07 +/- 1.695278671170036 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   21.07  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 243000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.1124   | \n",
      " |    critic_loss     | 0.7781   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 242899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=244000, episode_reward=-21.65 +/- 1.3255149416562777 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   21.65  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 244000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.1605   | \n",
      " |    critic_loss     | 1.0401   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 243899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=245000, episode_reward=-22.01 +/- 1.4477933563758394 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   22.01  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 245000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.7487   | \n",
      " |    critic_loss     | 1.1872   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 244899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=246000, episode_reward=-23.2 +/- 1.333407522201106 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |    23.2  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 246000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.3815   | \n",
      " |    critic_loss     | 1.0954   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 245899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=247000, episode_reward=-23.3 +/- 1.3957365742610466 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |    23.3  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 247000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.9997   | \n",
      " |    critic_loss     | 1.4999   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 246899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=248000, episode_reward=-23.78 +/- 1.33041880844981 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   23.78  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 248000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.1035   | \n",
      " |    critic_loss     | 1.2759   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 247899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=249000, episode_reward=-24.71 +/- 1.5594001721516515 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   24.71  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 249000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.5588   | \n",
      " |    critic_loss     | 1.3897   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 248899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=250000, episode_reward=-24.98 +/- 1.6803233768155965 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   24.98  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 250000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.1402   | \n",
      " |    critic_loss     | 1.5351   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 249899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=251000, episode_reward=-25.73 +/- 1.520356147362035 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   25.73  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 251000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.0989   | \n",
      " |    critic_loss     | 1.2747   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 250899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=252000, episode_reward=-25.69 +/- 1.9423491045939947 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   25.69  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 252000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.5917   | \n",
      " |    critic_loss     | 1.6479   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 251899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=253000, episode_reward=-26.78 +/- 1.6697550463192894 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   26.78  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 253000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.3713   | \n",
      " |    critic_loss     | 1.3428   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 252899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=254000, episode_reward=-26.79 +/- 1.6576463407527315 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   26.79  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 254000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.9816   | \n",
      " |    critic_loss     | 1.4954   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 253899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=255000, episode_reward=-27.33 +/- 1.999904536785028 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   27.33  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 255000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.7464   | \n",
      " |    critic_loss     | 0.9366   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 254899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=256000, episode_reward=-28.38 +/- 1.9575689186705738 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   28.38  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 256000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.2933   | \n",
      " |    critic_loss     | 1.5733   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 255899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=257000, episode_reward=-28.25 +/- 1.6287951403707996 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   28.25  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 257000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.6006   | \n",
      " |    critic_loss     | 1.1502   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 256899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=258000, episode_reward=-28.73 +/- 1.3802631941140846 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   28.73  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 258000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.1321   | \n",
      " |    critic_loss     | 1.0330   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 257899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=259000, episode_reward=-29.52 +/- 1.8847368197629044 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   29.52  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 259000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.9714   | \n",
      " |    critic_loss     | 1.2428   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 258899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=260000, episode_reward=-29.78 +/- 1.3101745421232418 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   29.78  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 260000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.0353   | \n",
      " |    critic_loss     | 0.7588   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 259899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=261000, episode_reward=-30.4 +/- 1.0128570041676446 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |    30.4  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 261000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.8156   | \n",
      " |    critic_loss     | 1.7039   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 260899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=262000, episode_reward=-31.4 +/- 1.9415948259146052 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |    31.4  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 262000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.7247   | \n",
      " |    critic_loss     | 1.1812   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 261899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=263000, episode_reward=-31.69 +/- 1.1608093841389033 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   31.69  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 263000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.6885   | \n",
      " |    critic_loss     | 1.1721   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 262899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=264000, episode_reward=-31.53 +/- 1.5048948829532158 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   31.53  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 264000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.0055   | \n",
      " |    critic_loss     | 1.0014   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 263899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=265000, episode_reward=-32.78 +/- 1.9700920786829972 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   32.78  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 265000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.6776   | \n",
      " |    critic_loss     | 1.1694   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 264899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=266000, episode_reward=-32.97 +/- 1.16215077823848 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   32.97  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 266000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.8287   | \n",
      " |    critic_loss     | 1.4572   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 265899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=267000, episode_reward=-33.29 +/- 1.026209537167776 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   33.29  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 267000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.3991   | \n",
      " |    critic_loss     | 0.8498   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 266899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=268000, episode_reward=-34.14 +/- 1.9030513031782101 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   34.14  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 268000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.5409   | \n",
      " |    critic_loss     | 1.1352   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 267899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=269000, episode_reward=-34.73 +/- 1.0059461771836298 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   34.73  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 269000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.0785   | \n",
      " |    critic_loss     | 1.5196   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 268899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=270000, episode_reward=-34.8 +/- 1.5598779018550581 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |    34.8  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 270000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.8196   | \n",
      " |    critic_loss     | 1.7049   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 269899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=271000, episode_reward=-35.26 +/- 1.5686160620947007 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   35.26  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 271000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.7224   | \n",
      " |    critic_loss     | 1.1806   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 270899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=272000, episode_reward=-36.17 +/- 1.2348268844634493 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   36.17  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 272000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.6007   | \n",
      " |    critic_loss     | 1.6502   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 271899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=273000, episode_reward=-36.78 +/- 1.9795326845947478 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   36.78  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 273000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.0614   | \n",
      " |    critic_loss     | 1.2654   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 272899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=274000, episode_reward=-37.21 +/- 1.8646255034564585 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   37.21  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 274000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.2006   | \n",
      " |    critic_loss     | 1.3002   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 273899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=275000, episode_reward=-37.54 +/- 1.8263269425803443 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   37.54  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 275000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.7987   | \n",
      " |    critic_loss     | 0.9497   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 274899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=276000, episode_reward=-38.17 +/- 1.5816176480347424 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   38.17  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 276000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.5241   | \n",
      " |    critic_loss     | 0.8810   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 275899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=277000, episode_reward=-38.33 +/- 1.4425133845851401 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   38.33  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 277000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.8548   | \n",
      " |    critic_loss     | 1.2137   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 276899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=278000, episode_reward=-38.93 +/- 1.439517224204541 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   38.93  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 278000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.2769   | \n",
      " |    critic_loss     | 1.5692   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 277899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=279000, episode_reward=-39.55 +/- 1.745874226568377 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   39.55  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 279000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.4244   | \n",
      " |    critic_loss     | 1.3561   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 278899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=280000, episode_reward=-39.85 +/- 1.1569708109615098 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   39.85  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 280000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.6356   | \n",
      " |    critic_loss     | 1.6589   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 279899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=281000, episode_reward=-40.52 +/- 1.3637278334893934 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   40.52  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 281000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.2139   | \n",
      " |    critic_loss     | 0.8035   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 280899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=282000, episode_reward=-41.26 +/- 1.055099197263102 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   41.26  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 282000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.3870   | \n",
      " |    critic_loss     | 1.3468   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 281899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=283000, episode_reward=-41.66 +/- 1.3523974392052196 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   41.66  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 283000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.2212   | \n",
      " |    critic_loss     | 1.5553   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 282899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=284000, episode_reward=-41.63 +/- 1.7672421208765017 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   41.63  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 284000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.5774   | \n",
      " |    critic_loss     | 0.8943   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 283899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=285000, episode_reward=-42.15 +/- 1.2358180649292279 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   42.15  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 285000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.8891   | \n",
      " |    critic_loss     | 1.2223   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 284899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=286000, episode_reward=-42.63 +/- 1.7907378836320653 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   42.63  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 286000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.1968   | \n",
      " |    critic_loss     | 0.7992   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 285899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=287000, episode_reward=-43.48 +/- 1.8762344399425848 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   43.48  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 287000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.9102   | \n",
      " |    critic_loss     | 0.9775   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 286899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=288000, episode_reward=-44.47 +/- 1.1114652288187412 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   44.47  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 288000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.0298   | \n",
      " |    critic_loss     | 0.7575   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 287899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=289000, episode_reward=-44.77 +/- 1.818083254150753 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   44.77  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 289000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.6948   | \n",
      " |    critic_loss     | 0.9237   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 288899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=290000, episode_reward=-44.9 +/- 1.6329666158937628 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |    44.9  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 290000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.4455   | \n",
      " |    critic_loss     | 0.8614   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 289899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=291000, episode_reward=-45.3 +/- 1.2130917268302117 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |    45.3  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 291000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.8257   | \n",
      " |    critic_loss     | 1.4564   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 290899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=292000, episode_reward=-46.29 +/- 1.8455494216030506 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   46.29  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 292000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.4106   | \n",
      " |    critic_loss     | 1.3526   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 291899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=293000, episode_reward=-46.88 +/- 1.4853832131170739 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   46.88  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 293000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.5323   | \n",
      " |    critic_loss     | 1.1331   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 292899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=294000, episode_reward=-46.69 +/- 1.383090216807012 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   46.69  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 294000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.8134   | \n",
      " |    critic_loss     | 1.2033   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 293899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=295000, episode_reward=-47.87 +/- 1.173978509327375 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   47.87  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 295000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.0374   | \n",
      " |    critic_loss     | 1.5094   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 294899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=296000, episode_reward=-48.32 +/- 1.1531765810826249 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   48.32  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 296000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.8254   | \n",
      " |    critic_loss     | 0.7064   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 295899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=297000, episode_reward=-48.65 +/- 1.5856108027990958 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   48.65  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 297000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.2412   | \n",
      " |    critic_loss     | 0.8103   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 296899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=298000, episode_reward=-48.71 +/- 1.0989668489787403 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   48.71  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 298000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.0036   | \n",
      " |    critic_loss     | 1.0009   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 297899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=299000, episode_reward=-49.87 +/- 1.1534026772270332 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   49.87  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 299000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.4357   | \n",
      " |    critic_loss     | 0.8589   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 298899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=300000, episode_reward=-50.19 +/- 1.571142954570584 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   50.19  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 300000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.2533   | \n",
      " |    critic_loss     | 1.0633   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 299899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=301000, episode_reward=-50.88 +/- 1.2647458411548334 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   50.88  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 301000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.6379   | \n",
      " |    critic_loss     | 1.4095   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 300899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=302000, episode_reward=-50.97 +/- 1.1705229334349667 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   50.97  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 302000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.4069   | \n",
      " |    critic_loss     | 1.6017   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 301899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=303000, episode_reward=-51.47 +/- 1.5450896287232554 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   51.47  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 303000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.8150   | \n",
      " |    critic_loss     | 1.2037   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 302899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=304000, episode_reward=-52.31 +/- 1.7684269065715752 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   52.31  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 304000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.2220   | \n",
      " |    critic_loss     | 1.3055   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 303899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=305000, episode_reward=-52.35 +/- 1.7864538906744962 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   52.35  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 305000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.3509   | \n",
      " |    critic_loss     | 1.3377   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 304899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=306000, episode_reward=-52.67 +/- 1.346688385457247 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   52.67  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 306000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.6440   | \n",
      " |    critic_loss     | 1.4110   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 305899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=307000, episode_reward=-53.43 +/- 1.7386511678254113 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   53.43  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 307000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.8843   | \n",
      " |    critic_loss     | 1.4711   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 306899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=308000, episode_reward=-53.53 +/- 1.436879325775182 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   53.53  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 308000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.4472   | \n",
      " |    critic_loss     | 1.6118   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 307899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=309000, episode_reward=-54.85 +/- 1.1788507549682645 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   54.85  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 309000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.8275   | \n",
      " |    critic_loss     | 1.2069   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 308899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=310000, episode_reward=-54.98 +/- 1.5893536991067343 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   54.98  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 310000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.7071   | \n",
      " |    critic_loss     | 1.4268   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 309899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=311000, episode_reward=-55.49 +/- 1.259696094627424 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   55.49  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 311000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.7245   | \n",
      " |    critic_loss     | 1.4311   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 310899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=312000, episode_reward=-56.38 +/- 1.8855085664823754 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   56.38  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 312000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.1861   | \n",
      " |    critic_loss     | 1.2965   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 311899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=313000, episode_reward=-56.34 +/- 1.9576757124947277 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   56.34  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 313000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.2652   | \n",
      " |    critic_loss     | 1.3163   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 312899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=314000, episode_reward=-56.94 +/- 1.0021090408627202 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   56.94  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 314000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.3650   | \n",
      " |    critic_loss     | 0.8412   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 313899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=315000, episode_reward=-57.46 +/- 1.4196284311664638 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   57.46  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 315000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.0961   | \n",
      " |    critic_loss     | 1.5240   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 314899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=316000, episode_reward=-57.99 +/- 1.1122668070278139 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   57.99  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 316000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.1976   | \n",
      " |    critic_loss     | 1.5494   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 315899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=317000, episode_reward=-58.67 +/- 1.4866073312691919 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   58.67  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 317000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.3386   | \n",
      " |    critic_loss     | 1.3346   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 316899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=318000, episode_reward=-58.96 +/- 1.4360465270287608 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   58.96  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 318000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.4647   | \n",
      " |    critic_loss     | 1.6162   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 317899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=319000, episode_reward=-59.11 +/- 1.019818335011681 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   59.11  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 319000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.9561   | \n",
      " |    critic_loss     | 1.4890   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 318899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=320000, episode_reward=-59.58 +/- 1.1997238716934864 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   59.58  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 320000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.4500   | \n",
      " |    critic_loss     | 1.3625   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 319899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=321000, episode_reward=-60.31 +/- 1.855353661732031 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   60.31  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 321000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.8372   | \n",
      " |    critic_loss     | 1.2093   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 320899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=322000, episode_reward=-61.04 +/- 1.612988474354596 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   61.04  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 322000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.1979   | \n",
      " |    critic_loss     | 1.5495   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 321899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=323000, episode_reward=-61.55 +/- 1.6351931507164044 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   61.55  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 323000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.7359   | \n",
      " |    critic_loss     | 0.9340   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 322899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=324000, episode_reward=-61.76 +/- 1.0608479770821675 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   61.76  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 324000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.6682   | \n",
      " |    critic_loss     | 1.4170   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 323899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=325000, episode_reward=-62.58 +/- 1.5467245894315291 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   62.58  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 325000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.4902   | \n",
      " |    critic_loss     | 1.6226   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 324899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=326000, episode_reward=-63.46 +/- 1.8227545521726478 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   63.46  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 326000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.1533   | \n",
      " |    critic_loss     | 0.7883   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 325899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=327000, episode_reward=-63.57 +/- 1.8745213535914467 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   63.57  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 327000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.4668   | \n",
      " |    critic_loss     | 1.6167   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 326899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=328000, episode_reward=-64.03 +/- 1.252696369849943 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   64.03  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 328000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.9682   | \n",
      " |    critic_loss     | 1.4921   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 327899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=329000, episode_reward=-64.11 +/- 1.738822771729688 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   64.11  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 329000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.8717   | \n",
      " |    critic_loss     | 0.7179   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 328899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=330000, episode_reward=-65.06 +/- 1.9425497971983345 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   65.06  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 330000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.8000   | \n",
      " |    critic_loss     | 0.9500   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 329899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=331000, episode_reward=-65.27 +/- 1.832398033202273 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   65.27  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 331000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.6948   | \n",
      " |    critic_loss     | 1.4237   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 330899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=332000, episode_reward=-65.66 +/- 1.0282709454285381 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   65.66  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 332000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.7410   | \n",
      " |    critic_loss     | 1.4352   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 331899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=333000, episode_reward=-66.18 +/- 1.0495665463049275 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   66.18  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 333000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.5907   | \n",
      " |    critic_loss     | 1.6477   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 332899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=334000, episode_reward=-67.34 +/- 1.9221194701887387 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   67.34  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 334000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.8425   | \n",
      " |    critic_loss     | 0.7106   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 333899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=335000, episode_reward=-67.86 +/- 1.2346395187274335 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   67.86  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 335000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.5855   | \n",
      " |    critic_loss     | 1.3964   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 334899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=336000, episode_reward=-68.48 +/- 1.47596420478173 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   68.48  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 336000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.7329   | \n",
      " |    critic_loss     | 1.4332   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 335899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=337000, episode_reward=-68.37 +/- 1.2795369999709783 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   68.37  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 337000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.9066   | \n",
      " |    critic_loss     | 0.7266   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 336899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=338000, episode_reward=-68.77 +/- 1.9994698759883 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   68.77  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 338000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.3097   | \n",
      " |    critic_loss     | 1.3274   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 337899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=339000, episode_reward=-69.43 +/- 1.7055651174625197 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   69.43  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 339000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.9370   | \n",
      " |    critic_loss     | 1.4843   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 338899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=340000, episode_reward=-69.85 +/- 1.9865868886853493 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   69.85  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 340000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.0528   | \n",
      " |    critic_loss     | 1.2632   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 339899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=341000, episode_reward=-70.41 +/- 1.5118595255413563 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   70.41  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 341000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.7184   | \n",
      " |    critic_loss     | 0.6796   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 340899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=342000, episode_reward=-70.62 +/- 1.9596303353720268 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   70.62  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 342000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.1303   | \n",
      " |    critic_loss     | 1.2826   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 341899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=343000, episode_reward=-71.92 +/- 1.6976219985582646 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   71.92  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 343000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.5409   | \n",
      " |    critic_loss     | 1.1352   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 342899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=344000, episode_reward=-72.04 +/- 1.5315959652784619 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   72.04  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 344000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.1799   | \n",
      " |    critic_loss     | 0.7950   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 343899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=345000, episode_reward=-72.38 +/- 1.746011731207291 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   72.38  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 345000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.0280   | \n",
      " |    critic_loss     | 0.7570   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 344899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=346000, episode_reward=-72.74 +/- 1.5097232497516 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   72.74  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 346000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.7186   | \n",
      " |    critic_loss     | 0.6796   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 345899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=347000, episode_reward=-73.34 +/- 1.6029888990558236 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   73.34  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 347000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.3981   | \n",
      " |    critic_loss     | 1.3495   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 346899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=348000, episode_reward=-74.4 +/- 1.314880797198526 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |    74.4  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 348000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.2711   | \n",
      " |    critic_loss     | 1.0678   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 347899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=349000, episode_reward=-74.12 +/- 1.1703265728349694 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   74.12  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 349000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.2020   | \n",
      " |    critic_loss     | 1.0505   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 348899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=350000, episode_reward=-75.26 +/- 1.4212861291697232 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   75.26  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 350000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.3630   | \n",
      " |    critic_loss     | 0.8408   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 349899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=351000, episode_reward=-75.78 +/- 1.6835544955894697 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   75.78  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 351000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.9549   | \n",
      " |    critic_loss     | 1.2387   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 350899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=352000, episode_reward=-75.83 +/- 1.8741433961345735 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   75.83  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 352000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.3348   | \n",
      " |    critic_loss     | 0.8337   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 351899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=353000, episode_reward=-76.61 +/- 1.8269766673887995 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   76.61  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 353000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.6340   | \n",
      " |    critic_loss     | 0.9085   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 352899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=354000, episode_reward=-76.75 +/- 1.8519711467462707 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   76.75  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 354000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.6395   | \n",
      " |    critic_loss     | 1.4099   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 353899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=355000, episode_reward=-77.11 +/- 1.0597241778908628 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   77.11  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 355000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.7274   | \n",
      " |    critic_loss     | 0.6819   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 354899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=356000, episode_reward=-77.64 +/- 1.8340161788001415 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   77.64  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 356000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.4927   | \n",
      " |    critic_loss     | 0.8732   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 355899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=357000, episode_reward=-78.44 +/- 1.4395258352089324 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   78.44  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 357000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.9565   | \n",
      " |    critic_loss     | 1.2391   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 356899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=358000, episode_reward=-78.55 +/- 1.022348390382962 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   78.55  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 358000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.3234   | \n",
      " |    critic_loss     | 1.0809   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 357899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=359000, episode_reward=-79.61 +/- 1.9453744548623502 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   79.61  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 359000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.1795   | \n",
      " |    critic_loss     | 1.0449   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 358899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=360000, episode_reward=-79.55 +/- 1.2658214900621465 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   79.55  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 360000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.6344   | \n",
      " |    critic_loss     | 0.9086   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 359899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=361000, episode_reward=-80.45 +/- 1.364297385736278 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   80.45  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 361000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.5266   | \n",
      " |    critic_loss     | 1.3816   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 360899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=362000, episode_reward=-81.16 +/- 1.1665812663998536 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   81.16  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 362000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.8279   | \n",
      " |    critic_loss     | 1.2070   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 361899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=363000, episode_reward=-81.91 +/- 1.9422760752644286 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   81.91  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 363000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.5860   | \n",
      " |    critic_loss     | 1.3965   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 362899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=364000, episode_reward=-82.32 +/- 1.6299210437344813 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   82.32  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 364000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.4969   | \n",
      " |    critic_loss     | 0.8742   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 363899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=365000, episode_reward=-82.96 +/- 1.5446844937786715 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   82.96  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 365000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.2745   | \n",
      " |    critic_loss     | 0.8186   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 364899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=366000, episode_reward=-83.41 +/- 1.926691624485266 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   83.41  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 366000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.8146   | \n",
      " |    critic_loss     | 0.9536   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 365899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=367000, episode_reward=-83.99 +/- 1.0968629214149908 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   83.99  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 367000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.3766   | \n",
      " |    critic_loss     | 1.3441   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 366899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=368000, episode_reward=-83.96 +/- 1.7906860715760995 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   83.96  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 368000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.0278   | \n",
      " |    critic_loss     | 1.0069   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 367899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=369000, episode_reward=-84.78 +/- 1.6109958432382867 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   84.78  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 369000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.0232   | \n",
      " |    critic_loss     | 0.7558   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 368899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=370000, episode_reward=-85.05 +/- 1.529740800652605 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   85.05  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 370000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.6714   | \n",
      " |    critic_loss     | 1.1679   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 369899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=371000, episode_reward=-85.3 +/- 1.3596448537998214 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |    85.3  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 371000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.3785   | \n",
      " |    critic_loss     | 0.8446   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 370899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=372000, episode_reward=-85.72 +/- 1.4238393347122074 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   85.72  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 372000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.8729   | \n",
      " |    critic_loss     | 0.7182   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 371899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=373000, episode_reward=-86.42 +/- 1.5344651822603077 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   86.42  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 373000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.1291   | \n",
      " |    critic_loss     | 1.5323   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 372899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=374000, episode_reward=-87.11 +/- 1.5055427488720499 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   87.11  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 374000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.7906   | \n",
      " |    critic_loss     | 1.1977   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 373899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=375000, episode_reward=-88.0 +/- 1.6058121075596627 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |    88.0  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 375000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.5885   | \n",
      " |    critic_loss     | 1.3971   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 374899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=376000, episode_reward=-87.62 +/- 1.614115672159992 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   87.62  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 376000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.1653   | \n",
      " |    critic_loss     | 1.2913   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 375899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=377000, episode_reward=-88.62 +/- 1.127771149594166 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   88.62  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 377000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.1178   | \n",
      " |    critic_loss     | 1.5294   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 376899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=378000, episode_reward=-89.43 +/- 1.374418890971723 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   89.43  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 378000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.0769   | \n",
      " |    critic_loss     | 1.0192   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 377899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=379000, episode_reward=-89.86 +/- 1.459017299694421 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   89.86  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 379000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.2865   | \n",
      " |    critic_loss     | 1.0716   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 378899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=380000, episode_reward=-90.04 +/- 1.9578099614516558 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   90.04  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 380000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.1145   | \n",
      " |    critic_loss     | 1.5286   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 379899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=381000, episode_reward=-90.01 +/- 1.12908882696851 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   90.01  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 381000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.4936   | \n",
      " |    critic_loss     | 1.3734   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 380899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=382000, episode_reward=-91.26 +/- 1.8857593014663903 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   91.26  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 382000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.3268   | \n",
      " |    critic_loss     | 1.3317   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 381899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=383000, episode_reward=-91.13 +/- 1.0750218459924938 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   91.13  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 383000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.1782   | \n",
      " |    critic_loss     | 0.7945   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 382899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=384000, episode_reward=-92.28 +/- 1.1959199552717374 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   92.28  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 384000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.5382   | \n",
      " |    critic_loss     | 1.3845   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 383899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=385000, episode_reward=-92.82 +/- 1.7858241768608085 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   92.82  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 385000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.6773   | \n",
      " |    critic_loss     | 1.4193   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 384899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=386000, episode_reward=-93.31 +/- 1.1853113923454708 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   93.31  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 386000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.3072   | \n",
      " |    critic_loss     | 1.3268   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 385899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=387000, episode_reward=-93.35 +/- 1.5153996831062972 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   93.35  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 387000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.2934   | \n",
      " |    critic_loss     | 1.5734   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 386899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=388000, episode_reward=-93.7 +/- 1.3205010866564912 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |    93.7  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 388000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.2611   | \n",
      " |    critic_loss     | 1.5653   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 387899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=389000, episode_reward=-94.62 +/- 1.4724004300631512 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   94.62  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 389000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.3282   | \n",
      " |    critic_loss     | 1.0821   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 388899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=390000, episode_reward=-95.36 +/- 1.3592364992175345 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   95.36  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 390000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.6453   | \n",
      " |    critic_loss     | 0.6613   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 389899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=391000, episode_reward=-95.24 +/- 1.222929776514806 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   95.24  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 391000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.2904   | \n",
      " |    critic_loss     | 1.0726   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 390899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=392000, episode_reward=-95.77 +/- 1.7767709653390855 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   95.77  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 392000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.0017   | \n",
      " |    critic_loss     | 1.2504   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 391899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=393000, episode_reward=-96.82 +/- 1.422831863316493 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   96.82  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 393000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.4704   | \n",
      " |    critic_loss     | 1.3676   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 392899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=394000, episode_reward=-97.28 +/- 1.5220654120392816 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   97.28  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 394000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.3649   | \n",
      " |    critic_loss     | 0.8412   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 393899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=395000, episode_reward=-97.81 +/- 1.6138967735870329 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   97.81  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 395000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.3458   | \n",
      " |    critic_loss     | 1.0864   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 394899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=396000, episode_reward=-97.73 +/- 1.7971801174233535 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   97.73  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 396000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.8657   | \n",
      " |    critic_loss     | 1.2164   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 395899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=397000, episode_reward=-98.59 +/- 1.6029505346017292 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   98.59  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 397000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.7042   | \n",
      " |    critic_loss     | 0.9260   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 396899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=398000, episode_reward=-99.35 +/- 1.6966636100452777 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   99.35  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 398000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.3979   | \n",
      " |    critic_loss     | 1.0995   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 397899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=399000, episode_reward=-100.0 +/- 1.5588586536766433 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   100.0  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 399000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.9489   | \n",
      " |    critic_loss     | 0.7372   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 398899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=400000, episode_reward=-99.81 +/- 1.3766365995865026 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   99.81  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 400000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.6215   | \n",
      " |    critic_loss     | 0.9054   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 399899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=401000, episode_reward=-100.04 +/- 1.8109076222544214 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  100.04  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 401000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.8422   | \n",
      " |    critic_loss     | 0.7106   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 400899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=402000, episode_reward=-101.14 +/- 1.392089145225652 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  101.14  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 402000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.9811   | \n",
      " |    critic_loss     | 1.4953   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 401899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=403000, episode_reward=-101.94 +/- 1.2131164811951674 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  101.94  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 403000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.1835   | \n",
      " |    critic_loss     | 1.2959   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 402899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=404000, episode_reward=-101.62 +/- 1.3017940163431212 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  101.62  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 404000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.9901   | \n",
      " |    critic_loss     | 1.4975   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 403899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=405000, episode_reward=-102.47 +/- 1.0344982012738435 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  102.47  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 405000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.2428   | \n",
      " |    critic_loss     | 0.8107   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 404899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=406000, episode_reward=-103.48 +/- 1.6229528464673155 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  103.48  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 406000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.9026   | \n",
      " |    critic_loss     | 1.4756   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 405899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=407000, episode_reward=-103.27 +/- 1.8388332470200943 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  103.27  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 407000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.6093   | \n",
      " |    critic_loss     | 1.1523   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 406899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=408000, episode_reward=-104.09 +/- 1.920423733781917 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  104.09  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 408000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.5162   | \n",
      " |    critic_loss     | 0.6291   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 407899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=409000, episode_reward=-104.24 +/- 1.501809851035273 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  104.24  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 409000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.8876   | \n",
      " |    critic_loss     | 1.2219   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 408899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=410000, episode_reward=-105.03 +/- 1.867409021774739 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  105.03  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 410000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.5300   | \n",
      " |    critic_loss     | 1.3825   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 409899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=411000, episode_reward=-105.77 +/- 1.6091259829128401 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  105.77  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 411000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.0950   | \n",
      " |    critic_loss     | 1.5238   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 410899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=412000, episode_reward=-106.07 +/- 1.0360971617986388 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  106.07  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 412000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.9230   | \n",
      " |    critic_loss     | 1.4807   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 411899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=413000, episode_reward=-106.5 +/- 1.3614698925816908 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   106.5  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 413000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.6583   | \n",
      " |    critic_loss     | 1.4146   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 412899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=414000, episode_reward=-106.55 +/- 1.3749026061156373 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  106.55  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 414000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.6250   | \n",
      " |    critic_loss     | 1.4062   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 413899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=415000, episode_reward=-107.68 +/- 1.7792411371150685 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  107.68  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 415000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.1725   | \n",
      " |    critic_loss     | 1.0431   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 414899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=416000, episode_reward=-108.15 +/- 1.6421679703299632 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  108.15  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 416000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.7222   | \n",
      " |    critic_loss     | 1.4305   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 415899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=417000, episode_reward=-108.91 +/- 1.6645834506652721 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  108.91  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 417000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.7805   | \n",
      " |    critic_loss     | 0.6951   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 416899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=418000, episode_reward=-108.51 +/- 1.4778389793226223 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  108.51  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 418000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.6812   | \n",
      " |    critic_loss     | 0.9203   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 417899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=419000, episode_reward=-109.92 +/- 1.760048523060491 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  109.92  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 419000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.4825   | \n",
      " |    critic_loss     | 1.3706   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 418899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=420000, episode_reward=-110.24 +/- 1.1412501847497387 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  110.24  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 420000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.3723   | \n",
      " |    critic_loss     | 1.3431   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 419899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=421000, episode_reward=-110.92 +/- 1.6319627465951152 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  110.92  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 421000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.8220   | \n",
      " |    critic_loss     | 1.2055   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 420899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=422000, episode_reward=-111.39 +/- 1.401100763766194 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  111.39  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 422000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.6425   | \n",
      " |    critic_loss     | 0.6606   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 421899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=423000, episode_reward=-111.03 +/- 1.5608929781174745 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  111.03  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 423000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.3238   | \n",
      " |    critic_loss     | 0.5810   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 422899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=424000, episode_reward=-111.85 +/- 1.6125547976666834 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  111.85  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 424000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.8241   | \n",
      " |    critic_loss     | 0.9560   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 423899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=425000, episode_reward=-112.05 +/- 1.186785695384959 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  112.05  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 425000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.8308   | \n",
      " |    critic_loss     | 1.2077   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 424899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=426000, episode_reward=-113.24 +/- 1.781991806667115 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  113.24  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 426000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.1482   | \n",
      " |    critic_loss     | 1.5370   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 425899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=427000, episode_reward=-113.83 +/- 1.7964277138479914 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  113.83  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 427000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.2607   | \n",
      " |    critic_loss     | 1.0652   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 426899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=428000, episode_reward=-113.61 +/- 1.9197941323501608 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  113.61  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 428000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.8654   | \n",
      " |    critic_loss     | 1.2164   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 427899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=429000, episode_reward=-114.38 +/- 1.2909302295485472 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  114.38  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 429000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.0725   | \n",
      " |    critic_loss     | 1.5181   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 428899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=430000, episode_reward=-115.09 +/- 1.2154803853468885 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  115.09  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 430000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.6068   | \n",
      " |    critic_loss     | 1.1517   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 429899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=431000, episode_reward=-115.17 +/- 1.8242887979337032 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  115.17  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 431000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.5266   | \n",
      " |    critic_loss     | 1.1316   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 430899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=432000, episode_reward=-116.49 +/- 1.3689070704717705 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  116.49  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 432000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.2921   | \n",
      " |    critic_loss     | 1.0730   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 431899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=433000, episode_reward=-116.92 +/- 1.8035896937136435 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  116.92  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 433000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.8984   | \n",
      " |    critic_loss     | 0.7246   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 432899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=434000, episode_reward=-116.99 +/- 1.349744138010681 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  116.99  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 434000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.6654   | \n",
      " |    critic_loss     | 0.9164   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 433899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=435000, episode_reward=-117.21 +/- 1.2081905696514101 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  117.21  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 435000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.1843   | \n",
      " |    critic_loss     | 1.2961   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 434899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=436000, episode_reward=-118.42 +/- 1.2082994455991092 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  118.42  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 436000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.1719   | \n",
      " |    critic_loss     | 0.7930   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 435899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=437000, episode_reward=-118.81 +/- 1.2177454063596143 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  118.81  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 437000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.8087   | \n",
      " |    critic_loss     | 0.9522   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 436899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=438000, episode_reward=-118.9 +/- 1.217115153434019 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   118.9  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 438000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.8384   | \n",
      " |    critic_loss     | 1.4596   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 437899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=439000, episode_reward=-119.53 +/- 1.0288591171132375 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  119.53  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 439000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.6721   | \n",
      " |    critic_loss     | 1.4180   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 438899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=440000, episode_reward=-119.68 +/- 1.177474213489587 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  119.68  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 440000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.4510   | \n",
      " |    critic_loss     | 0.6128   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 439899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=441000, episode_reward=-120.22 +/- 1.1828192007635838 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  120.22  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 441000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.9034   | \n",
      " |    critic_loss     | 1.2258   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 440899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=442000, episode_reward=-121.37 +/- 1.263650030469739 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  121.37  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 442000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.9125   | \n",
      " |    critic_loss     | 1.4781   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 441899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=443000, episode_reward=-121.32 +/- 1.7030745055430891 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  121.32  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 443000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.0731   | \n",
      " |    critic_loss     | 1.5183   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 442899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=444000, episode_reward=-121.61 +/- 1.2092958795113344 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  121.61  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 444000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.5053   | \n",
      " |    critic_loss     | 0.6263   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 443899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=445000, episode_reward=-122.66 +/- 1.1281636573614948 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  122.66  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 445000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.1285   | \n",
      " |    critic_loss     | 1.0321   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 444899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=446000, episode_reward=-122.75 +/- 1.0031774073437238 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  122.75  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 446000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.7203   | \n",
      " |    critic_loss     | 0.9301   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 445899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=447000, episode_reward=-123.94 +/- 1.425909241716455 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  123.94  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 447000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.9935   | \n",
      " |    critic_loss     | 0.7484   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 446899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=448000, episode_reward=-124.19 +/- 1.4234371473306344 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  124.19  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 448000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.5979   | \n",
      " |    critic_loss     | 1.1495   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 447899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=449000, episode_reward=-124.2 +/- 1.704716444311447 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   124.2  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 449000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.0756   | \n",
      " |    critic_loss     | 1.0189   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 448899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=450000, episode_reward=-124.66 +/- 1.3846847425128086 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  124.66  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 450000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.3460   | \n",
      " |    critic_loss     | 1.0865   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 449899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=451000, episode_reward=-125.96 +/- 1.2266688530478929 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  125.96  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 451000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.0834   | \n",
      " |    critic_loss     | 1.0209   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 450899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=452000, episode_reward=-125.87 +/- 1.6300373783522617 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  125.87  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 452000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.7649   | \n",
      " |    critic_loss     | 0.6912   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 451899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=453000, episode_reward=-126.12 +/- 1.1878353817016476 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  126.12  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 453000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.4796   | \n",
      " |    critic_loss     | 1.1199   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 452899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=454000, episode_reward=-127.35 +/- 1.811911312053938 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  127.35  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 454000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.4654   | \n",
      " |    critic_loss     | 0.8664   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 453899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=455000, episode_reward=-127.64 +/- 1.2983676732838902 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  127.64  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 455000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.2229   | \n",
      " |    critic_loss     | 1.3057   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 454899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=456000, episode_reward=-128.25 +/- 1.2200601830103408 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  128.25  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 456000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.2555   | \n",
      " |    critic_loss     | 0.8139   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 455899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=457000, episode_reward=-128.77 +/- 1.8548415805995788 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  128.77  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 457000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.8547   | \n",
      " |    critic_loss     | 0.9637   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 456899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=458000, episode_reward=-129.25 +/- 1.3916137368845924 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  129.25  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 458000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.6474   | \n",
      " |    critic_loss     | 0.6619   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 457899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=459000, episode_reward=-129.66 +/- 1.3824135680035676 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  129.66  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 459000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.9900   | \n",
      " |    critic_loss     | 0.9975   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 458899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=460000, episode_reward=-129.58 +/- 1.4905394186645462 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  129.58  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 460000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.2239   | \n",
      " |    critic_loss     | 1.0560   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 459899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=461000, episode_reward=-130.87 +/- 1.8100005095970495 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  130.87  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 461000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.4301   | \n",
      " |    critic_loss     | 0.6075   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 460899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=462000, episode_reward=-131.29 +/- 1.4787807275790352 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  131.29  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 462000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.5111   | \n",
      " |    critic_loss     | 1.1278   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 461899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=463000, episode_reward=-131.11 +/- 1.7735739269114472 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  131.11  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 463000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.3252   | \n",
      " |    critic_loss     | 1.3313   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 462899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=464000, episode_reward=-132.07 +/- 1.6822359623157968 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  132.07  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 464000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.7772   | \n",
      " |    critic_loss     | 1.1943   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 463899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=465000, episode_reward=-132.3 +/- 1.4258401143324337 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   132.3  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 465000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.6932   | \n",
      " |    critic_loss     | 1.1733   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 464899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=466000, episode_reward=-133.36 +/- 1.22080509919084 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  133.36  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 466000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.2175   | \n",
      " |    critic_loss     | 1.0544   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 465899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=467000, episode_reward=-133.9 +/- 1.3225491511000769 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   133.9  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 467000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.8848   | \n",
      " |    critic_loss     | 1.4712   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 466899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=468000, episode_reward=-133.72 +/- 1.4762612192433362 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  133.72  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 468000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.0386   | \n",
      " |    critic_loss     | 1.2597   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 467899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=469000, episode_reward=-134.86 +/- 1.3588275073551523 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  134.86  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 469000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.4708   | \n",
      " |    critic_loss     | 1.3677   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 468899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=470000, episode_reward=-134.89 +/- 1.639951697740924 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  134.89  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 470000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.2031   | \n",
      " |    critic_loss     | 0.8008   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 469899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=471000, episode_reward=-135.25 +/- 1.8898063557772562 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  135.25  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 471000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.5395   | \n",
      " |    critic_loss     | 1.3849   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 470899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=472000, episode_reward=-135.83 +/- 1.6471378466092714 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  135.83  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 472000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.9941   | \n",
      " |    critic_loss     | 1.2485   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 471899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=473000, episode_reward=-136.79 +/- 1.3501338388336133 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  136.79  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 473000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.0184   | \n",
      " |    critic_loss     | 0.7546   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 472899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=474000, episode_reward=-137.07 +/- 1.0607552401110052 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  137.07  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 474000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.3718   | \n",
      " |    critic_loss     | 1.0930   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 473899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=475000, episode_reward=-137.68 +/- 1.3058275730847053 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  137.68  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 475000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.6319   | \n",
      " |    critic_loss     | 0.9080   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 474899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=476000, episode_reward=-137.95 +/- 1.2489909286656036 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  137.95  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 476000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.9708   | \n",
      " |    critic_loss     | 1.4927   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 475899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=477000, episode_reward=-138.3 +/- 1.4948222046731992 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   138.3  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 477000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.9109   | \n",
      " |    critic_loss     | 1.4777   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 476899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=478000, episode_reward=-138.83 +/- 1.9868402971362276 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  138.83  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 478000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.4120   | \n",
      " |    critic_loss     | 0.6030   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 477899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=479000, episode_reward=-139.12 +/- 1.6124010933934163 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  139.12  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 479000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.8101   | \n",
      " |    critic_loss     | 0.9525   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 478899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=480000, episode_reward=-140.12 +/- 1.67235782197069 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  140.12  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 480000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.8235   | \n",
      " |    critic_loss     | 0.7059   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 479899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=481000, episode_reward=-140.36 +/- 1.1860410757453301 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  140.36  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 481000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.5438   | \n",
      " |    critic_loss     | 0.8859   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 480899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=482000, episode_reward=-141.4 +/- 1.8897770288628255 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   141.4  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 482000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.5028   | \n",
      " |    critic_loss     | 1.1257   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 481899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=483000, episode_reward=-141.49 +/- 1.3153021573795278 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  141.49  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 483000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.0218   | \n",
      " |    critic_loss     | 1.2554   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 482899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=484000, episode_reward=-141.91 +/- 1.2190946324837626 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  141.91  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 484000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.5854   | \n",
      " |    critic_loss     | 0.6463   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 483899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=485000, episode_reward=-142.59 +/- 1.8374224176376979 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  142.59  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 485000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.3670   | \n",
      " |    critic_loss     | 0.5918   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 484899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=486000, episode_reward=-143.03 +/- 1.4493948500513523 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  143.03  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 486000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.0184   | \n",
      " |    critic_loss     | 0.7546   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 485899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=487000, episode_reward=-143.83 +/- 1.7337123393462397 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  143.83  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 487000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.6942   | \n",
      " |    critic_loss     | 1.1735   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 486899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=488000, episode_reward=-143.97 +/- 1.7940964590004675 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  143.97  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 488000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.8389   | \n",
      " |    critic_loss     | 0.7097   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 487899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=489000, episode_reward=-144.64 +/- 1.1061220965040013 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  144.64  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 489000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.6056   | \n",
      " |    critic_loss     | 1.1514   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 488899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=490000, episode_reward=-145.28 +/- 1.2942761990148846 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  145.28  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 490000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.5936   | \n",
      " |    critic_loss     | 0.6484   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 489899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=491000, episode_reward=-145.12 +/- 1.1801031392752837 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  145.12  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 491000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.8345   | \n",
      " |    critic_loss     | 1.2086   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 490899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=492000, episode_reward=-145.82 +/- 1.7657883757212272 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  145.82  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 492000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.3670   | \n",
      " |    critic_loss     | 1.3417   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 491899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=493000, episode_reward=-146.29 +/- 1.8468565192945312 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  146.29  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 493000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.5716   | \n",
      " |    critic_loss     | 0.8929   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 492899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=494000, episode_reward=-146.96 +/- 1.8201119516210618 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  146.96  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 494000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.1436   | \n",
      " |    critic_loss     | 0.7859   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 493899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=495000, episode_reward=-147.73 +/- 1.3774872769260709 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  147.73  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 495000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.5950   | \n",
      " |    critic_loss     | 1.1488   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 494899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=496000, episode_reward=-147.77 +/- 1.871211354632598 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  147.77  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 496000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.1505   | \n",
      " |    critic_loss     | 1.0376   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 495899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=497000, episode_reward=-148.27 +/- 1.159847195063909 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  148.27  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 497000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.1259   | \n",
      " |    critic_loss     | 0.5315   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 496899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=498000, episode_reward=-148.6 +/- 1.2646745491785876 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   148.6  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 498000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.4404   | \n",
      " |    critic_loss     | 1.1101   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 497899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=499000, episode_reward=-149.49 +/- 1.428985847708673 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  149.49  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 499000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.2098   | \n",
      " |    critic_loss     | 1.3025   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 498899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=500000, episode_reward=-150.31 +/- 1.3233126571626785 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  150.31  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 500000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.4743   | \n",
      " |    critic_loss     | 0.6186   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 499899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=501000, episode_reward=-150.99 +/- 1.5849711524834822 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  150.99  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 501000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.9027   | \n",
      " |    critic_loss     | 1.2257   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 500899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=502000, episode_reward=-150.98 +/- 1.6505534385665808 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  150.98  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 502000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.2629   | \n",
      " |    critic_loss     | 0.5657   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 501899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=503000, episode_reward=-151.18 +/- 1.1072648934526759 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  151.18  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 503000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.9775   | \n",
      " |    critic_loss     | 0.9944   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 502899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=504000, episode_reward=-151.95 +/- 1.403298761780303 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  151.95  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 504000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.3946   | \n",
      " |    critic_loss     | 1.3487   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 503899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=505000, episode_reward=-152.96 +/- 1.2851843734230703 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  152.96  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 505000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.9623   | \n",
      " |    critic_loss     | 0.7406   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 504899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=506000, episode_reward=-152.67 +/- 1.3188472486576184 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  152.67  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 506000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.1434   | \n",
      " |    critic_loss     | 0.7858   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 505899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=507000, episode_reward=-153.93 +/- 1.7060969606323064 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  153.93  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 507000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.2767   | \n",
      " |    critic_loss     | 0.8192   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 506899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=508000, episode_reward=-153.9 +/- 1.6819350381843137 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   153.9  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 508000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.0131   | \n",
      " |    critic_loss     | 0.5033   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 507899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=509000, episode_reward=-154.87 +/- 1.5309008156003836 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  154.87  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 509000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.1183   | \n",
      " |    critic_loss     | 0.5296   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 508899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=510000, episode_reward=-155.46 +/- 1.1458644126581126 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  155.46  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 510000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.1375   | \n",
      " |    critic_loss     | 0.7844   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 509899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=511000, episode_reward=-155.35 +/- 1.33698004478356 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  155.35  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 511000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.0459   | \n",
      " |    critic_loss     | 0.5115   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 510899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=512000, episode_reward=-156.36 +/- 1.9903263164361222 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  156.36  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 512000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.8251   | \n",
      " |    critic_loss     | 0.9563   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 511899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=513000, episode_reward=-156.1 +/- 1.54668846261271 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   156.1  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 513000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.5763   | \n",
      " |    critic_loss     | 0.8941   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 512899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=514000, episode_reward=-157.29 +/- 1.0454931361463173 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  157.29  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 514000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.6710   | \n",
      " |    critic_loss     | 1.1677   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 513899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=515000, episode_reward=-157.29 +/- 1.0492840615131531 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  157.29  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 515000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.5234   | \n",
      " |    critic_loss     | 0.8809   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 514899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=516000, episode_reward=-157.89 +/- 1.1696505755219508 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  157.89  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 516000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.5459   | \n",
      " |    critic_loss     | 1.3865   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 515899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=517000, episode_reward=-158.43 +/- 1.2644971158368166 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  158.43  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 517000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.6054   | \n",
      " |    critic_loss     | 0.9013   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 516899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=518000, episode_reward=-158.66 +/- 1.2884582019436897 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  158.66  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 518000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.2047   | \n",
      " |    critic_loss     | 1.0512   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 517899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=519000, episode_reward=-159.25 +/- 1.1344224990035228 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  159.25  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 519000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.7477   | \n",
      " |    critic_loss     | 1.1869   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 518899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=520000, episode_reward=-159.69 +/- 1.9275934782615634 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  159.69  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 520000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.5304   | \n",
      " |    critic_loss     | 1.3826   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 519899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=521000, episode_reward=-160.17 +/- 1.0270369090403635 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  160.17  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 521000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.6369   | \n",
      " |    critic_loss     | 0.9092   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 520899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=522000, episode_reward=-160.77 +/- 1.6872187530391094 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  160.77  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 522000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.7710   | \n",
      " |    critic_loss     | 1.4427   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 521899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=523000, episode_reward=-161.66 +/- 1.9749048412553867 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  161.66  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 523000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.1915   | \n",
      " |    critic_loss     | 1.0479   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 522899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=524000, episode_reward=-162.41 +/- 1.3255711751816528 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  162.41  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 524000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.3111   | \n",
      " |    critic_loss     | 0.8278   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 523899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=525000, episode_reward=-162.72 +/- 1.1814963159456044 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  162.72  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 525000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.6211   | \n",
      " |    critic_loss     | 1.4053   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 524899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=526000, episode_reward=-162.77 +/- 1.4410463898677226 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  162.77  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 526000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.1990   | \n",
      " |    critic_loss     | 1.0498   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 525899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=527000, episode_reward=-163.6 +/- 1.0235035732970006 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   163.6  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 527000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.9542   | \n",
      " |    critic_loss     | 0.7385   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 526899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=528000, episode_reward=-163.7 +/- 1.593093334553152 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   163.7  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 528000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.6785   | \n",
      " |    critic_loss     | 1.1696   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 527899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=529000, episode_reward=-164.35 +/- 1.7753917209349661 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  164.35  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 529000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.8088   | \n",
      " |    critic_loss     | 1.2022   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 528899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=530000, episode_reward=-164.59 +/- 1.9138378788368957 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  164.59  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 530000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.2067   | \n",
      " |    critic_loss     | 0.5517   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 529899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=531000, episode_reward=-165.27 +/- 1.9262314679753 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  165.27  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 531000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.5685   | \n",
      " |    critic_loss     | 1.3921   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 530899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=532000, episode_reward=-165.51 +/- 1.4024676363937283 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  165.51  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 532000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.3323   | \n",
      " |    critic_loss     | 1.0831   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 531899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=533000, episode_reward=-166.92 +/- 1.009295060950778 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  166.92  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 533000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.9216   | \n",
      " |    critic_loss     | 1.2304   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 532899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=534000, episode_reward=-166.9 +/- 1.9010167723292066 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   166.9  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 534000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.8975   | \n",
      " |    critic_loss     | 0.4744   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 533899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=535000, episode_reward=-167.28 +/- 1.7525306078442007 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  167.28  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 535000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.9305   | \n",
      " |    critic_loss     | 0.4826   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 534899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=536000, episode_reward=-167.88 +/- 1.083422337499357 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  167.88  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 536000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.2331   | \n",
      " |    critic_loss     | 1.3083   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 535899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=537000, episode_reward=-168.8 +/- 1.7535492900335021 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   168.8  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 537000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.7823   | \n",
      " |    critic_loss     | 0.6956   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 536899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=538000, episode_reward=-169.33 +/- 1.4663404963482192 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  169.33  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 538000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.7370   | \n",
      " |    critic_loss     | 1.4343   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 537899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=539000, episode_reward=-169.26 +/- 1.5430347036536656 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  169.26  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 539000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.8165   | \n",
      " |    critic_loss     | 1.4541   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 538899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=540000, episode_reward=-170.41 +/- 1.7298829257915809 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  170.41  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 540000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.4527   | \n",
      " |    critic_loss     | 1.1132   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 539899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=541000, episode_reward=-170.11 +/- 1.691771271705801 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  170.11  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 541000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.2582   | \n",
      " |    critic_loss     | 0.5646   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 540899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=542000, episode_reward=-170.91 +/- 1.8180958377563838 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  170.91  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 542000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.6310   | \n",
      " |    critic_loss     | 0.6578   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 541899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=543000, episode_reward=-171.87 +/- 1.2362522658300819 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  171.87  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 543000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.0231   | \n",
      " |    critic_loss     | 0.7558   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 542899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=544000, episode_reward=-172.07 +/- 1.9279844811990723 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  172.07  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 544000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.5445   | \n",
      " |    critic_loss     | 1.1361   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 543899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=545000, episode_reward=-172.57 +/- 1.6651025758922506 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  172.57  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 545000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.4084   | \n",
      " |    critic_loss     | 1.1021   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 544899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=546000, episode_reward=-173.1 +/- 1.4425394442590336 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   173.1  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 546000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.2843   | \n",
      " |    critic_loss     | 0.8211   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 545899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=547000, episode_reward=-173.65 +/- 1.7350849018280385 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  173.65  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 547000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.9283   | \n",
      " |    critic_loss     | 0.9821   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 546899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=548000, episode_reward=-174.04 +/- 1.3624394780294067 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  174.04  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 548000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.2672   | \n",
      " |    critic_loss     | 0.8168   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 547899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=549000, episode_reward=-174.27 +/- 1.2802523176767504 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  174.27  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 549000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.5257   | \n",
      " |    critic_loss     | 1.1314   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 548899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=550000, episode_reward=-175.32 +/- 1.424607136693822 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  175.32  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 550000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.8498   | \n",
      " |    critic_loss     | 0.7125   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 549899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=551000, episode_reward=-175.91 +/- 1.7563821337899377 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  175.91  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 551000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.1189   | \n",
      " |    critic_loss     | 1.2797   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 550899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=552000, episode_reward=-175.79 +/- 1.332158342718744 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  175.79  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 552000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.1166   | \n",
      " |    critic_loss     | 1.0292   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 551899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=553000, episode_reward=-176.86 +/- 1.2548658966917072 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  176.86  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 553000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.2774   | \n",
      " |    critic_loss     | 1.3194   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 552899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=554000, episode_reward=-177.23 +/- 1.5012275592325794 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  177.23  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 554000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.4502   | \n",
      " |    critic_loss     | 0.8625   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 553899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=555000, episode_reward=-177.94 +/- 1.7333853202186917 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  177.94  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 555000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.9173   | \n",
      " |    critic_loss     | 1.2293   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 554899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=556000, episode_reward=-177.63 +/- 1.2155848738071084 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  177.63  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 556000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.6996   | \n",
      " |    critic_loss     | 1.4249   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 555899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=557000, episode_reward=-178.24 +/- 1.6560483628728049 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  178.24  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 557000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.0714   | \n",
      " |    critic_loss     | 0.7679   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 556899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=558000, episode_reward=-179.01 +/- 1.84850305935928 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  179.01  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 558000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.0855   | \n",
      " |    critic_loss     | 1.2714   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 557899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=559000, episode_reward=-179.97 +/- 1.5790626819055018 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  179.97  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 559000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.4704   | \n",
      " |    critic_loss     | 0.8676   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 558899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=560000, episode_reward=-179.81 +/- 1.985980876905769 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  179.81  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 560000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.9686   | \n",
      " |    critic_loss     | 0.4921   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 559899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=561000, episode_reward=-180.24 +/- 1.5634838948807321 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  180.24  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 561000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.6297   | \n",
      " |    critic_loss     | 0.6574   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 560899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=562000, episode_reward=-180.85 +/- 1.4740945776028367 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  180.85  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 562000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.6381   | \n",
      " |    critic_loss     | 0.6595   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 561899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=563000, episode_reward=-181.15 +/- 1.1378407907132004 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  181.15  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 563000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.1323   | \n",
      " |    critic_loss     | 1.2831   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 562899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=564000, episode_reward=-181.71 +/- 1.7956807831828248 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  181.71  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 564000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.6328   | \n",
      " |    critic_loss     | 0.9082   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 563899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=565000, episode_reward=-182.42 +/- 1.0965377781289947 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  182.42  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 565000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.7379   | \n",
      " |    critic_loss     | 0.6845   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 564899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=566000, episode_reward=-182.62 +/- 1.6874530109906467 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  182.62  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 566000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.1512   | \n",
      " |    critic_loss     | 0.5378   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 565899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=567000, episode_reward=-183.39 +/- 1.3014822218049331 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  183.39  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 567000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.4063   | \n",
      " |    critic_loss     | 1.1016   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 566899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=568000, episode_reward=-183.93 +/- 1.032910755932107 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  183.93  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 568000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.1265   | \n",
      " |    critic_loss     | 1.2816   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 567899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=569000, episode_reward=-184.05 +/- 1.0491262180689969 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  184.05  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 569000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.7376   | \n",
      " |    critic_loss     | 0.9344   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 568899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=570000, episode_reward=-184.84 +/- 1.2643410838867166 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  184.84  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 570000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.7096   | \n",
      " |    critic_loss     | 1.4274   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 569899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=571000, episode_reward=-185.1 +/- 1.255761471014111 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   185.1  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 571000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.8926   | \n",
      " |    critic_loss     | 0.4731   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 570899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=572000, episode_reward=-185.86 +/- 1.4419661287062646 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  185.86  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 572000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.7139   | \n",
      " |    critic_loss     | 1.1785   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 571899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=573000, episode_reward=-186.22 +/- 1.5226530642214031 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  186.22  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 573000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.5892   | \n",
      " |    critic_loss     | 0.6473   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 572899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=574000, episode_reward=-187.0 +/- 1.2700205859234517 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   187.0  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 574000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.8086   | \n",
      " |    critic_loss     | 0.7021   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 573899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=575000, episode_reward=-187.55 +/- 1.2313960981895122 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  187.55  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 575000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.0661   | \n",
      " |    critic_loss     | 1.2665   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 574899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=576000, episode_reward=-188.43 +/- 1.6616058672751208 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  188.43  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 576000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.4378   | \n",
      " |    critic_loss     | 0.8595   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 575899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=577000, episode_reward=-188.19 +/- 1.6413834110934373 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  188.19  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 577000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.7179   | \n",
      " |    critic_loss     | 0.4295   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 576899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=578000, episode_reward=-188.98 +/- 1.3248998733783002 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  188.98  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 578000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.4604   | \n",
      " |    critic_loss     | 0.8651   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 577899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=579000, episode_reward=-189.77 +/- 1.2878750041594111 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  189.77  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 579000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.5975   | \n",
      " |    critic_loss     | 1.3994   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 578899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=580000, episode_reward=-189.94 +/- 1.3902392848256748 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  189.94  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 580000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.8263   | \n",
      " |    critic_loss     | 0.7066   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 579899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=581000, episode_reward=-190.0 +/- 1.2499133699719893 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   190.0  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 581000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.2405   | \n",
      " |    critic_loss     | 1.0601   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 580899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=582000, episode_reward=-190.63 +/- 1.7694229471502438 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  190.63  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 582000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.6774   | \n",
      " |    critic_loss     | 0.4194   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 581899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=583000, episode_reward=-191.72 +/- 1.5600076431237735 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  191.72  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 583000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.6562   | \n",
      " |    critic_loss     | 1.4140   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 582899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=584000, episode_reward=-192.19 +/- 1.0518490694060891 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  192.19  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 584000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.2585   | \n",
      " |    critic_loss     | 1.3146   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 583899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=585000, episode_reward=-192.38 +/- 1.978171628025365 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  192.38  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 585000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.5249   | \n",
      " |    critic_loss     | 0.8812   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 584899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=586000, episode_reward=-193.15 +/- 1.0755162830435125 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  193.15  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 586000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.5057   | \n",
      " |    critic_loss     | 1.1264   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 585899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=587000, episode_reward=-193.81 +/- 1.4770321339690626 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  193.81  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 587000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.9993   | \n",
      " |    critic_loss     | 0.9998   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 586899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=588000, episode_reward=-193.69 +/- 1.2368047385181504 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  193.69  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 588000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.2167   | \n",
      " |    critic_loss     | 0.5542   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 587899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=589000, episode_reward=-194.58 +/- 1.8111761660077252 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  194.58  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 589000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.5292   | \n",
      " |    critic_loss     | 1.1323   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 588899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=590000, episode_reward=-195.33 +/- 1.224885419604601 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  195.33  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 590000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.2724   | \n",
      " |    critic_loss     | 1.3181   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 589899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=591000, episode_reward=-195.99 +/- 1.7988983530798373 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  195.99  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 591000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.4688   | \n",
      " |    critic_loss     | 0.8672   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 590899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=592000, episode_reward=-196.15 +/- 1.4372736057870075 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  196.15  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 592000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.3834   | \n",
      " |    critic_loss     | 0.5958   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 591899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=593000, episode_reward=-196.92 +/- 1.5566246075119108 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  196.92  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 593000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.5288   | \n",
      " |    critic_loss     | 0.6322   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 592899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=594000, episode_reward=-197.26 +/- 1.8185822420746296 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  197.26  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 594000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.8626   | \n",
      " |    critic_loss     | 0.4657   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 593899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=595000, episode_reward=-197.92 +/- 1.219709253530716 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  197.92  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 595000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.7600   | \n",
      " |    critic_loss     | 0.4400   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 594899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=596000, episode_reward=-197.97 +/- 1.5986026463477203 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  197.97  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 596000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.7620   | \n",
      " |    critic_loss     | 0.9405   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 595899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=597000, episode_reward=-198.29 +/- 1.0095392403065087 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  198.29  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 597000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.5401   | \n",
      " |    critic_loss     | 0.6350   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 596899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=598000, episode_reward=-198.98 +/- 1.7181626353569133 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  198.98  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 598000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.1228   | \n",
      " |    critic_loss     | 1.2807   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 597899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=599000, episode_reward=-199.01 +/- 1.8524321713067082 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  199.01  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 599000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.7823   | \n",
      " |    critic_loss     | 0.4456   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 598899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=600000, episode_reward=-200.22 +/- 1.5237265721748225 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  200.22  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 600000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.0124   | \n",
      " |    critic_loss     | 0.5031   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 599899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=601000, episode_reward=-200.1 +/- 1.4530594898983686 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   200.1  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 601000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.8346   | \n",
      " |    critic_loss     | 0.9587   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 600899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=602000, episode_reward=-201.05 +/- 1.089202784795925 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  201.05  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 602000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.2826   | \n",
      " |    critic_loss     | 1.3206   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 601899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=603000, episode_reward=-201.44 +/- 1.8234168267192816 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  201.44  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 603000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.6395   | \n",
      " |    critic_loss     | 0.9099   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 602899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=604000, episode_reward=-202.49 +/- 1.6368237861047499 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  202.49  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 604000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.9465   | \n",
      " |    critic_loss     | 0.4866   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 603899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=605000, episode_reward=-202.21 +/- 1.0307667950898858 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  202.21  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 605000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.6195   | \n",
      " |    critic_loss     | 0.9049   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 604899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=606000, episode_reward=-203.29 +/- 1.9640929387848367 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  203.29  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 606000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.3514   | \n",
      " |    critic_loss     | 0.8378   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 605899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=607000, episode_reward=-203.66 +/- 1.945981664390119 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  203.66  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 607000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.7530   | \n",
      " |    critic_loss     | 0.6882   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 606899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=608000, episode_reward=-203.77 +/- 1.859405238990365 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  203.77  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 608000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.2409   | \n",
      " |    critic_loss     | 1.3102   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 607899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=609000, episode_reward=-204.1 +/- 1.4158650052755957 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   204.1  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 609000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.7983   | \n",
      " |    critic_loss     | 0.6996   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 608899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=610000, episode_reward=-205.48 +/- 1.122687132127341 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  205.48  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 610000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.8991   | \n",
      " |    critic_loss     | 1.2248   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 609899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=611000, episode_reward=-205.41 +/- 1.9078606951695032 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  205.41  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 611000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.5549   | \n",
      " |    critic_loss     | 1.3887   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 610899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=612000, episode_reward=-205.9 +/- 1.3274036816679984 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   205.9  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 612000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.3988   | \n",
      " |    critic_loss     | 1.3497   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 611899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=613000, episode_reward=-206.1 +/- 1.4390635729140389 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   206.1  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 613000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.7860   | \n",
      " |    critic_loss     | 0.4465   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 612899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=614000, episode_reward=-206.86 +/- 1.8117942285294164 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  206.86  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 614000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.3984   | \n",
      " |    critic_loss     | 1.0996   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 613899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=615000, episode_reward=-207.45 +/- 1.1085335761964827 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  207.45  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 615000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.9190   | \n",
      " |    critic_loss     | 0.4797   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 614899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=616000, episode_reward=-208.07 +/- 1.3195076203557965 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  208.07  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 616000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.3562   | \n",
      " |    critic_loss     | 1.3391   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 615899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=617000, episode_reward=-208.02 +/- 1.7251289280031086 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  208.02  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 617000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.3635   | \n",
      " |    critic_loss     | 0.8409   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 616899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=618000, episode_reward=-208.56 +/- 1.965897978650959 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  208.56  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 618000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.9768   | \n",
      " |    critic_loss     | 0.4942   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 617899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=619000, episode_reward=-209.39 +/- 1.4012021801040437 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  209.39  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 619000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.5424   | \n",
      " |    critic_loss     | 0.6356   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 618899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=620000, episode_reward=-209.84 +/- 1.7710602654493321 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  209.84  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 620000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.4332   | \n",
      " |    critic_loss     | 1.1083   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 619899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=621000, episode_reward=-210.78 +/- 1.8597098989398013 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  210.78  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 621000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.9674   | \n",
      " |    critic_loss     | 1.2419   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 620899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=622000, episode_reward=-211.29 +/- 1.2202251825293442 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  211.29  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 622000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.6831   | \n",
      " |    critic_loss     | 0.9208   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 621899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=623000, episode_reward=-211.33 +/- 1.9874973345584988 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  211.33  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 623000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.8749   | \n",
      " |    critic_loss     | 0.9687   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 622899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=624000, episode_reward=-212.14 +/- 1.0756895647741915 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  212.14  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 624000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.9297   | \n",
      " |    critic_loss     | 0.7324   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 623899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=625000, episode_reward=-212.63 +/- 1.6515183069816786 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  212.63  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 625000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.5342   | \n",
      " |    critic_loss     | 1.1335   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 624899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=626000, episode_reward=-212.68 +/- 1.984339656283848 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  212.68  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 626000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.5344   | \n",
      " |    critic_loss     | 0.8836   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 625899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=627000, episode_reward=-213.75 +/- 1.2683343148700483 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  213.75  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 627000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.9136   | \n",
      " |    critic_loss     | 0.9784   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 626899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=628000, episode_reward=-214.11 +/- 1.793114987397936 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  214.11  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 628000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.3197   | \n",
      " |    critic_loss     | 0.8299   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 627899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=629000, episode_reward=-214.87 +/- 1.4827906050712616 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  214.87  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 629000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.6359   | \n",
      " |    critic_loss     | 0.4090   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 628899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=630000, episode_reward=-214.92 +/- 1.433735526566688 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  214.92  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 630000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.9866   | \n",
      " |    critic_loss     | 0.7467   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 629899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=631000, episode_reward=-215.24 +/- 1.5958930974672436 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  215.24  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 631000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.2254   | \n",
      " |    critic_loss     | 0.5564   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 630899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=632000, episode_reward=-215.78 +/- 1.9692279023567878 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  215.78  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 632000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.9931   | \n",
      " |    critic_loss     | 0.9983   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 631899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=633000, episode_reward=-216.68 +/- 1.1829237339091827 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  216.68  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 633000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.2615   | \n",
      " |    critic_loss     | 1.0654   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 632899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=634000, episode_reward=-216.62 +/- 1.9160656824065265 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  216.62  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 634000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.2792   | \n",
      " |    critic_loss     | 1.3198   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 633899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=635000, episode_reward=-217.82 +/- 1.588905256437014 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  217.82  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 635000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.0573   | \n",
      " |    critic_loss     | 1.0143   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 634899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=636000, episode_reward=-217.97 +/- 1.2301226384439548 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  217.97  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 636000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.8661   | \n",
      " |    critic_loss     | 1.2165   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 635899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=637000, episode_reward=-218.72 +/- 1.2764347690251878 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  218.72  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 637000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.7014   | \n",
      " |    critic_loss     | 1.1753   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 636899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=638000, episode_reward=-218.62 +/- 1.113849800206069 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  218.62  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 638000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.2758   | \n",
      " |    critic_loss     | 0.5689   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 637899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=639000, episode_reward=-219.3 +/- 1.615878040211397 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   219.3  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 639000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.5134   | \n",
      " |    critic_loss     | 1.1283   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 638899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=640000, episode_reward=-220.02 +/- 1.9014360315910204 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  220.02  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 640000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.9957   | \n",
      " |    critic_loss     | 0.4989   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 639899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=641000, episode_reward=-220.76 +/- 1.970521012243219 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  220.76  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 641000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.1098   | \n",
      " |    critic_loss     | 0.7774   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 640899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=642000, episode_reward=-221.34 +/- 1.2266948872653316 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  221.34  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 642000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.7723   | \n",
      " |    critic_loss     | 0.6931   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 641899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=643000, episode_reward=-221.7 +/- 1.754365089281308 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   221.7  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 643000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.7996   | \n",
      " |    critic_loss     | 0.9499   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 642899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=644000, episode_reward=-221.72 +/- 1.5052312261828977 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  221.72  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 644000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.6304   | \n",
      " |    critic_loss     | 1.1576   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 643899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=645000, episode_reward=-223.0 +/- 1.8944309103751533 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   223.0  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 645000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.6273   | \n",
      " |    critic_loss     | 1.1568   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 644899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=646000, episode_reward=-223.08 +/- 1.5340431418624147 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  223.08  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 646000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.0665   | \n",
      " |    critic_loss     | 0.5166   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 645899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=647000, episode_reward=-223.87 +/- 1.8984488245609386 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  223.87  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 647000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.7807   | \n",
      " |    critic_loss     | 1.1952   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 646899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=648000, episode_reward=-223.69 +/- 1.8109170687818255 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  223.69  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 648000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.8150   | \n",
      " |    critic_loss     | 0.4538   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 647899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=649000, episode_reward=-224.02 +/- 1.4248818526937697 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  224.02  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 649000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.8365   | \n",
      " |    critic_loss     | 0.9591   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 648899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=650000, episode_reward=-225.11 +/- 1.9489732325816096 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  225.11  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 650000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.9201   | \n",
      " |    critic_loss     | 1.2300   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 649899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=651000, episode_reward=-225.4 +/- 1.2007474756435248 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   225.4  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 651000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.5148   | \n",
      " |    critic_loss     | 0.8787   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 650899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=652000, episode_reward=-226.43 +/- 1.3889112750501087 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  226.43  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 652000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.9581   | \n",
      " |    critic_loss     | 0.7395   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 651899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=653000, episode_reward=-226.26 +/- 1.9455037677488902 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  226.26  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 653000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.9702   | \n",
      " |    critic_loss     | 0.9926   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 652899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=654000, episode_reward=-226.98 +/- 1.8853571700086678 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  226.98  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 654000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.0222   | \n",
      " |    critic_loss     | 1.0055   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 653899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=655000, episode_reward=-227.47 +/- 1.7579776651969419 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  227.47  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 655000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.5818   | \n",
      " |    critic_loss     | 0.8955   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 654899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=656000, episode_reward=-227.56 +/- 1.7174366135720083 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  227.56  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 656000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.7987   | \n",
      " |    critic_loss     | 0.4497   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 655899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=657000, episode_reward=-228.95 +/- 1.0063424143770627 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  228.95  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 657000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.1349   | \n",
      " |    critic_loss     | 0.5337   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 656899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=658000, episode_reward=-228.52 +/- 1.838515531120434 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  228.52  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 658000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.3599   | \n",
      " |    critic_loss     | 0.8400   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 657899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=659000, episode_reward=-229.92 +/- 1.644897851791851 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  229.92  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 659000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.9028   | \n",
      " |    critic_loss     | 0.4757   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 658899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=660000, episode_reward=-230.17 +/- 1.7176919942313043 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  230.17  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 660000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.8953   | \n",
      " |    critic_loss     | 0.7238   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 659899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=661000, episode_reward=-230.71 +/- 1.176821567913459 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  230.71  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 661000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.5644   | \n",
      " |    critic_loss     | 1.1411   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 660899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=662000, episode_reward=-231.31 +/- 1.5340435012640403 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  231.31  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 662000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.1602   | \n",
      " |    critic_loss     | 0.7900   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 661899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=663000, episode_reward=-231.42 +/- 1.6548912126619995 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  231.42  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 663000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.5056   | \n",
      " |    critic_loss     | 0.8764   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 662899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=664000, episode_reward=-231.88 +/- 1.1792125240069564 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  231.88  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 664000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.3961   | \n",
      " |    critic_loss     | 0.3490   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 663899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=665000, episode_reward=-232.01 +/- 1.8806588227619767 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  232.01  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 665000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.8850   | \n",
      " |    critic_loss     | 1.2212   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 664899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=666000, episode_reward=-232.57 +/- 1.560654861263826 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  232.57  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 666000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.1501   | \n",
      " |    critic_loss     | 0.7875   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 665899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=667000, episode_reward=-233.12 +/- 1.8631609216668041 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  233.12  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 667000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.5527   | \n",
      " |    critic_loss     | 1.1382   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 666899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=668000, episode_reward=-234.37 +/- 1.159555450263714 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  234.37  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 668000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.8627   | \n",
      " |    critic_loss     | 0.7157   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 667899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=669000, episode_reward=-234.02 +/- 1.3174456047078127 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  234.02  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 669000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.9626   | \n",
      " |    critic_loss     | 0.7406   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 668899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=670000, episode_reward=-235.22 +/- 1.5019814832209453 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  235.22  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 670000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.1150   | \n",
      " |    critic_loss     | 0.5287   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 669899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=671000, episode_reward=-235.22 +/- 1.6560674387945011 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  235.22  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 671000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.1091   | \n",
      " |    critic_loss     | 0.5273   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 670899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=672000, episode_reward=-235.58 +/- 1.6524380029702739 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  235.58  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 672000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.2287   | \n",
      " |    critic_loss     | 1.0572   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 671899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=673000, episode_reward=-236.67 +/- 1.1887517643832353 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  236.67  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 673000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.8949   | \n",
      " |    critic_loss     | 0.9737   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 672899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=674000, episode_reward=-236.98 +/- 1.365627505045578 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  236.98  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 674000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.0603   | \n",
      " |    critic_loss     | 0.5151   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 673899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=675000, episode_reward=-237.71 +/- 1.1360516909046166 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  237.71  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 675000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.5128   | \n",
      " |    critic_loss     | 0.8782   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 674899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=676000, episode_reward=-237.79 +/- 1.8595486210068706 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  237.79  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 676000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.1305   | \n",
      " |    critic_loss     | 1.0326   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 675899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=677000, episode_reward=-238.13 +/- 1.0129025367519162 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  238.13  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 677000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.8096   | \n",
      " |    critic_loss     | 0.7024   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 676899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=678000, episode_reward=-239.06 +/- 1.0949078478216907 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  239.06  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 678000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.8844   | \n",
      " |    critic_loss     | 0.9711   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 677899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=679000, episode_reward=-239.77 +/- 1.0615600958746525 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  239.77  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 679000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.5571   | \n",
      " |    critic_loss     | 0.3893   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 678899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=680000, episode_reward=-239.92 +/- 1.2577689035224915 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  239.92  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 680000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.1233   | \n",
      " |    critic_loss     | 1.2808   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 679899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=681000, episode_reward=-240.5 +/- 1.6966183655135567 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   240.5  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 681000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.0217   | \n",
      " |    critic_loss     | 0.5054   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 680899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=682000, episode_reward=-240.91 +/- 1.3456344723694906 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  240.91  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 682000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.5346   | \n",
      " |    critic_loss     | 0.6336   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 681899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=683000, episode_reward=-241.83 +/- 1.1464508634563968 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  241.83  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 683000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.9957   | \n",
      " |    critic_loss     | 0.4989   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 682899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=684000, episode_reward=-242.28 +/- 1.0950732908950604 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  242.28  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 684000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.8600   | \n",
      " |    critic_loss     | 0.4650   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 683899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=685000, episode_reward=-242.45 +/- 1.2664284915910837 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  242.45  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 685000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.3238   | \n",
      " |    critic_loss     | 1.0810   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 684899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=686000, episode_reward=-243.34 +/- 1.0048303934190495 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  243.34  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 686000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.3546   | \n",
      " |    critic_loss     | 0.5886   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 685899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=687000, episode_reward=-243.21 +/- 1.131758733709522 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  243.21  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 687000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.5545   | \n",
      " |    critic_loss     | 0.3886   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 686899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=688000, episode_reward=-243.67 +/- 1.1462234063991485 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  243.67  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 688000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.2721   | \n",
      " |    critic_loss     | 1.0680   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 687899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=689000, episode_reward=-244.97 +/- 1.7502456601631842 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  244.97  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 689000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.7843   | \n",
      " |    critic_loss     | 1.1961   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 688899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=690000, episode_reward=-245.23 +/- 1.6612400914414933 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  245.23  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 690000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.8870   | \n",
      " |    critic_loss     | 0.9718   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 689899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=691000, episode_reward=-245.3 +/- 1.282706869241462 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   245.3  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 691000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.0501   | \n",
      " |    critic_loss     | 0.5125   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 690899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=692000, episode_reward=-245.9 +/- 1.3534999601576276 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   245.9  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 692000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.4971   | \n",
      " |    critic_loss     | 1.1243   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 691899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=693000, episode_reward=-246.25 +/- 1.794817107926307 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  246.25  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 693000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.5478   | \n",
      " |    critic_loss     | 0.6370   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 692899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=694000, episode_reward=-247.15 +/- 1.3954256254214825 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  247.15  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 694000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.8575   | \n",
      " |    critic_loss     | 0.9644   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 693899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=695000, episode_reward=-247.28 +/- 1.4817490385602339 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  247.28  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 695000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.5371   | \n",
      " |    critic_loss     | 0.3843   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 694899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=696000, episode_reward=-248.11 +/- 1.4941683461718065 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  248.11  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 696000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.2930   | \n",
      " |    critic_loss     | 1.0733   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 695899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=697000, episode_reward=-248.15 +/- 1.8887931452643991 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  248.15  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 697000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.3579   | \n",
      " |    critic_loss     | 0.3395   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 696899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=698000, episode_reward=-248.87 +/- 1.14221762594842 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  248.87  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 698000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.4152   | \n",
      " |    critic_loss     | 0.8538   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 697899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=699000, episode_reward=-249.54 +/- 1.0819031172089488 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  249.54  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 699000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.3222   | \n",
      " |    critic_loss     | 0.3305   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 698899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=700000, episode_reward=-250.06 +/- 1.9757956916472497 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  250.06  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 700000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.9645   | \n",
      " |    critic_loss     | 0.7411   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 699899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=701000, episode_reward=-250.87 +/- 1.851566566343627 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  250.87  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 701000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.9757   | \n",
      " |    critic_loss     | 0.4939   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 700899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=702000, episode_reward=-251.28 +/- 1.4399245267131782 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  251.28  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 702000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.5365   | \n",
      " |    critic_loss     | 0.6341   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 701899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=703000, episode_reward=-251.97 +/- 1.090058457679357 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  251.97  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 703000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.8502   | \n",
      " |    critic_loss     | 0.9625   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 702899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=704000, episode_reward=-252.49 +/- 1.3791867665234105 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  252.49  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 704000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.2739   | \n",
      " |    critic_loss     | 0.5685   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 703899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=705000, episode_reward=-252.97 +/- 1.1477994162248484 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  252.97  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 705000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.4418   | \n",
      " |    critic_loss     | 1.1104   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 704899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=706000, episode_reward=-253.44 +/- 1.7210661917507153 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  253.44  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 706000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.6969   | \n",
      " |    critic_loss     | 0.4242   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 705899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=707000, episode_reward=-253.79 +/- 1.4249195717178171 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  253.79  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 707000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.5302   | \n",
      " |    critic_loss     | 0.8826   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 706899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=708000, episode_reward=-253.96 +/- 1.6825176657607597 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  253.96  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 708000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.5375   | \n",
      " |    critic_loss     | 0.3844   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 707899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=709000, episode_reward=-254.32 +/- 1.726264721088569 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  254.32  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 709000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.1343   | \n",
      " |    critic_loss     | 1.0336   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 708899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=710000, episode_reward=-254.84 +/- 1.8424454311497758 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  254.84  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 710000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.2208   | \n",
      " |    critic_loss     | 1.0552   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 709899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=711000, episode_reward=-255.32 +/- 1.100417647060151 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  255.32  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 711000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.6125   | \n",
      " |    critic_loss     | 0.6531   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 710899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=712000, episode_reward=-255.82 +/- 1.0394528333378963 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  255.82  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 712000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.5117   | \n",
      " |    critic_loss     | 0.8779   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 711899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=713000, episode_reward=-256.08 +/- 1.3200991729298508 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  256.08  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 713000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.1044   | \n",
      " |    critic_loss     | 0.5261   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 712899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=714000, episode_reward=-256.51 +/- 1.9142133007137154 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  256.51  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 714000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.7393   | \n",
      " |    critic_loss     | 0.9348   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 713899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=715000, episode_reward=-257.33 +/- 1.7356205124878408 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  257.33  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 715000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.2655   | \n",
      " |    critic_loss     | 0.5664   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 714899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=716000, episode_reward=-257.86 +/- 1.0455950770324838 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  257.86  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 716000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.7716   | \n",
      " |    critic_loss     | 0.6929   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 715899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=717000, episode_reward=-258.02 +/- 1.5632924959881096 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  258.02  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 717000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.0674   | \n",
      " |    critic_loss     | 0.5168   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 716899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=718000, episode_reward=-258.94 +/- 1.8046222784415278 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  258.94  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 718000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.6387   | \n",
      " |    critic_loss     | 0.9097   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 717899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=719000, episode_reward=-259.98 +/- 1.2872098009212078 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  259.98  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 719000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.2609   | \n",
      " |    critic_loss     | 0.3152   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 718899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=720000, episode_reward=-260.49 +/- 1.1093206795918595 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  260.49  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 720000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.8265   | \n",
      " |    critic_loss     | 0.4566   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 719899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=721000, episode_reward=-260.66 +/- 1.0426937927237954 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  260.66  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 721000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.1067   | \n",
      " |    critic_loss     | 1.0267   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 720899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=722000, episode_reward=-260.53 +/- 1.2797552960803804 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  260.53  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 722000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.8240   | \n",
      " |    critic_loss     | 0.9560   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 721899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=723000, episode_reward=-261.39 +/- 1.3120947460324803 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  261.39  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 723000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.2162   | \n",
      " |    critic_loss     | 0.3040   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 722899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=724000, episode_reward=-262.16 +/- 1.1637468897628946 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  262.16  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 724000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.1802   | \n",
      " |    critic_loss     | 1.0450   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 723899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=725000, episode_reward=-262.52 +/- 1.367188312007216 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  262.52  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 725000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.7414   | \n",
      " |    critic_loss     | 0.6854   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 724899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=726000, episode_reward=-263.48 +/- 1.2858535728196814 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  263.48  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 726000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.1549   | \n",
      " |    critic_loss     | 0.2887   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 725899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=727000, episode_reward=-263.19 +/- 1.9559364157283026 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  263.19  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 727000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.0129   | \n",
      " |    critic_loss     | 1.2532   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 726899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=728000, episode_reward=-263.94 +/- 1.4482472935503803 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  263.94  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 728000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.2328   | \n",
      " |    critic_loss     | 1.0582   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 727899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=729000, episode_reward=-264.28 +/- 1.9338043309971575 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  264.28  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 729000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.1938   | \n",
      " |    critic_loss     | 0.2985   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 728899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=730000, episode_reward=-264.97 +/- 1.494001960522674 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  264.97  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 730000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.2132   | \n",
      " |    critic_loss     | 0.8033   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 729899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=731000, episode_reward=-265.37 +/- 1.2112657184817814 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  265.37  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 731000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.1635   | \n",
      " |    critic_loss     | 0.2909   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 730899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=732000, episode_reward=-266.38 +/- 1.6340344217160854 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  266.38  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 732000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.4357   | \n",
      " |    critic_loss     | 0.6089   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 731899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=733000, episode_reward=-266.13 +/- 1.1384469230521732 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  266.13  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 733000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.0995   | \n",
      " |    critic_loss     | 0.5249   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 732899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=734000, episode_reward=-266.53 +/- 1.5412091274977902 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  266.53  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 734000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.0663   | \n",
      " |    critic_loss     | 0.5166   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 733899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=735000, episode_reward=-267.91 +/- 1.103441166506058 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  267.91  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 735000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.3387   | \n",
      " |    critic_loss     | 0.5847   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 734899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=736000, episode_reward=-268.49 +/- 1.970342410919733 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  268.49  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 736000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.1933   | \n",
      " |    critic_loss     | 1.0483   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 735899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=737000, episode_reward=-268.66 +/- 1.9737519436099968 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  268.66  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 737000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.2343   | \n",
      " |    critic_loss     | 0.3086   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 736899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=738000, episode_reward=-268.77 +/- 1.7443198375781743 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  268.77  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 738000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.9921   | \n",
      " |    critic_loss     | 1.2480   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 737899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=739000, episode_reward=-269.61 +/- 1.5168068676510265 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  269.61  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 739000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.3717   | \n",
      " |    critic_loss     | 0.3429   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 738899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=740000, episode_reward=-269.71 +/- 1.2242970289652517 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  269.71  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 740000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.3984   | \n",
      " |    critic_loss     | 0.3496   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 739899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=741000, episode_reward=-270.42 +/- 1.8072676554969056 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  270.42  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 741000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.2748   | \n",
      " |    critic_loss     | 0.8187   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 740899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=742000, episode_reward=-270.7 +/- 1.5096591032909252 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   270.7  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 742000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.3512   | \n",
      " |    critic_loss     | 1.0878   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 741899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=743000, episode_reward=-271.99 +/- 1.874203539896575 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  271.99  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 743000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.5439   | \n",
      " |    critic_loss     | 1.1360   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 742899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=744000, episode_reward=-271.7 +/- 1.940353062809389 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   271.7  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 744000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.1069   | \n",
      " |    critic_loss     | 1.0267   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 743899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=745000, episode_reward=-272.11 +/- 1.5891826758452114 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  272.11  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 745000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.5792   | \n",
      " |    critic_loss     | 0.6448   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 744899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=746000, episode_reward=-273.24 +/- 1.9797601746659232 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  273.24  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 746000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.0531   | \n",
      " |    critic_loss     | 0.7633   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 745899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=747000, episode_reward=-273.93 +/- 1.8724073940843315 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  273.93  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 747000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.9927   | \n",
      " |    critic_loss     | 0.7482   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 746899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=748000, episode_reward=-274.22 +/- 1.6565552269001742 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  274.22  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 748000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.3124   | \n",
      " |    critic_loss     | 1.0781   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 747899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=749000, episode_reward=-274.29 +/- 1.9183257260397926 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  274.29  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 749000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.8504   | \n",
      " |    critic_loss     | 0.7126   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 748899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=750000, episode_reward=-275.26 +/- 1.941436244904558 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  275.26  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 750000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.9036   | \n",
      " |    critic_loss     | 0.4759   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 749899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=751000, episode_reward=-275.85 +/- 1.0747735615660723 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  275.85  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 751000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.8963   | \n",
      " |    critic_loss     | 1.2241   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 750899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=752000, episode_reward=-276.44 +/- 1.0327723022002369 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  276.44  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 752000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.0157   | \n",
      " |    critic_loss     | 0.2539   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 751899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=753000, episode_reward=-276.21 +/- 1.8510548731959369 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  276.21  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 753000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.2669   | \n",
      " |    critic_loss     | 1.0667   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 752899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=754000, episode_reward=-277.13 +/- 1.3023302769733216 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  277.13  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 754000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.6519   | \n",
      " |    critic_loss     | 0.4130   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 753899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=755000, episode_reward=-277.85 +/- 1.1289763682494927 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  277.85  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 755000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.5630   | \n",
      " |    critic_loss     | 1.1407   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 754899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=756000, episode_reward=-277.91 +/- 1.3928734657704136 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  277.91  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 756000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.3886   | \n",
      " |    critic_loss     | 1.0972   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 755899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=757000, episode_reward=-278.89 +/- 1.2871338180125969 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  278.89  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 757000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.5453   | \n",
      " |    critic_loss     | 0.8863   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 756899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=758000, episode_reward=-278.96 +/- 1.315984887585099 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  278.96  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 758000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.0126   | \n",
      " |    critic_loss     | 0.7531   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 757899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=759000, episode_reward=-279.12 +/- 1.2825434095955892 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  279.12  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 759000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.5500   | \n",
      " |    critic_loss     | 1.1375   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 758899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=760000, episode_reward=-280.11 +/- 1.389612014886389 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  280.11  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 760000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.9263   | \n",
      " |    critic_loss     | 0.9816   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 759899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=761000, episode_reward=-280.69 +/- 1.4911783229857334 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  280.69  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 761000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.4020   | \n",
      " |    critic_loss     | 0.3505   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 760899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=762000, episode_reward=-280.92 +/- 1.399393768030973 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  280.92  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 762000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.0496   | \n",
      " |    critic_loss     | 0.2624   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 761899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=763000, episode_reward=-281.77 +/- 1.4888605715891587 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  281.77  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 763000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.5845   | \n",
      " |    critic_loss     | 0.6461   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 762899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=764000, episode_reward=-282.09 +/- 1.8615161325804745 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  282.09  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 764000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.5331   | \n",
      " |    critic_loss     | 0.3833   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 763899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=765000, episode_reward=-282.71 +/- 1.855281439265097 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  282.71  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 765000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.1779   | \n",
      " |    critic_loss     | 1.0445   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 764899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=766000, episode_reward=-282.6 +/- 1.275388008328476 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   282.6  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 766000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.3140   | \n",
      " |    critic_loss     | 1.0785   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 765899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=767000, episode_reward=-283.64 +/- 1.533617326639625 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  283.64  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 767000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.9932   | \n",
      " |    critic_loss     | 0.4983   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 766899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=768000, episode_reward=-283.62 +/- 1.386939984659527 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  283.62  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 768000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.4781   | \n",
      " |    critic_loss     | 0.3695   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 767899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=769000, episode_reward=-284.64 +/- 1.3372656221531538 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  284.64  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 769000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.1208   | \n",
      " |    critic_loss     | 0.5302   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 768899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=770000, episode_reward=-285.0 +/- 1.1252092605263386 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   285.0  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 770000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.3511   | \n",
      " |    critic_loss     | 0.5878   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 769899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=771000, episode_reward=-285.51 +/- 1.7163428881983558 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  285.51  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 771000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.6648   | \n",
      " |    critic_loss     | 0.4162   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 770899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=772000, episode_reward=-285.68 +/- 1.544564336302656 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  285.68  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 772000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.3154   | \n",
      " |    critic_loss     | 0.5789   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 771899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=773000, episode_reward=-286.55 +/- 1.8895586007022749 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  286.55  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 773000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.6069   | \n",
      " |    critic_loss     | 0.6517   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 772899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=774000, episode_reward=-287.07 +/- 1.0155118659968347 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  287.07  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 774000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.9813   | \n",
      " |    critic_loss     | 0.7453   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 773899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=775000, episode_reward=-287.13 +/- 1.1030467830030428 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  287.13  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 775000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.9449   | \n",
      " |    critic_loss     | 0.7362   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 774899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=776000, episode_reward=-287.83 +/- 1.457479792885835 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  287.83  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 776000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.6391   | \n",
      " |    critic_loss     | 0.9098   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 775899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=777000, episode_reward=-288.67 +/- 1.8905901754321346 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  288.67  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 777000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 0.9192   | \n",
      " |    critic_loss     | 0.2298   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 776899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=778000, episode_reward=-288.79 +/- 1.4098532289402033 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  288.79  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 778000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 0.9482   | \n",
      " |    critic_loss     | 0.2370   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 777899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=779000, episode_reward=-289.22 +/- 1.4006723846382099 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  289.22  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 779000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.7579   | \n",
      " |    critic_loss     | 1.1895   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 778899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=780000, episode_reward=-289.76 +/- 1.0188795768395316 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  289.76  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 780000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 0.9153   | \n",
      " |    critic_loss     | 0.2288   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 779899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=781000, episode_reward=-290.91 +/- 1.1899096185405806 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  290.91  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 781000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.5249   | \n",
      " |    critic_loss     | 0.6312   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 780899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=782000, episode_reward=-290.69 +/- 1.519616177263428 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  290.69  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 782000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.3620   | \n",
      " |    critic_loss     | 0.3405   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 781899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=783000, episode_reward=-291.9 +/- 1.6566891709238738 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   291.9  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 783000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.9112   | \n",
      " |    critic_loss     | 0.9778   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 782899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=784000, episode_reward=-291.85 +/- 1.6713289519343255 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  291.85  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 784000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.4730   | \n",
      " |    critic_loss     | 0.8682   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 783899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=785000, episode_reward=-292.77 +/- 1.6366506158085432 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  292.77  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 785000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.5704   | \n",
      " |    critic_loss     | 0.3926   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 784899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=786000, episode_reward=-292.64 +/- 1.2855535988328188 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  292.64  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 786000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.5820   | \n",
      " |    critic_loss     | 0.3955   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 785899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=787000, episode_reward=-293.43 +/- 1.3811760365707135 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  293.43  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 787000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.0098   | \n",
      " |    critic_loss     | 1.0024   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 786899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=788000, episode_reward=-293.92 +/- 1.7862965118880845 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  293.92  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 788000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.8688   | \n",
      " |    critic_loss     | 0.7172   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 787899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=789000, episode_reward=-294.79 +/- 1.2150738093398206 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  294.79  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 789000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.0176   | \n",
      " |    critic_loss     | 1.0044   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 788899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=790000, episode_reward=-294.61 +/- 1.3168866062350102 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  294.61  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 790000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.0364   | \n",
      " |    critic_loss     | 0.5091   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 789899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=791000, episode_reward=-295.24 +/- 1.2833069214228854 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  295.24  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 791000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.4841   | \n",
      " |    critic_loss     | 0.6210   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 790899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=792000, episode_reward=-295.56 +/- 1.6552261370476842 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  295.56  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 792000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.0845   | \n",
      " |    critic_loss     | 0.7711   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 791899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=793000, episode_reward=-296.85 +/- 1.6181928839095898 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  296.85  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 793000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.7825   | \n",
      " |    critic_loss     | 1.1956   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 792899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=794000, episode_reward=-297.27 +/- 1.7352637282295766 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  297.27  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 794000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.2908   | \n",
      " |    critic_loss     | 0.3227   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 793899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=795000, episode_reward=-297.73 +/- 1.2584419562522602 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  297.73  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 795000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.2220   | \n",
      " |    critic_loss     | 0.5555   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 794899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=796000, episode_reward=-297.73 +/- 1.7366455005429187 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  297.73  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 796000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.1884   | \n",
      " |    critic_loss     | 0.2971   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 795899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=797000, episode_reward=-298.78 +/- 1.3935814747608624 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  298.78  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 797000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.1145   | \n",
      " |    critic_loss     | 0.7786   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 796899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=798000, episode_reward=-298.84 +/- 1.0561675155873278 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  298.84  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 798000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.9739   | \n",
      " |    critic_loss     | 0.9935   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 797899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=799000, episode_reward=-299.73 +/- 1.0470259568624551 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  299.73  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 799000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.9627   | \n",
      " |    critic_loss     | 0.4907   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 798899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=800000, episode_reward=-300.41 +/- 1.8727784821971165 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  300.41  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 800000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.5125   | \n",
      " |    critic_loss     | 0.6281   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 799899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=801000, episode_reward=-300.98 +/- 1.8611414651266132 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  300.98  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 801000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.5751   | \n",
      " |    critic_loss     | 0.3938   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 800899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=802000, episode_reward=-300.64 +/- 1.0977873130762164 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  300.64  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 802000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.9256   | \n",
      " |    critic_loss     | 0.9814   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 801899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=803000, episode_reward=-301.05 +/- 1.8458968608570259 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  301.05  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 803000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.1348   | \n",
      " |    critic_loss     | 0.5337   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 802899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=804000, episode_reward=-302.17 +/- 1.4820506931386856 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  302.17  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 804000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.4477   | \n",
      " |    critic_loss     | 0.6119   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 803899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=805000, episode_reward=-302.61 +/- 1.5618807880957495 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  302.61  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 805000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.0550   | \n",
      " |    critic_loss     | 0.7637   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 804899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=806000, episode_reward=-303.42 +/- 1.5337956646501478 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  303.42  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 806000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.2332   | \n",
      " |    critic_loss     | 1.0583   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 805899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=807000, episode_reward=-303.67 +/- 1.2003723014524161 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  303.67  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 807000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.4031   | \n",
      " |    critic_loss     | 0.8508   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 806899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=808000, episode_reward=-303.99 +/- 1.3839395165491986 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  303.99  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 808000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.0446   | \n",
      " |    critic_loss     | 0.2612   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 807899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=809000, episode_reward=-304.93 +/- 1.7007036259204833 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  304.93  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 809000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.5322   | \n",
      " |    critic_loss     | 1.1330   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 808899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=810000, episode_reward=-305.35 +/- 1.0359331959377607 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  305.35  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 810000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.7133   | \n",
      " |    critic_loss     | 0.9283   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 809899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=811000, episode_reward=-305.36 +/- 1.4683172391835164 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  305.36  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 811000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.3801   | \n",
      " |    critic_loss     | 1.0950   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 810899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=812000, episode_reward=-306.26 +/- 1.497521349143442 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  306.26  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 812000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.1064   | \n",
      " |    critic_loss     | 0.5266   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 811899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=813000, episode_reward=-306.54 +/- 1.2437073837474901 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  306.54  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 813000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.4011   | \n",
      " |    critic_loss     | 1.1003   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 812899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=814000, episode_reward=-307.32 +/- 1.9281629155669617 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  307.32  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 814000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.8717   | \n",
      " |    critic_loss     | 0.4679   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 813899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=815000, episode_reward=-307.75 +/- 1.3259596114240333 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  307.75  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 815000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.4531   | \n",
      " |    critic_loss     | 0.3633   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 814899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=816000, episode_reward=-308.04 +/- 1.6578012945956684 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  308.04  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 816000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.7435   | \n",
      " |    critic_loss     | 0.6859   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 815899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=817000, episode_reward=-308.84 +/- 1.3145171812977487 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  308.84  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 817000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.3537   | \n",
      " |    critic_loss     | 0.3384   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 816899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=818000, episode_reward=-308.96 +/- 1.00633971900149 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  308.96  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 818000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.3563   | \n",
      " |    critic_loss     | 0.8391   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 817899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=819000, episode_reward=-309.71 +/- 1.5157093541278783 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  309.71  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 819000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.4821   | \n",
      " |    critic_loss     | 1.1205   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 818899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=820000, episode_reward=-309.6 +/- 1.3913737069472236 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   309.6  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 820000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.1189   | \n",
      " |    critic_loss     | 0.5297   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 819899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=821000, episode_reward=-310.6 +/- 1.4793365274014687 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   310.6  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 821000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.6725   | \n",
      " |    critic_loss     | 0.9181   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 820899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=822000, episode_reward=-310.83 +/- 1.901556897361083 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  310.83  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 822000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.9640   | \n",
      " |    critic_loss     | 0.4910   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 821899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=823000, episode_reward=-311.8 +/- 1.9731115805354995 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   311.8  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 823000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.0737   | \n",
      " |    critic_loss     | 0.7684   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 822899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=824000, episode_reward=-312.19 +/- 1.297780882916117 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  312.19  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 824000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.1391   | \n",
      " |    critic_loss     | 0.7848   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 823899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=825000, episode_reward=-312.98 +/- 1.1912024491170443 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  312.98  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 825000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.1031   | \n",
      " |    critic_loss     | 0.2758   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 824899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=826000, episode_reward=-312.85 +/- 1.8846212954089174 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  312.85  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 826000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.5550   | \n",
      " |    critic_loss     | 1.1387   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 825899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=827000, episode_reward=-313.19 +/- 1.5115063997981375 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  313.19  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 827000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.8047   | \n",
      " |    critic_loss     | 0.9512   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 826899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=828000, episode_reward=-313.99 +/- 1.0192801713936206 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  313.99  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 828000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.0587   | \n",
      " |    critic_loss     | 0.2647   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 827899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=829000, episode_reward=-314.33 +/- 1.8835094468050908 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  314.33  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 829000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.1231   | \n",
      " |    critic_loss     | 0.2808   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 828899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=830000, episode_reward=-315.22 +/- 1.1085740448617427 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  315.22  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 830000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.3590   | \n",
      " |    critic_loss     | 1.0898   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 829899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=831000, episode_reward=-315.52 +/- 1.5404053460880989 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  315.52  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 831000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.5690   | \n",
      " |    critic_loss     | 0.3923   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 830899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=832000, episode_reward=-315.53 +/- 1.5099869142800038 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  315.53  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 832000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.2008   | \n",
      " |    critic_loss     | 0.5502   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 831899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=833000, episode_reward=-316.9 +/- 1.1936523296709098 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   316.9  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 833000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.1998   | \n",
      " |    critic_loss     | 0.2999   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 832899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=834000, episode_reward=-316.53 +/- 1.3707076234739086 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  316.53  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 834000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.1181   | \n",
      " |    critic_loss     | 0.7795   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 833899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=835000, episode_reward=-317.7 +/- 1.9615498241281693 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   317.7  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 835000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.6188   | \n",
      " |    critic_loss     | 1.1547   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 834899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=836000, episode_reward=-318.06 +/- 1.3522816787767966 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  318.06  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 836000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.2767   | \n",
      " |    critic_loss     | 0.5692   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 835899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=837000, episode_reward=-318.65 +/- 1.0041375555881558 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  318.65  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 837000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.6140   | \n",
      " |    critic_loss     | 1.1535   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 836899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=838000, episode_reward=-319.14 +/- 1.895124187127276 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  319.14  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 838000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.5037   | \n",
      " |    critic_loss     | 0.8759   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 837899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=839000, episode_reward=-319.83 +/- 1.344380435624483 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  319.83  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 839000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.0780   | \n",
      " |    critic_loss     | 0.5195   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 838899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=840000, episode_reward=-319.77 +/- 1.2264663550966541 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  319.77  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 840000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.7635   | \n",
      " |    critic_loss     | 0.9409   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 839899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=841000, episode_reward=-320.09 +/- 1.1860500241775176 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  320.09  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 841000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.0526   | \n",
      " |    critic_loss     | 0.5131   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 840899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=842000, episode_reward=-320.68 +/- 1.7071591995333875 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  320.68  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 842000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.5282   | \n",
      " |    critic_loss     | 0.8820   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 841899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=843000, episode_reward=-321.03 +/- 1.695344656504425 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  321.03  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 843000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.1076   | \n",
      " |    critic_loss     | 1.0269   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 842899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=844000, episode_reward=-322.02 +/- 1.6127946486262732 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  322.02  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 844000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.9104   | \n",
      " |    critic_loss     | 0.7276   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 843899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=845000, episode_reward=-322.92 +/- 1.0942238185301942 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  322.92  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 845000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.2070   | \n",
      " |    critic_loss     | 0.8018   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 844899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=846000, episode_reward=-323.03 +/- 1.3527915473159657 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  323.03  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 846000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.1204   | \n",
      " |    critic_loss     | 0.7801   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 845899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=847000, episode_reward=-323.78 +/- 1.7301578937501345 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  323.78  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 847000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.2378   | \n",
      " |    critic_loss     | 0.3095   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 846899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=848000, episode_reward=-323.96 +/- 1.1467775353002136 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  323.96  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 848000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.4838   | \n",
      " |    critic_loss     | 0.8709   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 847899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=849000, episode_reward=-324.23 +/- 1.8353053194040965 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  324.23  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 849000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.6844   | \n",
      " |    critic_loss     | 0.9211   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 848899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=850000, episode_reward=-325.13 +/- 1.4373622252247142 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  325.13  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 850000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.5269   | \n",
      " |    critic_loss     | 1.1317   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 849899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=851000, episode_reward=-325.24 +/- 1.863355209546114 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  325.24  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 851000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.7938   | \n",
      " |    critic_loss     | 0.4484   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 850899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=852000, episode_reward=-326.19 +/- 1.756274778105583 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  326.19  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 852000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.0049   | \n",
      " |    critic_loss     | 0.7512   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 851899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=853000, episode_reward=-326.69 +/- 1.8477954871670104 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  326.69  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 853000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.5350   | \n",
      " |    critic_loss     | 0.3838   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 852899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=854000, episode_reward=-326.91 +/- 1.2724687667861132 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  326.91  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 854000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.4608   | \n",
      " |    critic_loss     | 1.1152   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 853899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=855000, episode_reward=-328.0 +/- 1.8837332357614636 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   328.0  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 855000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.2632   | \n",
      " |    critic_loss     | 0.5658   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 854899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=856000, episode_reward=-327.62 +/- 1.9690961609590973 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  327.62  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 856000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 0.8999   | \n",
      " |    critic_loss     | 0.2250   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 855899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=857000, episode_reward=-328.05 +/- 1.3308978976441173 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  328.05  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 857000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.8156   | \n",
      " |    critic_loss     | 0.7039   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 856899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=858000, episode_reward=-329.4 +/- 1.8363503533372443 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   329.4  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 858000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 0.8995   | \n",
      " |    critic_loss     | 0.2249   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 857899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=859000, episode_reward=-329.27 +/- 1.0158101129796402 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  329.27  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 859000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.5318   | \n",
      " |    critic_loss     | 0.3830   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 858899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=860000, episode_reward=-329.68 +/- 1.5815659965239546 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  329.68  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 860000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.0949   | \n",
      " |    critic_loss     | 0.2737   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 859899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=861000, episode_reward=-330.69 +/- 1.4959313740068994 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  330.69  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 861000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.3221   | \n",
      " |    critic_loss     | 0.3305   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 860899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=862000, episode_reward=-331.13 +/- 1.9445679089537293 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  331.13  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 862000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.9611   | \n",
      " |    critic_loss     | 0.4903   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 861899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=863000, episode_reward=-331.07 +/- 1.949820372720045 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  331.07  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 863000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.1629   | \n",
      " |    critic_loss     | 1.0407   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 862899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=864000, episode_reward=-331.7 +/- 1.7372279776957964 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   331.7  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 864000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.7544   | \n",
      " |    critic_loss     | 0.9386   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 863899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=865000, episode_reward=-332.24 +/- 1.802337009520005 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  332.24  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 865000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.7036   | \n",
      " |    critic_loss     | 0.9259   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 864899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=866000, episode_reward=-332.6 +/- 1.6353827736643736 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   332.6  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 866000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.5861   | \n",
      " |    critic_loss     | 0.3965   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 865899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=867000, episode_reward=-333.42 +/- 1.6139458428903217 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  333.42  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 867000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.0120   | \n",
      " |    critic_loss     | 1.0030   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 866899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=868000, episode_reward=-334.19 +/- 1.3661787287656324 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  334.19  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 868000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 0.6799   | \n",
      " |    critic_loss     | 0.1700   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 867899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=869000, episode_reward=-334.85 +/- 1.3443305152698672 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  334.85  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 869000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.1957   | \n",
      " |    critic_loss     | 0.7989   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 868899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=870000, episode_reward=-335.04 +/- 1.036933621257631 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  335.04  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 870000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.5756   | \n",
      " |    critic_loss     | 0.8939   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 869899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=871000, episode_reward=-335.96 +/- 1.8179295273272604 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  335.96  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 871000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.4617   | \n",
      " |    critic_loss     | 0.8654   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 870899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=872000, episode_reward=-336.32 +/- 1.9690926399581468 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  336.32  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 872000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.4282   | \n",
      " |    critic_loss     | 0.6071   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 871899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=873000, episode_reward=-336.51 +/- 1.7934941388880632 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  336.51  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 873000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.7021   | \n",
      " |    critic_loss     | 0.6755   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 872899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=874000, episode_reward=-336.71 +/- 1.0122038375543931 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  336.71  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 874000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.4699   | \n",
      " |    critic_loss     | 1.1175   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 873899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=875000, episode_reward=-337.08 +/- 1.3208207487488028 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  337.08  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 875000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 0.9165   | \n",
      " |    critic_loss     | 0.2291   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 874899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=876000, episode_reward=-338.4 +/- 1.0182698903255432 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   338.4  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 876000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 0.5749   | \n",
      " |    critic_loss     | 0.1437   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 875899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=877000, episode_reward=-338.27 +/- 1.4651513750512208 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  338.27  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 877000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.2522   | \n",
      " |    critic_loss     | 0.8130   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 876899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=878000, episode_reward=-338.54 +/- 1.8562173121643961 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  338.54  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 878000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.0184   | \n",
      " |    critic_loss     | 1.0046   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 877899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=879000, episode_reward=-339.31 +/- 1.773298574839043 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  339.31  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 879000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.0975   | \n",
      " |    critic_loss     | 0.7744   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 878899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=880000, episode_reward=-340.31 +/- 1.3828408735907196 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  340.31  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 880000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.0602   | \n",
      " |    critic_loss     | 0.5151   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 879899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=881000, episode_reward=-340.29 +/- 1.1555344257616595 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  340.29  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 881000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 0.8611   | \n",
      " |    critic_loss     | 0.2153   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 880899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=882000, episode_reward=-341.23 +/- 1.5053945512957991 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  341.23  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 882000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 0.8015   | \n",
      " |    critic_loss     | 0.2004   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 881899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=883000, episode_reward=-341.94 +/- 1.3725180929791738 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  341.94  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 883000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.9709   | \n",
      " |    critic_loss     | 0.4927   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 882899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=884000, episode_reward=-341.85 +/- 1.8575643231618764 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  341.85  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 884000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.6117   | \n",
      " |    critic_loss     | 0.4029   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 883899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=885000, episode_reward=-342.2 +/- 1.0555703750738854 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   342.2  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 885000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 0.6885   | \n",
      " |    critic_loss     | 0.1721   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 884899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=886000, episode_reward=-343.13 +/- 1.3197359899762895 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  343.13  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 886000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 0.8543   | \n",
      " |    critic_loss     | 0.2136   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 885899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=887000, episode_reward=-343.76 +/- 1.2095972760536382 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  343.76  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 887000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.3679   | \n",
      " |    critic_loss     | 0.3420   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 886899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=888000, episode_reward=-344.26 +/- 1.8474255318737884 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  344.26  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 888000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.3283   | \n",
      " |    critic_loss     | 0.5821   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 887899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=889000, episode_reward=-344.46 +/- 1.6715680531880688 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  344.46  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 889000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.1042   | \n",
      " |    critic_loss     | 0.5261   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 888899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=890000, episode_reward=-344.89 +/- 1.6884588515834464 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  344.89  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 890000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.9142   | \n",
      " |    critic_loss     | 0.7286   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 889899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=891000, episode_reward=-345.24 +/- 1.9113243223576464 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  345.24  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 891000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 0.8097   | \n",
      " |    critic_loss     | 0.2024   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 890899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=892000, episode_reward=-345.77 +/- 1.6712787156290365 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  345.77  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 892000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.3432   | \n",
      " |    critic_loss     | 0.5858   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 891899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=893000, episode_reward=-346.94 +/- 1.2524942806265276 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  346.94  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 893000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.8148   | \n",
      " |    critic_loss     | 0.4537   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 892899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=894000, episode_reward=-346.74 +/- 1.3779488056918516 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  346.74  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 894000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.4299   | \n",
      " |    critic_loss     | 0.3575   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 893899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=895000, episode_reward=-347.15 +/- 1.356809815968059 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  347.15  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 895000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.5229   | \n",
      " |    critic_loss     | 0.8807   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 894899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=896000, episode_reward=-348.24 +/- 1.6955650390188657 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  348.24  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 896000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.8221   | \n",
      " |    critic_loss     | 0.7055   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 895899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=897000, episode_reward=-348.58 +/- 1.3554879061567733 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  348.58  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 897000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.8063   | \n",
      " |    critic_loss     | 0.9516   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 896899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=898000, episode_reward=-349.14 +/- 1.565222583965046 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  349.14  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 898000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.0252   | \n",
      " |    critic_loss     | 0.7563   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 897899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=899000, episode_reward=-349.72 +/- 1.8776990209225006 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  349.72  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 899000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.4218   | \n",
      " |    critic_loss     | 0.8554   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 898899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=900000, episode_reward=-349.52 +/- 1.456112673763011 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  349.52  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 900000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.9414   | \n",
      " |    critic_loss     | 0.4853   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 899899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=901000, episode_reward=-350.8 +/- 1.2625413099694676 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   350.8  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 901000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.8948   | \n",
      " |    critic_loss     | 0.9737   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 900899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=902000, episode_reward=-350.8 +/- 1.4365139594716712 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   350.8  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 902000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.1951   | \n",
      " |    critic_loss     | 0.5488   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 901899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=903000, episode_reward=-351.45 +/- 1.1104457728018573 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  351.45  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 903000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.7464   | \n",
      " |    critic_loss     | 0.4366   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 902899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=904000, episode_reward=-352.46 +/- 1.853966810230332 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  352.46  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 904000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.7621   | \n",
      " |    critic_loss     | 0.9405   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 903899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=905000, episode_reward=-352.85 +/- 1.2826161147829331 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  352.85  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 905000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.3765   | \n",
      " |    critic_loss     | 1.0941   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 904899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=906000, episode_reward=-352.67 +/- 1.7993092701997602 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  352.67  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 906000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 0.5261   | \n",
      " |    critic_loss     | 0.1315   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 905899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=907000, episode_reward=-353.36 +/- 1.8993809468431584 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  353.36  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 907000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.0982   | \n",
      " |    critic_loss     | 1.0245   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 906899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=908000, episode_reward=-353.97 +/- 1.1588078176882346 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  353.97  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 908000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.8063   | \n",
      " |    critic_loss     | 0.4516   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 907899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=909000, episode_reward=-354.72 +/- 1.3377161894986223 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  354.72  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 909000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.0305   | \n",
      " |    critic_loss     | 0.5076   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 908899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=910000, episode_reward=-354.92 +/- 1.4755288732181118 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  354.92  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 910000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.0642   | \n",
      " |    critic_loss     | 0.5161   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 909899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=911000, episode_reward=-355.78 +/- 1.5336860474832883 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  355.78  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 911000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.0839   | \n",
      " |    critic_loss     | 0.7710   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 910899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=912000, episode_reward=-355.91 +/- 1.8532836555128864 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  355.91  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 912000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.8797   | \n",
      " |    critic_loss     | 0.4699   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 911899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=913000, episode_reward=-356.99 +/- 1.4765897971156878 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  356.99  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 913000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.6155   | \n",
      " |    critic_loss     | 0.9039   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 912899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=914000, episode_reward=-357.48 +/- 1.7834223766951562 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  357.48  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 914000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.2994   | \n",
      " |    critic_loss     | 0.8249   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 913899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=915000, episode_reward=-357.88 +/- 1.5916406427822403 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  357.88  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 915000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.0962   | \n",
      " |    critic_loss     | 1.0240   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 914899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=916000, episode_reward=-357.81 +/- 1.710380678505556 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  357.81  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 916000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.8808   | \n",
      " |    critic_loss     | 0.9702   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 915899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=917000, episode_reward=-358.5 +/- 1.112627191511787 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   358.5  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 917000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 0.3792   | \n",
      " |    critic_loss     | 0.0948   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 916899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=918000, episode_reward=-359.32 +/- 1.8503091069046627 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  359.32  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 918000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.1562   | \n",
      " |    critic_loss     | 0.2891   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 917899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=919000, episode_reward=-359.04 +/- 1.0287440373083911 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  359.04  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 919000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 0.4090   | \n",
      " |    critic_loss     | 0.1022   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 918899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=920000, episode_reward=-360.25 +/- 1.78983452584909 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  360.25  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 920000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.6200   | \n",
      " |    critic_loss     | 0.9050   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 919899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=921000, episode_reward=-360.66 +/- 1.3081934193351936 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  360.66  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 921000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.8399   | \n",
      " |    critic_loss     | 0.9600   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 920899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=922000, episode_reward=-360.63 +/- 1.4181249764188495 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  360.63  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 922000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 0.9227   | \n",
      " |    critic_loss     | 0.2307   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 921899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=923000, episode_reward=-361.91 +/- 1.7268488984258707 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  361.91  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 923000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 0.8148   | \n",
      " |    critic_loss     | 0.2037   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 922899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=924000, episode_reward=-361.69 +/- 1.9868320458557662 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  361.69  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 924000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.3717   | \n",
      " |    critic_loss     | 0.8429   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 923899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=925000, episode_reward=-362.8 +/- 1.801207195260027 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   362.8  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 925000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.3021   | \n",
      " |    critic_loss     | 0.3255   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 924899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=926000, episode_reward=-362.54 +/- 1.6038140404792602 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  362.54  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 926000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.7805   | \n",
      " |    critic_loss     | 0.9451   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 925899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=927000, episode_reward=-363.52 +/- 1.166364578093726 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  363.52  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 927000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.8950   | \n",
      " |    critic_loss     | 0.9737   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 926899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=928000, episode_reward=-364.4 +/- 1.860788831106081 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   364.4  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 928000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.4124   | \n",
      " |    critic_loss     | 0.8531   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 927899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=929000, episode_reward=-364.81 +/- 1.0502560809057706 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  364.81  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 929000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.5885   | \n",
      " |    critic_loss     | 0.6471   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 928899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=930000, episode_reward=-365.49 +/- 1.5907032533195729 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  365.49  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 930000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.6186   | \n",
      " |    critic_loss     | 0.6547   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 929899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=931000, episode_reward=-365.67 +/- 1.025809066514109 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  365.67  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 931000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.6576   | \n",
      " |    critic_loss     | 0.4144   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 930899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=932000, episode_reward=-366.21 +/- 1.077434039367672 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  366.21  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 932000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 0.6765   | \n",
      " |    critic_loss     | 0.1691   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 931899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=933000, episode_reward=-366.46 +/- 1.5975311303160595 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  366.46  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 933000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.7217   | \n",
      " |    critic_loss     | 0.9304   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 932899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=934000, episode_reward=-367.44 +/- 1.9166811858570096 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  367.44  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 934000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.2792   | \n",
      " |    critic_loss     | 0.5698   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 933899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=935000, episode_reward=-367.86 +/- 1.6384409630356318 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  367.86  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 935000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 0.7982   | \n",
      " |    critic_loss     | 0.1996   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 934899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=936000, episode_reward=-367.77 +/- 1.2822620632065147 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  367.77  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 936000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.1809   | \n",
      " |    critic_loss     | 0.2952   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 935899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=937000, episode_reward=-368.12 +/- 1.2125143679966066 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  368.12  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 937000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.7828   | \n",
      " |    critic_loss     | 0.4457   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 936899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=938000, episode_reward=-369.37 +/- 1.428486809453969 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  369.37  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 938000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.9283   | \n",
      " |    critic_loss     | 0.4821   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 937899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=939000, episode_reward=-369.25 +/- 1.0609484104807474 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  369.25  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 939000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.7920   | \n",
      " |    critic_loss     | 0.9480   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 938899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=940000, episode_reward=-369.58 +/- 1.523520766564183 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  369.58  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 940000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.3752   | \n",
      " |    critic_loss     | 0.8438   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 939899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=941000, episode_reward=-370.71 +/- 1.6849650147236288 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  370.71  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 941000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.2379   | \n",
      " |    critic_loss     | 0.3095   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 940899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=942000, episode_reward=-370.68 +/- 1.2269147450178242 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  370.68  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 942000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.4480   | \n",
      " |    critic_loss     | 0.3620   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 941899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=943000, episode_reward=-371.96 +/- 1.7345198342063193 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  371.96  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 943000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.8931   | \n",
      " |    critic_loss     | 0.4733   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 942899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=944000, episode_reward=-371.8 +/- 1.3427193950562089 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   371.8  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 944000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 0.5695   | \n",
      " |    critic_loss     | 0.1424   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 943899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=945000, episode_reward=-372.36 +/- 1.2162094400409698 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  372.36  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 945000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.5346   | \n",
      " |    critic_loss     | 0.3836   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 944899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=946000, episode_reward=-372.5 +/- 1.8577452361504818 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   372.5  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 946000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.9199   | \n",
      " |    critic_loss     | 0.7300   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 945899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=947000, episode_reward=-373.91 +/- 1.0453676142908033 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  373.91  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 947000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.3715   | \n",
      " |    critic_loss     | 0.8429   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 946899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=948000, episode_reward=-374.12 +/- 1.7064629738774317 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  374.12  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 948000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 0.4076   | \n",
      " |    critic_loss     | 0.1019   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 947899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=949000, episode_reward=-374.85 +/- 1.3209379420744805 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  374.85  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 949000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.3354   | \n",
      " |    critic_loss     | 0.5838   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 948899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=950000, episode_reward=-374.86 +/- 1.8095258704903503 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  374.86  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 950000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.9113   | \n",
      " |    critic_loss     | 0.4778   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 949899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=951000, episode_reward=-375.78 +/- 1.6518914240119948 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  375.78  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 951000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.4939   | \n",
      " |    critic_loss     | 0.3735   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 950899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=952000, episode_reward=-376.11 +/- 1.2018460883291433 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  376.11  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 952000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 0.7135   | \n",
      " |    critic_loss     | 0.1784   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 951899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=953000, episode_reward=-376.32 +/- 1.8168536744026933 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  376.32  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 953000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.0139   | \n",
      " |    critic_loss     | 0.7535   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 952899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=954000, episode_reward=-376.59 +/- 1.3434274229018848 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  376.59  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 954000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.3127   | \n",
      " |    critic_loss     | 0.3282   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 953899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=955000, episode_reward=-377.22 +/- 1.849440444729332 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  377.22  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 955000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 0.8503   | \n",
      " |    critic_loss     | 0.2126   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 954899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=956000, episode_reward=-378.36 +/- 1.7981067722999107 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  378.36  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 956000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 0.2884   | \n",
      " |    critic_loss     | 0.0721   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 955899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=957000, episode_reward=-378.13 +/- 1.876982957098563 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  378.13  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 957000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.2798   | \n",
      " |    critic_loss     | 0.5699   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 956899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=958000, episode_reward=-378.97 +/- 1.9531245279617435 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  378.97  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 958000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.1194   | \n",
      " |    critic_loss     | 0.7798   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 957899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=959000, episode_reward=-379.2 +/- 1.4934037361803036 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   379.2  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 959000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.0417   | \n",
      " |    critic_loss     | 1.0104   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 958899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=960000, episode_reward=-379.6 +/- 1.7779032652499391 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   379.6  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 960000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.6237   | \n",
      " |    critic_loss     | 0.9059   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 959899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=961000, episode_reward=-380.89 +/- 1.3211337653023012 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  380.89  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 961000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.1997   | \n",
      " |    critic_loss     | 0.7999   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 960899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=962000, episode_reward=-380.87 +/- 1.0000196520984794 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  380.87  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 962000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 0.9290   | \n",
      " |    critic_loss     | 0.2322   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 961899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=963000, episode_reward=-381.3 +/- 1.738113735955611 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   381.3  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 963000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.3027   | \n",
      " |    critic_loss     | 0.5757   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 962899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=964000, episode_reward=-381.59 +/- 1.7241324038821118 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  381.59  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 964000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.3771   | \n",
      " |    critic_loss     | 0.3443   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 963899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=965000, episode_reward=-382.54 +/- 1.7941041285423789 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  382.54  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 965000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.5316   | \n",
      " |    critic_loss     | 0.3829   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 964899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=966000, episode_reward=-383.18 +/- 1.2240256032860044 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  383.18  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 966000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.8969   | \n",
      " |    critic_loss     | 0.9742   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 965899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=967000, episode_reward=-383.38 +/- 1.0570402680668494 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  383.38  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 967000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.0370   | \n",
      " |    critic_loss     | 0.2593   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 966899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=968000, episode_reward=-384.06 +/- 1.67585650573708 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  384.06  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 968000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.8850   | \n",
      " |    critic_loss     | 0.7213   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 967899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=969000, episode_reward=-384.45 +/- 1.745997484964831 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  384.45  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 969000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.0601   | \n",
      " |    critic_loss     | 0.5150   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 968899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=970000, episode_reward=-384.53 +/- 1.8883570463837056 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  384.53  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 970000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.8535   | \n",
      " |    critic_loss     | 0.7134   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 969899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=971000, episode_reward=-385.61 +/- 1.260603482454064 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  385.61  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 971000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.2978   | \n",
      " |    critic_loss     | 0.5745   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 970899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=972000, episode_reward=-385.78 +/- 1.2996994944956388 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  385.78  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 972000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.6898   | \n",
      " |    critic_loss     | 0.9224   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 971899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=973000, episode_reward=-386.13 +/- 1.7664311194335411 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  386.13  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 973000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.1758   | \n",
      " |    critic_loss     | 0.7940   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 972899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=974000, episode_reward=-386.51 +/- 1.4162708167641034 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  386.51  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 974000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.7109   | \n",
      " |    critic_loss     | 0.9277   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 973899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=975000, episode_reward=-387.97 +/- 1.708140465346566 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  387.97  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 975000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.0346   | \n",
      " |    critic_loss     | 0.5086   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 974899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=976000, episode_reward=-387.85 +/- 1.4503982269123352 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  387.85  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 976000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.5007   | \n",
      " |    critic_loss     | 0.3752   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 975899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=977000, episode_reward=-388.23 +/- 1.2541694369759449 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  388.23  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 977000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.6636   | \n",
      " |    critic_loss     | 0.6659   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 976899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=978000, episode_reward=-388.62 +/- 1.2004781686277206 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  388.62  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 978000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.0184   | \n",
      " |    critic_loss     | 0.2546   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 977899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=979000, episode_reward=-389.59 +/- 1.321777940195087 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  389.59  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 979000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 0.8222   | \n",
      " |    critic_loss     | 0.2055   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 978899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=980000, episode_reward=-390.48 +/- 1.2298293964537268 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  390.48  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 980000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.6999   | \n",
      " |    critic_loss     | 0.4250   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 979899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=981000, episode_reward=-390.1 +/- 1.4449853532840855 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   390.1  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 981000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 0.4785   | \n",
      " |    critic_loss     | 0.1196   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 980899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=982000, episode_reward=-390.69 +/- 1.8413898943810478 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  390.69  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 982000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.2811   | \n",
      " |    critic_loss     | 0.5703   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 981899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=983000, episode_reward=-391.72 +/- 1.046160772722538 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  391.72  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 983000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.0043   | \n",
      " |    critic_loss     | 0.5011   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 982899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=984000, episode_reward=-391.51 +/- 1.389674042462849 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  391.51  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 984000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.9122   | \n",
      " |    critic_loss     | 0.7281   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 983899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=985000, episode_reward=-392.85 +/- 1.5687330161815964 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  392.85  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 985000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.8872   | \n",
      " |    critic_loss     | 0.4718   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 984899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=986000, episode_reward=-393.26 +/- 1.5320984076118829 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  393.26  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 986000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 0.5768   | \n",
      " |    critic_loss     | 0.1442   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 985899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=987000, episode_reward=-393.24 +/- 1.8806786790242647 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  393.24  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 987000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 0.3851   | \n",
      " |    critic_loss     | 0.0963   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 986899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=988000, episode_reward=-393.84 +/- 1.0147690871202224 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  393.84  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 988000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.5068   | \n",
      " |    critic_loss     | 0.8767   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 987899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=989000, episode_reward=-394.27 +/- 1.524111025277414 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  394.27  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 989000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 0.4874   | \n",
      " |    critic_loss     | 0.1218   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 988899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=990000, episode_reward=-394.58 +/- 1.3795391424633308 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  394.58  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 990000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.0607   | \n",
      " |    critic_loss     | 0.7652   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 989899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=991000, episode_reward=-395.37 +/- 1.2622422494712802 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  395.37  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 991000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.8802   | \n",
      " |    critic_loss     | 0.4700   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 990899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=992000, episode_reward=-395.61 +/- 1.0719857461192674 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  395.61  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 992000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.4617   | \n",
      " |    critic_loss     | 0.8654   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 991899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=993000, episode_reward=-396.34 +/- 1.3783002106100963 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  396.34  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 993000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.1140   | \n",
      " |    critic_loss     | 0.5285   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 992899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=994000, episode_reward=-397.45 +/- 1.6837760908555763 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  397.45  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 994000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 0.3853   | \n",
      " |    critic_loss     | 0.0963   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 993899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=995000, episode_reward=-397.06 +/- 1.0525601295789007 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  397.06  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 995000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.2112   | \n",
      " |    critic_loss     | 0.5528   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 994899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=996000, episode_reward=-398.05 +/- 1.9218522444057418 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  398.05  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 996000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 0.9535   | \n",
      " |    critic_loss     | 0.2384   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 995899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=997000, episode_reward=-398.74 +/- 1.3286003065284944 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  398.74  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 997000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 0.5669   | \n",
      " |    critic_loss     | 0.1417   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 996899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=998000, episode_reward=-399.45 +/- 1.7980277020064146 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  399.45  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 998000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.3693   | \n",
      " |    critic_loss     | 0.5923   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 997899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=999000, episode_reward=-399.27 +/- 1.0538253145423149 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  399.27  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 999000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.5615   | \n",
      " |    critic_loss     | 0.8904   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 998899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=1000000, episode_reward=-399.5 +/- 1.959313682105638 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   399.5  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 1000000  | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.8441   | \n",
      " |    critic_loss     | 0.9610   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 999899   | \n",
      " --------------------------------- \n"
     ]
    },
    {
     "data": {},
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Start training\n",
    "time_steps = 1000000\n",
    "model.learn(total_timesteps=time_steps, callback=eval_callback, log_interval=10)\n",
    "model.save(\"artifacts/model/td3_car_racing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This last section of the notebook contains a visualization of the mean reward for each episode during training. Although we see high fluctuations in the beginning the training seems relatively stable and converges against a mean reward of around 400 per episode. \n",
    "\n",
    "Finally, we created a visualization tool that generates an animated sample interaction of the agent with the environment which is saved locally in `artifacts/animation/td3_car_racer.gif`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from stable_baselines3.common import results_plotter\n",
    "from utils.visualization import render_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results_plotter.plot_results([log_dir], time_steps, results_plotter.X_EPISODES, \"TD3 Training on Car Racing V2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved evaluation GIF to \"artifacts/td3_car_racer.gif\""
     ]
    }
   ],
   "source": [
    "# Save as gif\n",
    "render_example(env_id=env_id, continuous=True, render_frames=True, output_file=\"artifacts/animation/td3_car_racer.gif\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
