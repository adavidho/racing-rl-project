{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TD3 on Car Racing V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # General\n",
    "import platform\n",
    "assert platform.python_version() == \"3.10.14\"\n",
    "# !pip install 'gymnasium[box2d]'\n",
    "# !pip install 'syne-tune[basic]'\n",
    "# !pip install 'stable-baselines3[extra]'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/adavidho/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from syne_tune import Tuner\n",
    "from syne_tune.backend import PythonBackend\n",
    "from syne_tune.experiments import load_experiment\n",
    "from syne_tune.config_space import loguniform, uniform\n",
    "from syne_tune.optimizer.baselines import ASHA\n",
    "from syne_tune.stopping_criterion import StoppingCriterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the hyperparameter search space\n",
    "config_space = {\n",
    "    \"learning_rate\": loguniform(1e-8, 0.1),\n",
    "    \"tau\":  loguniform(1e-8, 1),\n",
    "    \"gamma\": uniform(0.9, 0.999),    \n",
    "    \"steps\": 100000 \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the tuning function\n",
    "def train_hpo_model(learning_rate: float, tau: float, gamma: float, steps: int):\n",
    "    # Worker imports\n",
    "    import numpy as np\n",
    "    from stable_baselines3.common.env_util import make_vec_env\n",
    "    from stable_baselines3.common.callbacks import BaseCallback\n",
    "    from stable_baselines3.common.noise import NormalActionNoise\n",
    "    from stable_baselines3 import TD3\n",
    "    \n",
    "    from syne_tune import Reporter\n",
    "\n",
    "    # Create the vectorized environment\n",
    "    env_id = \"CarRacing-v2\"\n",
    "    vec_env = make_vec_env(env_id, n_envs=4)\n",
    "    \n",
    "    # Initialize the PPO agent with the given hyperparameters\n",
    "    n_actions = vec_env.action_space.shape[-1]\n",
    "    action_noise = NormalActionNoise(mean=np.zeros(n_actions), sigma=0.1 * np.ones(n_actions))  \n",
    "    model = TD3(\"CnnPolicy\", vec_env,  \n",
    "                action_noise=action_noise,\n",
    "                learning_rate=learning_rate,\n",
    "                tau=tau,\n",
    "                gamma=gamma,\n",
    "                batch_size=32,\n",
    "                verbose=1)\n",
    "\n",
    "    report = Reporter()\n",
    "    class WorkerCallback(BaseCallback):\n",
    "        def _on_step(self) -> bool:\n",
    "            # Log the mean reward\n",
    "            mean_reward = sum(self.locals[\"rewards\"]) / len(self.locals[\"rewards\"])\n",
    "            step = self.locals[\"num_collected_steps\"]\n",
    "            report(step=step, mean_reward=mean_reward)\n",
    "            return True \n",
    "    \n",
    "    # Train the agent\n",
    "    worker_callback = WorkerCallback()\n",
    "    model.learn(total_timesteps=steps, callback=worker_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = \"mean_reward\"\n",
    "scheduler = ASHA(\n",
    "    config_space,\n",
    "    metric=metric,\n",
    "    max_resource_attr=\"steps\",\n",
    "    resource_attr=\"step\",\n",
    "    mode=\"max\",\n",
    ")\n",
    "trial_backend = PythonBackend(\n",
    "    tune_function=train_hpo_model, config_space=config_space\n",
    ")\n",
    "stop_criterion = StoppingCriterion(\n",
    "    max_wallclock_time=61200, \n",
    ")\n",
    "tuner = Tuner(\n",
    "    trial_backend=trial_backend,\n",
    "    scheduler=scheduler,\n",
    "    stop_criterion=stop_criterion,\n",
    "    n_workers=8,\n",
    "    save_tuner=False,\n",
    "    wait_trial_completion_when_stopping=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "Resource summary (last result is reported):\n",
      " trial_id     status  iter  learning_rate          tau        gamma      steps  step  mean_reward worker-time\n",
      "        0    Stopped   129    7.88267e-06  5.47516e-03  5.40364e-01    4059.34     1      1.45570    8118.69\n",
      "        1  Completed  1212    9.51271e-06  1.66619e-03  9.14142e-01   11781.48     1      7.42613    23562.96\n",
      "        2  Completed   579    2.81416e-06  7.36647e-03  9.09822e-01    9259.44     1      7.68924    18518.88\n",
      "        3  Completed  1141    8.75483e-06  9.09624e-03  9.46999e-01    5047.35     1      4.12335    10094.70\n",
      "        4  Completed  1441    3.02342e-06  2.27268e-03  3.71751e-01    7804.32     1      5.41025    15608.63\n",
      "        5  Completed  2349    2.80457e-06  4.34460e-03  3.29197e-01    2359.99     1      6.60238    4719.98\n",
      "        6    Stopped   544    7.00647e-06  3.60703e-03  1.39702e-01    2005.98     1      2.47842    4011.96\n",
      "        7    Stopped  1487    9.00671e-06  2.90336e-03  2.78509e-01    1075.38     1      6.05057    2150.77\n",
      "        8  Completed  1639    1.07017e-06  3.82686e-04  3.70520e-01    4295.53     1      2.85244    8591.06\n",
      "        9    Stopped  1713    2.36688e-06  8.44528e-03  9.97344e-01    9233.44     1      0.15017    18466.87\n",
      "       10  Completed   560    7.02269e-06  7.76287e-03  7.45652e-01    12204.7     1      1.61729    24409.40\n",
      "       11  Completed   340    7.58807e-06  8.11626e-03  9.89857e-01    1707.45     1      6.24368    3414.90\n",
      "       12    Stopped  1392    1.97748e-06  3.68983e-03  2.93725e-01   10881.95     1      7.25104    21763.90\n",
      "       13  Completed   343    1.42435e-06  5.22673e-03  1.37058e-01    4575.87     1      5.53144    9151.74\n",
      "       14  Completed  2005    3.53674e-06  7.41411e-03  3.68132e-01    8022.13     1      2.61987    16044.25\n",
      "       15  Completed   992    2.25932e-06  1.54798e-03  3.29174e-01    4859.96     1      -0.02841    9719.92\n",
      "       16    Stopped  1217    8.00970e-06  5.76228e-03  3.95267e-01    3905.25     1      2.13568    7810.50\n",
      "       17  Completed   996    4.91732e-06  1.01548e-03  6.25453e-01    5720.62     1      6.91986    11441.25\n",
      "       18  Completed  2316    2.60023e-06  2.75630e-03  6.18198e-01    6156.14     1      0.94944    12312.27\n",
      "       19    Stopped   747    8.04746e-06  7.11112e-03  9.62531e-01     7000.3     1      3.49205    14000.59\n",
      "       20  Completed    57    2.43844e-06  1.54133e-03  7.16713e-01   11595.76     1      2.32014    23191.53\n",
      "       21  Completed  2206    9.73509e-06  2.66009e-03  6.51295e-01    6423.67     1      6.20175    12847.33\n",
      "       22  Completed  1714    8.82639e-06  6.19088e-03  7.89288e-01    6652.69     1      3.23654    13305.38\n",
      "       23    Stopped  1832    5.71371e-06  2.24940e-03  3.88589e-01    8755.32     1      6.99545    17510.63\n",
      "       24  Completed  1726    3.24975e-06  9.84938e-04  7.74659e-01     3690.2     1      4.91961    7380.41\n",
      "       25  Completed  1751    9.72574e-07  5.04947e-03  8.45978e-01    6659.93     1      4.85218    13319.87\n",
      "       26    Stopped   597    5.31965e-06  8.91170e-03  1.35707e-01    5992.62     1      1.90903    11985.23\n",
      "       27  Completed  1553    7.54000e-06  6.79448e-04  2.93615e-01    7412.78     1      6.47996    14825.57\n",
      "       28  Completed  2257    2.55348e-06  4.63681e-03  5.39701e-01    6095.53     1      5.23767    12191.06\n",
      "       29    Stopped  2491    9.90095e-06  3.93748e-03  1.06807e-01    2524.77     1      1.91492    5049.55\n",
      "       30    Stopped   217    1.26489e-06  6.14639e-03  3.59887e-01    3158.21     1      5.18002    6316.42\n",
      "       31    Stopped   250    5.66919e-06  2.81702e-03  5.60340e-01    9341.76     1      2.73308    18683.53\n",
      "       32    Stopped    57    8.37760e-06  8.29517e-03  6.38748e-01    8302.17     1      2.75784    16604.33\n",
      "       33  Completed   743    6.90610e-06  7.82710e-03  5.21296e-01    1201.45     1      2.29919    2402.89\n",
      "       34  Completed   831    3.13316e-06  1.91780e-03  9.29530e-01    1223.62     1      3.56811    2447.24\n",
      "       35    Stopped  1511    9.10049e-06  5.90612e-03  1.16444e-01   11241.39     1      7.73914    22482.78\n",
      "       36  Completed   432    4.61529e-06  8.49544e-03  2.62635e-01   11018.13     1      6.65145    22036.26\n",
      "       37  Completed  1445    2.92577e-06  3.18180e-03  1.33656e-01   10247.92     1      2.61367    20495.84\n",
      "       38  Completed   364    2.23890e-06  7.72430e-03  5.36344e-01    3602.22     1      4.76991    7204.45\n",
      "       39  Completed   588    8.15467e-06  4.48694e-03  6.60049e-01     980.41     1      5.54257    1960.83\n",
      "       40    Stopped   750    5.51741e-06  7.15542e-03  5.32162e-01   10482.17     1      0.27583    20964.33\n",
      "       41    Stopped   896    1.11464e-06  7.73801e-03  3.96889e-01    6036.99     1      0.97577    12073.97\n",
      "       42    Stopped  1544    1.63953e-06  3.98309e-03  1.71531e-01   11766.97     1      7.45702    23533.93\n",
      "       43  Completed  2414    9.83541e-06  6.49824e-04  2.83006e-01    12169.5     1      3.62718    24339.00\n",
      "       44    Stopped  1761    1.47186e-06  6.15661e-03  7.32349e-01   10388.31     1      2.22391    20776.62\n",
      "       45    Stopped   747    5.40588e-06  3.35839e-03  7.09856e-01    7939.95     1      2.75962    15879.91\n",
      "       46    Stopped   215    7.93124e-06  7.07115e-03  4.48053e-01   10545.82     1      0.75507    21091.63\n",
      "       47    Stopped   265    1.93626e-06  6.65800e-03  4.59677e-01    9436.69     1      2.97459    18873.38\n",
      "       48  Completed  1015    4.40743e-06  3.38483e-03  3.35020e-01     387.09     1      3.04570     774.19\n",
      "       49    Stopped  2318    3.36427e-06  3.13472e-03  8.59811e-01    6909.94     1      0.43930    13819.89\n",
      "       50    Stopped  1200    5.41155e-06  5.03509e-03  8.11715e-01    1570.81     1      1.51727    3141.63\n",
      "       51  Completed  1055    6.45886e-06  9.17578e-03  9.62999e-01    11353.8     1      7.94708    22707.60\n",
      "       52  Completed  1409    1.58618e-06  5.25140e-03  8.99765e-01     9676.9     1      1.78925    19353.81\n",
      "       53  Completed  1563    6.92153e-06  9.25764e-04  1.32385e-01   10182.61     1      2.79400    20365.22\n",
      "       54  Completed   401    7.00876e-06  6.64191e-03  8.20658e-01     3568.7     1      2.05062    7137.40\n",
      "       55    Stopped   736    4.26459e-06  8.72110e-03  8.35020e-01    9615.83     1      2.68006    19231.66\n",
      "       56    Stopped   877    8.64056e-06  2.76251e-03  4.54792e-01      498.3     1      0.16323     996.60\n",
      "       57    Stopped  2341    9.10097e-06  6.60789e-03  8.70480e-01     3109.9     1      1.81744    6219.79\n",
      "       58    Stopped   126    6.54134e-06  2.47878e-03  2.37907e-01    7462.93     1      0.12380    14925.87\n",
      "       59  Completed  1110    2.87644e-06  8.24108e-03  8.72884e-01   10204.23     1      6.08235    20408.46\n",
      "       60    Stopped   308    7.82605e-06  5.97046e-03  3.21145e-01    6267.93     1      4.18277    12535.86\n",
      "       61  Completed  1345    5.19019e-06  9.80995e-04  3.93383e-01    3549.63     1      4.08114    7099.25\n",
      "       62  Completed  1288    2.07490e-06  4.63766e-03  8.94037e-01    5726.66     1      7.04520    11453.33\n",
      "       63    Stopped  1717    6.50211e-06  2.28788e-03  6.58750e-01    2888.54     1      4.62126    5777.08\n",
      "       64  Completed  1752    1.72532e-06  7.06774e-03  4.58216e-01    7697.08     1      0.25567    15394.16\n",
      "       65  Completed  1628    6.98244e-06  6.25319e-04  2.53028e-01     723.85     1      0.09448    1447.70\n",
      "       66  Completed  1645    3.17433e-06  6.34344e-03  6.18934e-01    4840.53     1      1.99041    9681.05\n",
      "       67  Completed  2317    7.86420e-06  7.31177e-03  1.04092e-01    6990.63     1      3.68419    13981.26\n",
      "       68  Completed  1472    1.65647e-06  4.63594e-03  1.97397e-01   10462.19     1      2.84005    20924.39\n",
      "       69    Stopped    90    2.93163e-06  9.39999e-03  1.57231e-01     634.78     1      6.30970    1269.57\n",
      "       70    Stopped   510    2.52107e-07  1.75000e-03  3.88787e-01    2339.94     1      1.75555    4679.89\n",
      "       71    Stopped  2076    4.23648e-06  1.35062e-03  1.35523e-01    9962.53     1      0.97173    19925.05\n",
      "       72  Completed  1462    7.16818e-06  2.35686e-03  5.08743e-01   12114.72     1      6.39971    24229.44\n",
      "       73    Stopped  2107    8.44304e-06  8.10656e-03  4.56538e-01     701.74     1      3.99412    1403.48\n",
      "       74  Completed   659    4.63792e-06  6.42154e-04  1.09950e-01    1193.69     1      3.18772    2387.38\n",
      "       75    Stopped   909    8.37659e-06  4.25702e-03  7.46610e-01    3844.93     1      7.44772    7689.86\n",
      "       76  Completed  1860    5.14473e-06  6.69183e-03  6.35404e-01     1214.3     1      2.89447    2428.60\n",
      "       77  Completed   392    4.01273e-06  7.67186e-03  6.12562e-01   11820.07     1      1.65866    23640.14\n",
      "       78    Stopped  1735    4.99406e-06  1.15696e-03  9.62473e-01   10612.32     1      4.52666    21224.64\n",
      "       79    Stopped  2231    8.72222e-06  6.21060e-03  5.37402e-01     5412.4     1      2.34887    10824.79\n",
      "       80  Completed  1016    4.26966e-06  1.43553e-03  6.21139e-01   11194.59     1      5.39214    22389.18\n",
      "       81    Stopped  1098    6.75730e-06  1.51158e-03  7.65371e-01    5155.77     1      1.45974    10311.55\n",
      "       82    Stopped  1283    3.66126e-06  4.82018e-04  6.08379e-01    3987.86     1      4.10459    7975.72\n",
      "       83  Completed  1025    1.24660e-06  1.31207e-03  2.70316e-01    4414.86     1      0.03751    8829.72\n",
      "       84  Completed   119    4.22545e-06  6.94355e-03  6.31098e-01    4212.45     1      7.76488    8424.90\n",
      "       85  Completed  1828    8.10843e-06  7.61751e-03  9.46741e-01   12023.35     1      4.57265    24046.70\n",
      "       86  Completed  1434    7.93096e-06  7.34076e-03  6.89533e-01     340.02     1      4.16105     680.03\n",
      "       87    Stopped    65    4.99997e-06  5.02945e-03  3.17574e-01    6019.51     1      4.69052    12039.03\n",
      "       88    Stopped  2162    9.57450e-06  2.64032e-05  6.31605e-01     949.98     1      6.01141    1899.96\n",
      "       89    Stopped  2359    3.46363e-06  1.24247e-03  2.45991e-01    6431.87     1      3.56692    12863.75\n",
      "       90    Stopped  1464    3.75926e-06  5.00380e-03  1.78565e-01    4494.22     1      3.41023    8988.44\n",
      "       91  Completed  2498    8.22387e-06  4.31494e-03  3.57104e-01    2651.58     1      3.92822    5303.17\n",
      "       92    Stopped  1458    9.49434e-07  9.60915e-03  9.16444e-01    2410.19     1      6.21598    4820.38\n",
      "       93  Completed  1695    1.66016e-06  6.04706e-03  3.09973e-01    3277.88     1      2.97231    6555.77\n",
      "       94  Completed  1413    8.32534e-06  5.76129e-03  4.13532e-01    3220.21     1      5.34308    6440.43\n",
      "       95  Completed   959    8.94678e-07  9.61655e-03  9.81525e-01    9314.84     1      5.77212    18629.69\n",
      "       96  Completed   588    5.48424e-06  5.89240e-03  1.73108e-01     2327.6     1      0.60297    4655.20\n",
      "       97    Stopped   453    4.59025e-06  7.12314e-03  4.56345e-01    5643.95     1      6.98260    11287.90\n",
      "       98    Stopped  1540    1.60462e-06  6.52525e-03  4.46816e-01     567.78     1      1.56046    1135.56\n",
      "       99  Completed  1068    4.04215e-06  8.51611e-03  5.66226e-01    2375.32     1      7.54971    4750.64\n",
      "      100    Stopped   469    2.11390e-06  4.64776e-03  6.06871e-01    1126.02     1      6.15129    2252.03\n",
      "      101    Stopped   241    7.25638e-06  9.56171e-03  4.22667e-01    2727.83     1      5.97634    5455.65\n",
      "      102    Stopped   414    8.41713e-06  5.06819e-03  4.21000e-01     330.63     1      2.46890     661.26\n",
      "      103  Completed   625    6.77486e-06  9.45537e-03  1.54842e-01    8786.21     1      7.02130    17572.42\n",
      "      104  Completed  2145    1.62770e-06  1.41239e-04  6.92853e-01   10435.59     1      7.77961    20871.18\n",
      "      105    Stopped  1800    8.61339e-06  7.11097e-03  2.79704e-01   10252.26     1      2.74575    20504.53\n",
      "      106    Stopped   764    5.34841e-06  9.79535e-03  4.12917e-01    1050.56     1      6.13436    2101.11\n",
      "      107    Stopped   434    6.01251e-06  3.63513e-03  7.48842e-01     495.37     1      6.35931     990.74\n",
      "      108    Stopped  2486    5.08276e-06  5.09032e-03  2.08535e-01   11055.42     1      1.84961    22110.83\n",
      "      109  Completed   368    1.24201e-06  4.53076e-03  4.54241e-01    8856.31     1      2.08249    17712.61\n",
      "      110    Stopped   981    6.20512e-06  2.12157e-03  1.04303e-01    8142.89     1      0.94079    16285.79\n",
      "      111  Completed  1642    8.53239e-06  8.66689e-03  6.09516e-01    1984.49     1      3.63198    3968.98\n",
      "      112  Completed  2307    7.69425e-06  3.75192e-03  4.67668e-01     523.69     1      4.79259    1047.39\n",
      "      113  Completed   563    6.06513e-07  4.19731e-03  7.43438e-01    4993.56     1      1.57592    9987.12\n",
      "      114    Stopped  1724    5.16336e-06  2.43923e-03  3.79763e-01    5750.41     1      1.93025    11500.82\n",
      "      115    Stopped  1748    7.13056e-06  3.00878e-03  6.02539e-01    1909.08     1      0.56577    3818.16\n",
      "      116  Completed  2080    9.25484e-06  7.95223e-03  3.26746e-01    6094.62     1      5.76689    12189.25\n",
      "      117    Stopped   425    6.34786e-06  6.38743e-03  8.83824e-01    5567.15     1      1.48633    11134.31\n",
      "      118    Stopped   885    2.83204e-06  8.99601e-03  7.89608e-01    8303.58     1      2.00976    16607.15\n",
      "      119    Stopped  1036    4.00672e-06  7.03746e-03  6.38455e-01    9953.38     1      7.59064    19906.77\n",
      "      120    Stopped  1828    8.34233e-06  1.02494e-03  6.57248e-01    7197.65     1      5.29138    14395.30\n",
      "      121    Stopped  2187    1.71626e-06  8.65776e-03  4.44911e-01    7049.06     1      5.24428    14098.13\n",
      "      122  Completed   721    2.76201e-07  6.45760e-03  6.34805e-01   11311.76     1      5.48391    22623.52\n",
      "      123    Stopped   789    1.69073e-06  2.79903e-03  9.43667e-01    7355.02     1      3.30029    14710.04\n",
      "      124    Stopped   407    5.66550e-06  7.58162e-03  6.99005e-01    8640.47     1      4.23437    17280.93\n",
      "      125  Completed   424    3.98487e-06  7.50562e-03  9.53103e-01   10155.81     1      3.74752    20311.62\n",
      "      126    Stopped  1324    4.65384e-06  2.88222e-03  2.72040e-01   11996.81     1      0.03255    23993.62\n",
      "      127    Stopped   449    2.69356e-06  5.17698e-03  9.97254e-01    3320.02     1      2.98358    6640.05\n",
      "      128  Completed  1221    5.31104e-06  6.25198e-03  3.21826e-01   10143.14     1      4.84983    20286.28\n",
      "      129    Stopped  1998    5.72863e-06  1.61303e-03  3.15610e-01    6958.05     1      6.43685    13916.10\n",
      "      130  Completed  1557    3.47298e-06  7.42604e-03  8.79533e-01     2350.3     1      6.01563    4700.60\n",
      "      131    Stopped   832    3.93122e-06  2.84169e-03  8.03730e-01    3410.76     1      5.90847    6821.51\n",
      "      132  Completed  1143    1.47892e-06  2.60092e-03  1.37607e-01     376.49     1      3.84716     752.98\n",
      "      133  Completed   836    6.15562e-06  4.10578e-03  9.83998e-01    9514.18     1      0.48635    19028.36\n",
      "      134  Completed  1443    9.93953e-06  9.59821e-03  1.79557e-01   10992.07     1      3.93953    21984.14\n",
      "      135  Completed  2344    5.13191e-06  4.05839e-04  1.31915e-01   10901.94     1      3.22941    21803.88\n",
      "      136    Stopped  2044    7.95138e-06  5.99621e-03  1.22214e-01    4651.99     1      5.80370    9303.98\n",
      "      137    Stopped  2107    2.76595e-06  5.33673e-03  5.22807e-01    3201.99     1      4.46431    6403.99\n",
      "      138    Stopped   575    7.26733e-07  9.98730e-03  7.23169e-01    8197.19     1      2.85324    16394.39\n",
      "      139  Completed  1916    1.43980e-06  4.81869e-03  3.93343e-01   11075.41     1      4.39150    22150.82\n",
      "      140    Stopped  2496    7.66561e-06  5.28575e-03  8.84794e-01     859.53     1      1.77066    1719.07\n",
      "      141    Stopped   107    3.51327e-06  9.25479e-03  9.79220e-01    4564.56     1      4.25865    9129.12\n",
      "      142    Stopped  1754    9.64110e-06  5.74212e-03  5.40663e-01   10955.09     1      4.25412    21910.19\n",
      "      143  Completed  1173    5.06285e-06  7.07243e-03  9.59350e-01   10651.08     1      7.61489    21302.17\n",
      "      144  Completed   809    9.30005e-06  3.26581e-03  9.48932e-01    3520.94     1      5.37064    7041.89\n",
      "      145    Stopped  2227    6.07835e-07  3.39831e-03  2.18970e-01    9922.32     1      4.51312    19844.64\n",
      "      146  Completed   171    1.50382e-06  5.06206e-03  8.73861e-01     4175.4     1      4.36160    8350.80\n",
      "      147  Completed  1108    5.09952e-06  6.93871e-03  4.23882e-01   10631.56     1      5.27879    21263.12\n",
      "      148    Stopped   456    4.86559e-06  3.80768e-03  6.03177e-01    5085.68     1      7.72693    10171.36\n",
      "      149  Completed   646    5.15647e-06  3.44362e-03  5.36460e-01    9000.11     1      7.97013    18000.22\n",
      "      150    Stopped  1987    4.08236e-06  4.54366e-03  8.32307e-01   10391.02     1      1.59572    20782.05\n",
      "      151    Stopped  2314    3.42010e-06  9.08745e-03  5.76429e-01    2626.42     1      2.79163    5252.85\n",
      "      152    Stopped  1170    4.73582e-06  2.47079e-04  3.45226e-01    1928.82     1      0.90510    3857.64\n",
      "      153    Stopped  1834    6.14721e-06  4.79858e-03  2.81483e-01     357.02     1      2.34896     714.05\n",
      "      154    Stopped   856    2.62653e-06  1.52627e-03  4.38264e-01     9339.1     1      1.63093    18678.19\n",
      "      155  Completed   712    8.38003e-06  4.96474e-04  3.30687e-01    6771.19     1      6.98381    13542.38\n",
      "      156    Stopped   679    2.72786e-06  4.43114e-03  6.47116e-01    8779.07     1      7.79965    17558.15\n",
      "      157  Completed   627    1.47630e-06  7.99997e-03  6.26929e-01    3653.86     1      3.27422    7307.71\n",
      "      158  Completed    88    5.95893e-07  7.91324e-03  4.47897e-01    9332.37     1      3.53165    18664.75\n",
      "      159  Completed  1539    8.47959e-06  6.24164e-03  4.42267e-01    9402.64     1      0.08813    18805.28\n",
      "      160    Stopped  1003    4.00085e-06  1.18264e-03  6.00027e-01    7648.28     1      3.83153    15296.56\n",
      "      161    Stopped   657    1.44647e-06  9.64536e-04  9.75774e-01    9108.87     1      0.78815    18217.74\n",
      "      162  Completed   166    2.94963e-06  1.95705e-03  5.71325e-01    5078.59     1      5.18606    10157.17\n",
      "      163    Stopped  2298    4.67771e-06  7.44678e-03  4.34708e-01    5393.67     1      3.48243    10787.34\n",
      "      164    Stopped   788    3.85210e-06  9.64025e-03  1.50034e-01    8306.85     1      7.31252    16613.70\n",
      "      165  Completed   959    9.76625e-06  2.25824e-03  7.95924e-01    12140.8     1      0.29289    24281.60\n",
      "      166    Stopped   851    8.75651e-06  5.84709e-03  9.23249e-01    2527.06     1      1.56892    5054.13\n",
      "      167  Completed   705    4.37240e-06  8.83100e-03  4.56967e-01   11945.44     1      7.07740    23890.89\n",
      "      168    Stopped  1560    5.57323e-06  3.27339e-03  3.12649e-01   11992.12     1      0.94402    23984.24\n",
      "      169    Stopped  1420    7.45873e-06  2.39102e-03  2.49854e-01    5330.16     1      6.78746    10660.31\n",
      "      170    Stopped  1841    5.47876e-06  1.72812e-03  8.99797e-01    3084.71     1      3.26751    6169.41\n",
      "      171  Completed  1162    5.79865e-06  1.72666e-03  6.98199e-01     735.94     1      5.22346    1471.88\n",
      "      172    Stopped  1919    2.12708e-08  7.21205e-03  8.35874e-01    7413.84     1      6.57275    14827.68\n",
      "      173  Completed  2291    7.54323e-06  5.03269e-03  8.65622e-01   10795.35     1      2.22933    21590.70\n",
      "      174  Completed  1480    4.44224e-06  7.86137e-03  4.52062e-01    5321.33     1      6.32144    10642.66\n",
      "      175    Stopped   562    3.09094e-06  8.19653e-03  1.22203e-01    7274.95     1      0.58805    14549.90\n",
      "      176    Stopped  1125    6.90886e-06  9.18660e-04  3.66888e-01    4661.79     1      6.08829    9323.57\n",
      "      177  Completed   195    8.84848e-06  7.16577e-03  5.41898e-01   10525.27     1      1.03387    21050.53\n",
      "      178    Stopped   602    4.43645e-06  2.49586e-03  2.73029e-01    4964.66     1      0.26393    9929.32\n",
      "      179  Completed  1976    9.17967e-06  6.06609e-03  9.78217e-01    3931.13     1      5.01785    7862.26\n",
      "      180  Completed   700    9.61151e-06  3.61541e-04  9.81336e-01   10092.64     1      4.16196    20185.29\n",
      "      181    Stopped  1504    5.01608e-06  7.84659e-03  1.82695e-01   11877.18     1      6.21092    23754.35\n",
      "      182    Stopped  1017    2.91122e-06  7.28293e-03  4.48280e-01    7199.83     1      5.00420    14399.66\n",
      "      183  Completed   870    6.51225e-06  7.39091e-03  8.74588e-01    2470.97     1      2.82256    4941.94\n",
      "      184    Stopped  1103    9.67540e-07  7.82233e-03  3.41762e-01    2606.51     1      7.38844    5213.03\n",
      "      185    Stopped   761    6.79849e-06  2.55001e-03  3.06103e-01     8956.8     1      6.60023    17913.59\n",
      "      186  Completed  1607    2.54703e-06  7.16376e-03  9.93864e-01    8902.05     1      5.04777    17804.10\n",
      "      187    Stopped  1986    7.05850e-06  1.94080e-03  6.26056e-01   10419.85     1      6.20293    20839.71\n",
      "      188    Stopped  1362    6.14456e-06  9.25183e-03  8.01084e-01    9824.47     1      5.18308    19648.95\n",
      "      189    Stopped  1438    1.90167e-06  2.23542e-03  4.01951e-01     2034.2     1      2.52891    4068.39\n",
      "      190  Completed   759    7.61498e-06  4.56867e-05  7.17631e-01     2698.0     1      0.63421    5395.99\n",
      "      191  Completed  1174    6.77026e-06  9.92918e-03  8.00798e-01    1368.44     1      0.21521    2736.88\n",
      "      192    Stopped  1763    8.38080e-06  3.41806e-03  5.13313e-01     7119.7     1      3.69942    14239.40\n",
      "      193  Completed  1177    7.04130e-06  6.80693e-03  3.45361e-01    8974.65     1      6.99852    17949.30\n",
      "      194    Stopped   180    1.70132e-06  9.59721e-03  8.16155e-01    8247.25     1      2.88014    16494.51\n",
      "      195    Stopped  1017    8.11785e-06  7.87192e-03  1.60149e-01    1975.42     1      4.44269    3950.84\n",
      "      196    Stopped   296    5.65232e-06  7.19609e-03  5.51688e-01   12163.47     1      2.74414    24326.95\n",
      "      197    Stopped  1218    2.12014e-06  2.77220e-03  2.56412e-01    2587.16     1      7.07727    5174.32\n",
      "      198  Completed  2219    6.86986e-06  3.27096e-03  6.01081e-01    1539.19     1      2.81423    3078.37\n",
      "      199    Stopped  2050    3.39544e-06  7.87805e-03  5.38735e-01    7845.39     1      7.91333    15690.78\n",
      "      200  Completed   624    8.87522e-06  7.14831e-03  8.49753e-01    6659.22     1      2.32750    13318.43\n",
      "      201    Stopped  1937    5.45394e-07  5.30921e-03  3.17240e-01    3167.48     1      2.05846    6334.97\n",
      "      202    Stopped  1810    3.59108e-06  7.90609e-03  1.03777e-01   11853.38     1      7.84746    23706.77\n",
      "      203    Stopped  1219    6.30095e-06  2.05432e-06  1.73082e-01   10469.09     1      7.37932    20938.19\n",
      "      204    Stopped  1113    5.16491e-06  6.66410e-03  3.11921e-01    1767.48     1      5.52384    3534.96\n",
      "      205  Completed  1022    8.08740e-06  5.33601e-04  2.00422e-01    9193.46     1      0.17013    18386.91\n",
      "      206    Stopped  1343    6.42238e-06  6.33178e-03  7.05755e-01    9150.06     1      3.20794    18300.12\n",
      "      207  Completed  1515    3.00016e-06  1.91195e-03  5.57219e-01     533.18     1      4.44708    1066.36\n",
      "      208    Stopped   964    1.38318e-06  5.34546e-03  6.30020e-01    6635.58     1      0.78403    13271.16\n",
      "      209    Stopped  2225    9.10591e-06  3.76221e-03  4.65385e-01    7753.97     1      0.60265    15507.93\n",
      "      210  Completed  1556    7.55468e-06  2.88637e-03  1.68702e-01    4180.03     1      5.79163    8360.06\n",
      "      211    Stopped   171    4.04363e-06  1.01893e-03  4.44756e-01    1752.36     1      4.46433    3504.72\n",
      "      212  Completed   134    7.26360e-06  6.52544e-03  9.76838e-01    3080.17     1      4.86081    6160.34\n",
      "      213    Stopped   467    4.52326e-06  4.68101e-03  4.62515e-01     2274.5     1      2.45322    4548.99\n",
      "      214  Completed  2465    7.07735e-06  3.39519e-03  9.17643e-01    6060.78     1      0.97360    12121.55\n",
      "      215  Completed  1120    9.84955e-06  6.29784e-03  1.54985e-01    3606.87     1      3.15153    7213.74\n",
      "      216  Completed   448    7.04311e-06  6.20165e-03  7.75705e-01    8663.28     1      3.49560    17326.56\n",
      "      217    Stopped   781    1.41928e-06  2.40365e-03  2.92774e-01    7221.53     1      5.12049    14443.06\n",
      "      218  Completed  1546    4.32960e-06  3.60356e-03  6.39329e-01    5084.35     1      6.66948    10168.70\n",
      "      219    Stopped  1669    5.98888e-06  8.63824e-03  9.59465e-01   10007.99     1      1.99362    20015.97\n",
      "      220    Stopped  1632    3.43679e-06  4.00790e-04  3.40367e-01    6820.45     1      3.16579    13640.90\n",
      "      221    Stopped  1398    6.72243e-06  7.88111e-03  8.13498e-01    8930.47     1      7.92158    17860.95\n",
      "      222  Completed  1289    8.04918e-06  3.73450e-03  3.12022e-01   11524.21     1      7.97152    23048.41\n",
      "      223    Stopped  2330    4.47352e-06  8.61220e-03  1.21254e-01    9950.67     1      5.33649    19901.35\n",
      "      224    Stopped  1446    3.68292e-06  6.00554e-04  6.48596e-01   10004.84     1      7.77043    20009.69\n",
      "      225    Stopped   363    4.59054e-06  2.68553e-03  5.53559e-01    2022.52     1      1.07632    4045.05\n",
      "      226  Completed   891    2.31983e-06  4.36246e-03  5.14961e-01    9008.95     1      5.31948    18017.91\n",
      "      227  Completed  2188    4.44333e-06  3.83674e-03  2.59253e-01     8769.2     1      3.50871    17538.40\n",
      "      228  Completed  2268    4.65299e-06  4.25137e-03  1.24970e-01    5670.76     1      2.99719    11341.53\n",
      "      229    Stopped  1821    6.81075e-06  1.02437e-05  1.81978e-01    8885.46     1      0.65413    17770.91\n",
      "      230  Completed   673    5.10670e-07  7.34200e-03  1.45681e-01    4616.07     1      4.39831    9232.14\n",
      "      231    Stopped   808    8.68663e-06  3.01028e-03  2.36918e-01    8044.63     1      5.77214    16089.26\n",
      "      232  Completed  2022    3.82468e-06  4.65182e-04  8.22171e-01    3824.72     1      3.34196    7649.45\n",
      "      233  Completed  2258    5.35861e-07  1.74288e-03  8.31864e-01    3603.74     1      1.65920    7207.48\n",
      "      234    Stopped   636    1.26800e-06  6.49282e-03  2.59621e-01    4632.42     1      -0.03665    9264.83\n",
      "      235  Completed  1126    6.10943e-06  6.63014e-03  8.32669e-01    2123.91     1      2.77844    4247.83\n",
      "      236  Completed  2485    4.89675e-06  4.76509e-03  7.04109e-01    9521.74     1      4.73549    19043.49\n",
      "      237    Stopped  1841    5.40363e-06  8.17251e-03  7.49426e-01    9765.73     1      5.64219    19531.46\n",
      "      238  Completed  1586    9.41573e-08  1.04631e-03  7.28852e-01    1746.12     1      3.08431    3492.23\n",
      "      239  Completed   859    5.85617e-06  3.61603e-03  7.91159e-01    3953.12     1      0.47380    7906.25\n",
      "      240    Stopped  2034    5.03585e-06  1.38059e-03  7.14520e-01    8967.66     1      5.00837    17935.32\n",
      "      241    Stopped  1686    4.11000e-07  2.82189e-03  7.13220e-01    9481.74     1      0.49629    18963.48\n",
      "      242    Stopped   933    5.26300e-07  8.58912e-03  3.01543e-01    5211.36     1      6.07601    10422.72\n",
      "      243    Stopped   478    9.54193e-06  7.76529e-03  4.43583e-01   10637.86     1      3.99196    21275.72\n",
      "      244  Completed  2057    2.59349e-06  8.08087e-03  4.49747e-01    3227.68     1      7.80750    6455.36\n",
      "      245    Stopped   573    4.26463e-07  7.63080e-03  1.62799e-01    3242.17     1      3.22641    6484.35\n",
      "      246    Stopped  1540    1.90519e-06  7.61357e-03  2.27222e-01    6738.34     1      7.04434    13476.67\n",
      "      247    Stopped  2496    6.91815e-07  8.07868e-03  7.64705e-01    8848.86     1      2.34264    17697.73\n",
      "      248  Completed  1180    7.24415e-06  3.11981e-04  6.63612e-01    7848.73     1      7.26847    15697.45\n",
      "      249  Completed  1763    1.64369e-06  9.40451e-03  2.80132e-01    1003.65     1      4.32912    2007.30\n",
      "      250  Completed   495    1.24574e-07  9.87554e-03  6.82253e-01    9234.68     1      6.99785    18469.36\n",
      "      251    Stopped   194    3.84303e-06  9.33745e-03  5.60583e-01     9196.5     1      6.52543    18393.00\n",
      "      252  Completed   368    9.82789e-06  3.49948e-03  1.94969e-01    9560.33     1      6.94045    19120.67\n",
      "      253  Completed  2131    9.26640e-07  8.85686e-03  4.82228e-01    1591.65     1      6.74860    3183.31\n",
      "      254    Stopped  1396    4.39333e-06  4.66159e-03  9.84911e-01     5212.1     1      7.28122    10424.20\n",
      "      255  Completed  1402    8.73819e-06  8.28074e-03  2.51979e-01    5392.37     1      0.67369    10784.74\n",
      "      256  Completed  2001    3.05018e-06  1.36390e-03  4.47278e-01    1424.31     1      0.14264    2848.61\n",
      "      257  Completed  1928    7.52898e-06  8.80997e-03  1.96400e-01    6539.71     1      6.19131    13079.42\n",
      "      258    Stopped  2453    7.53963e-06  6.21264e-03  3.95551e-01    6249.88     1      1.70934    12499.75\n",
      "      259  Completed   796    2.32327e-06  4.31996e-03  8.13334e-01    4950.51     1      5.65935    9901.01\n",
      "      260    Stopped   475    8.25941e-07  8.58729e-03  5.19428e-01    6481.71     1      4.68538    12963.41\n",
      "      261  Completed  1451    2.27442e-06  6.25719e-03  8.71433e-01    10665.5     1      3.59801    21331.00\n",
      "      262    Stopped  1003    2.29662e-07  2.21798e-03  6.43331e-01    7823.23     1      2.05964    15646.47\n",
      "      263    Stopped  1746    8.70898e-06  3.07343e-03  6.59274e-01     7295.2     1      3.16960    14590.39\n",
      "      264    Stopped  1369    1.06644e-06  1.72372e-03  7.97102e-01    4563.88     1      4.63451    9127.75\n",
      "      265  Completed  1197    6.61343e-06  1.45115e-03  4.82931e-01    6741.85     1      5.50086    13483.70\n",
      "      266  Completed   545    7.14578e-06  8.48512e-03  8.32915e-01    9788.72     1      7.46393    19577.44\n",
      "      267  Completed   769    6.53755e-06  9.94118e-04  1.16153e-01    7051.91     1      4.15141    14103.82\n",
      "      268  Completed   451    4.69322e-06  9.58752e-03  1.70084e-01    5295.24     1      6.66744    10590.47\n",
      "      269    Stopped    39    4.94640e-06  6.71043e-03  6.44682e-01    3164.16     1      0.68128    6328.32\n",
      "      270  Completed  1311    6.80564e-06  8.88899e-04  1.79868e-01    11287.8     1      7.53795    22575.60\n",
      "      271  Completed   439    7.84821e-06  9.56146e-03  4.11567e-01   10569.05     1      1.72337    21138.11\n",
      "      272    Stopped   281    5.70602e-06  6.01541e-03  7.43975e-01    1206.39     1      7.65892    2412.78\n",
      "      273  Completed   392    2.88150e-06  5.63591e-03  8.65240e-01    6961.32     1      7.17937    13922.65\n",
      "      274  Completed  1450    3.54170e-06  7.44232e-05  9.26115e-01     2662.7     1      3.75478    5325.41\n",
      "      275  Completed  1223    2.74802e-06  5.31268e-03  7.37388e-01    5887.76     1      5.18028    11775.53\n",
      "      276  Completed   302    1.28646e-06  2.14318e-03  5.77149e-01    10987.1     1      0.80768    21974.20\n",
      "      277  Completed   298    1.50104e-06  2.61327e-03  5.32515e-01    1168.43     1      0.15280    2336.85\n",
      "      278    Stopped  1972    5.89111e-08  5.04696e-03  4.40020e-01    7670.55     1      4.61624    15341.09\n",
      "      279    Stopped   158    8.60080e-06  5.33115e-03  6.81625e-01    8341.44     1      5.72891    16682.89\n",
      "      280    Stopped  1460    4.41431e-07  2.94154e-03  3.05320e-01    2545.96     1      1.84892    5091.91\n",
      "      281  Completed   550    9.71950e-06  9.31098e-03  4.60177e-01    5629.38     1      5.74897    11258.76\n",
      "      282    Stopped   943    3.83356e-06  4.63986e-03  8.61516e-01    9401.73     1      5.69608    18803.46\n",
      "      283  Completed  1072    3.43502e-06  1.45704e-03  6.02325e-01    9263.02     1      5.41061    18526.03\n",
      "      284  Completed   159    8.47055e-06  9.83725e-03  3.19245e-01    8025.82     1      7.66707    16051.64\n",
      "      285  Completed   629    1.79098e-06  6.59604e-03  5.10016e-01    8215.97     1      2.46401    16431.94\n",
      "      286  Completed  1447    8.63906e-06  2.21163e-04  6.58666e-01    2485.26     1      6.94226    4970.52\n",
      "      287    Stopped  1018    4.63202e-06  5.09598e-03  6.43127e-01    8077.41     1      5.95305    16154.83\n",
      "      288    Stopped   827    9.40221e-06  3.17826e-04  8.03918e-01    7974.71     1      0.85549    15949.41\n",
      "      289  Completed  1165    5.93144e-06  4.79863e-04  6.91788e-01   10779.64     1      5.85144    21559.28\n",
      "      290  Completed   482    9.80731e-06  9.56363e-04  7.17852e-01    5374.57     1      2.66106    10749.14\n",
      "      291    Stopped  1355    7.14078e-07  2.61492e-03  3.39487e-01    9780.26     1      2.81230    19560.52\n",
      "      292  Completed   590    7.65494e-06  8.19817e-03  4.76830e-01    5086.84     1      5.96230    10173.67\n",
      "0 trials running, 293 finished (293 until the end), 61207.29s wallclock-time\n",
      "\n",
      "mean_reward: best 7.686551213264465 for trial-id 30\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "# Start hyperparameter tuning\n",
    "tuner.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get results\n",
    "tuner_path = tuner.tuner_path\n",
    "tuning_experiment = load_experiment(tuner_path)\n",
    "best_run = tuning_experiment.best_config()\n",
    "tuning_experiment.results.to_csv(\"artifacts/tune_logs/tuning_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tuning jobs: 65004\n",
      "Best mean reward: 75.6865\n",
      "Best tuning configuration:\n",
      " - config_learning_rate: 5.6672e-05\n",
      " - config_tau: 7.2524e-06\n",
      " - config_gamma: 9.7588e-01\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of tuning jobs:\", len(tuning_experiment.results))\n",
    "print(\"Best mean reward:\", round(best_run[\"mean_reward\"], 4))\n",
    "print(\"Best tuning configuration:\")\n",
    "for i in [\"config_learning_rate\", \"config_tau\", \"config_gamma\"]:\n",
    "    print(f\" - {i}: {best_run[i]:.4e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.noise import NormalActionNoise\n",
    "from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnRewardThreshold\n",
    "from stable_baselines3 import TD3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the vectorized environment\n",
    "env_id = \"CarRacing-v2\"\n",
    "log_dir = \"./artifacts/train_logs/\"\n",
    "env = gym.make(env_id, domain_randomize=False, render_mode=\"rgb_array\") \n",
    "env = Monitor(env, log_dir, allow_early_resets=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n"
     ]
    }
   ],
   "source": [
    "# Initialize optimal hyperparameters\n",
    "n_actions = env.action_space.shape[-1]\n",
    "action_noise = NormalActionNoise(mean=np.zeros(n_actions), sigma=0.1 * np.ones(n_actions))  \n",
    "learning_rate = best_run.config_learning_rate\n",
    "tau = best_run.config_tau\n",
    "gamma = best_run.config_gamma\n",
    "# Create the TD3 Agent\n",
    "model = TD3(\n",
    "    \"CnnPolicy\", \n",
    "    env,  \n",
    "    action_noise=action_noise,\n",
    "    learning_rate=0.001,\n",
    "    tau=0.5,\n",
    "    gamma=gamma,\n",
    "    batch_size=32,\n",
    "    verbose=1,\n",
    "    device=\"cuda\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use DummyVecEnv to create a vectorized environment\n",
    "eval_env = Monitor(gym.make('CarRacing-v2'), log_dir)\n",
    "# Define callback for early stopping and include it in the eval callback\n",
    "callback_on_best = StopTrainingOnRewardThreshold(reward_threshold=500, verbose=1)\n",
    "eval_callback = EvalCallback(\n",
    "    eval_env, \n",
    "    callback_on_new_best=callback_on_best, \n",
    "    verbose=1, \n",
    "    best_model_save_path=\"./artifacts/model\",\n",
    "    log_path=\"./artifacts/logs/\", \n",
    "    eval_freq=1000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adavidho/miniconda3/envs/rl/lib/python3.10/site-packages/stable_baselines3/common/callbacks.py:414: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.vec_transpose.VecTransposeImage object at 0x7effd97c34c0> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x7f00103b3b80>\n",
      "  warnings.warn(\"Training and eval env are not of the same type\" f\"{self.training_env} != {self.eval_env}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " /home/adavidho/miniconda3/envs/rl/lib/python3.10/site-packages/stable_baselines3/common/callbacks.py:414: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.vec_transpose.VecTransposeImage object at 0x7efea0e82a10> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x7f001074bac0> \n",
      "   warnings.warn(\"Training and eval env are not of the same type\" f\"{self.training_env} != {self.eval_env}\") \n",
      " Eval num_timesteps=1000, episode_reward=--99.57 +/- 1.5337690882966362 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -99.57  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 1000     | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.3848   | \n",
      " |    critic_loss     | 1.3462   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 899      | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=2000, episode_reward=--99.48 +/- 1.4203702697201341 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -99.48  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 2000     | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.4851   | \n",
      " |    critic_loss     | 1.6213   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 1899     | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=3000, episode_reward=--98.89 +/- 1.7442667845396127 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -98.89  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 3000     | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.6249   | \n",
      " |    critic_loss     | 1.6562   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 2899     | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=4000, episode_reward=--97.7 +/- 1.2368646393598706 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   -97.7  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 4000     | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 7.9877   | \n",
      " |    critic_loss     | 1.9969   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 3899     | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=5000, episode_reward=--97.26 +/- 1.824201903722336 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -97.26  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 5000     | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.7497   | \n",
      " |    critic_loss     | 1.1874   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 4899     | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=6000, episode_reward=--97.39 +/- 1.7492999040317017 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -97.39  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 6000     | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.0634   | \n",
      " |    critic_loss     | 1.5158   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 5899     | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=7000, episode_reward=--96.93 +/- 1.37084059303713 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -96.93  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 7000     | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 7.7946   | \n",
      " |    critic_loss     | 1.9486   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 6899     | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=8000, episode_reward=--96.34 +/- 1.4725001430258227 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -96.34  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 8000     | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.7642   | \n",
      " |    critic_loss     | 1.1911   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 7899     | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=9000, episode_reward=--95.64 +/- 1.9914658103498502 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -95.64  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 9000     | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.4052   | \n",
      " |    critic_loss     | 1.3513   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 8899     | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=10000, episode_reward=--95.2 +/- 1.4285685990474668 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   -95.2  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 10000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.8837   | \n",
      " |    critic_loss     | 1.2209   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 9899     | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=11000, episode_reward=--94.04 +/- 1.533482626437959 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -94.04  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 11000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.6923   | \n",
      " |    critic_loss     | 1.1731   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 10899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=12000, episode_reward=--94.0 +/- 1.8582603442057197 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   -94.0  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 12000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 7.9326   | \n",
      " |    critic_loss     | 1.9832   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 11899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=13000, episode_reward=--93.5 +/- 1.8526991585688268 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   -93.5  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 13000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.1157   | \n",
      " |    critic_loss     | 1.5289   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 12899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=14000, episode_reward=--93.46 +/- 1.7381673020069286 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -93.46  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 14000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.8287   | \n",
      " |    critic_loss     | 1.4572   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 13899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=15000, episode_reward=--92.42 +/- 1.5709266774261093 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -92.42  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 15000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.0356   | \n",
      " |    critic_loss     | 1.0089   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 14899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=16000, episode_reward=--92.38 +/- 1.169118760984877 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -92.38  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 16000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.3562   | \n",
      " |    critic_loss     | 1.0890   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 15899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=17000, episode_reward=--91.62 +/- 1.1767687922480872 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -91.62  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 17000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.1120   | \n",
      " |    critic_loss     | 1.5280   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 16899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=18000, episode_reward=--91.08 +/- 1.3662179863978992 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -91.08  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 18000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.3056   | \n",
      " |    critic_loss     | 1.3264   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 17899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=19000, episode_reward=--90.37 +/- 1.1317025646141103 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -90.37  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 19000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.8006   | \n",
      " |    critic_loss     | 1.7001   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 18899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=20000, episode_reward=--89.72 +/- 1.8269224335247536 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -89.72  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 20000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.0119   | \n",
      " |    critic_loss     | 1.0030   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 19899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=21000, episode_reward=--89.54 +/- 1.9068128211097064 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -89.54  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 21000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 7.3794   | \n",
      " |    critic_loss     | 1.8449   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 20899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=22000, episode_reward=--89.39 +/- 1.7837339691133738 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -89.39  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 22000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.2319   | \n",
      " |    critic_loss     | 1.0580   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 21899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=23000, episode_reward=--88.01 +/- 1.9357849904767983 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -88.01  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 23000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.6014   | \n",
      " |    critic_loss     | 1.1504   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 22899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=24000, episode_reward=--88.21 +/- 1.6694055024962133 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -88.21  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 24000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.9012   | \n",
      " |    critic_loss     | 1.2253   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 23899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=25000, episode_reward=--87.88 +/- 1.489922911032012 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -87.88  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 25000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.4195   | \n",
      " |    critic_loss     | 1.1049   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 24899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=26000, episode_reward=--86.62 +/- 1.323330988198137 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -86.62  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 26000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 7.0182   | \n",
      " |    critic_loss     | 1.7546   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 25899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=27000, episode_reward=--86.08 +/- 1.0216916209515055 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -86.08  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 27000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.0497   | \n",
      " |    critic_loss     | 1.0124   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 26899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=28000, episode_reward=--86.47 +/- 1.3189268460054329 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -86.47  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 28000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.6969   | \n",
      " |    critic_loss     | 1.1742   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 27899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=29000, episode_reward=--85.92 +/- 1.5720245564881439 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -85.92  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 29000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.5122   | \n",
      " |    critic_loss     | 1.3781   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 28899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=30000, episode_reward=--85.22 +/- 1.32942831823593 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -85.22  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 30000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.8295   | \n",
      " |    critic_loss     | 1.7074   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 29899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=31000, episode_reward=--84.96 +/- 1.566273318248791 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -84.96  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 31000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.2548   | \n",
      " |    critic_loss     | 1.5637   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 30899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=32000, episode_reward=--83.56 +/- 1.0935605011091067 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -83.56  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 32000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.9085   | \n",
      " |    critic_loss     | 1.7271   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 31899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=33000, episode_reward=--83.5 +/- 1.080751034218603 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   -83.5  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 33000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.8108   | \n",
      " |    critic_loss     | 1.4527   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 32899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=34000, episode_reward=--83.38 +/- 1.3446770249626974 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -83.38  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 34000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.7674   | \n",
      " |    critic_loss     | 1.4419   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 33899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=35000, episode_reward=--82.35 +/- 1.785865991138833 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -82.35  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 35000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.0839   | \n",
      " |    critic_loss     | 1.2710   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 34899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=36000, episode_reward=--81.72 +/- 1.7828421168456459 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -81.72  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 36000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 7.5539   | \n",
      " |    critic_loss     | 1.8885   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 35899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=37000, episode_reward=--81.48 +/- 1.183922884012997 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -81.48  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 37000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.8918   | \n",
      " |    critic_loss     | 1.4730   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 36899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=38000, episode_reward=--81.23 +/- 1.9004344575630094 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -81.23  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 38000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.7016   | \n",
      " |    critic_loss     | 1.6754   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 37899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=39000, episode_reward=--80.31 +/- 1.7116661036403569 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -80.31  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 39000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 7.3368   | \n",
      " |    critic_loss     | 1.8342   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 38899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=40000, episode_reward=--80.22 +/- 1.7656909988328633 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -80.22  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 40000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.7840   | \n",
      " |    critic_loss     | 1.6960   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 39899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=41000, episode_reward=--79.48 +/- 1.5603414833376832 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -79.48  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 41000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.0663   | \n",
      " |    critic_loss     | 1.2666   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 40899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=42000, episode_reward=--79.04 +/- 1.6343299708354937 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -79.04  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 42000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.7175   | \n",
      " |    critic_loss     | 1.6794   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 41899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=43000, episode_reward=--78.19 +/- 1.8469797532494652 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -78.19  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 43000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.2229   | \n",
      " |    critic_loss     | 1.3057   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 42899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=44000, episode_reward=--78.23 +/- 1.7786219757269146 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -78.23  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 44000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 7.8194   | \n",
      " |    critic_loss     | 1.9548   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 43899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=45000, episode_reward=--77.06 +/- 1.5682629667954713 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -77.06  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 45000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.9894   | \n",
      " |    critic_loss     | 1.7474   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 44899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=46000, episode_reward=--76.85 +/- 1.2276956459589923 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -76.85  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 46000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.0653   | \n",
      " |    critic_loss     | 1.2663   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 45899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=47000, episode_reward=--76.67 +/- 1.0382700166919707 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -76.67  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 47000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.8959   | \n",
      " |    critic_loss     | 1.2240   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 46899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=48000, episode_reward=--76.2 +/- 1.4460416505144371 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   -76.2  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 48000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.8092   | \n",
      " |    critic_loss     | 1.2023   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 47899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=49000, episode_reward=--75.86 +/- 1.324338012513101 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -75.86  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 49000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.9956   | \n",
      " |    critic_loss     | 1.4989   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 48899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=50000, episode_reward=--74.99 +/- 1.455835371539664 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -74.99  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 50000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.6528   | \n",
      " |    critic_loss     | 1.1632   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 49899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=51000, episode_reward=--74.5 +/- 1.3274828805850611 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   -74.5  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 51000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 7.4638   | \n",
      " |    critic_loss     | 1.8660   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 50899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=52000, episode_reward=--73.54 +/- 1.0632868278500167 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -73.54  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 52000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.6707   | \n",
      " |    critic_loss     | 1.6677   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 51899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=53000, episode_reward=--73.67 +/- 1.8381475498832343 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -73.67  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 53000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.3359   | \n",
      " |    critic_loss     | 1.3340   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 52899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=54000, episode_reward=--73.48 +/- 1.824324882687887 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -73.48  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 54000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.9291   | \n",
      " |    critic_loss     | 1.7323   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 53899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=55000, episode_reward=--72.07 +/- 1.3306286669196972 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -72.07  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 55000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.0769   | \n",
      " |    critic_loss     | 1.2692   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 54899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=56000, episode_reward=--72.02 +/- 1.8575551887973156 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -72.02  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 56000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.6866   | \n",
      " |    critic_loss     | 1.4216   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 55899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=57000, episode_reward=--71.64 +/- 1.9396100392200992 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -71.64  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 57000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 7.0539   | \n",
      " |    critic_loss     | 1.7635   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 56899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=58000, episode_reward=--71.13 +/- 1.0793623781062522 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -71.13  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 58000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 7.3747   | \n",
      " |    critic_loss     | 1.8437   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 57899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=59000, episode_reward=--70.74 +/- 1.6791602186864143 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -70.74  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 59000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.1390   | \n",
      " |    critic_loss     | 1.0347   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 58899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=60000, episode_reward=--69.69 +/- 1.6132250815071303 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -69.69  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 60000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.9930   | \n",
      " |    critic_loss     | 1.7483   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 59899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=61000, episode_reward=--69.41 +/- 1.7448890446373366 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -69.41  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 61000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 7.0321   | \n",
      " |    critic_loss     | 1.7580   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 60899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=62000, episode_reward=--68.7 +/- 1.671648120124296 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   -68.7  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 62000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.5093   | \n",
      " |    critic_loss     | 1.1273   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 61899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=63000, episode_reward=--68.02 +/- 1.3156652123835597 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -68.02  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 63000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.4002   | \n",
      " |    critic_loss     | 1.1000   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 62899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=64000, episode_reward=--68.4 +/- 1.9842886648372924 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   -68.4  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 64000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.2752   | \n",
      " |    critic_loss     | 1.5688   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 63899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=65000, episode_reward=--67.78 +/- 1.8502782068241395 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -67.78  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 65000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 7.5977   | \n",
      " |    critic_loss     | 1.8994   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 64899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=66000, episode_reward=--66.71 +/- 1.3588174493511878 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -66.71  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 66000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.7665   | \n",
      " |    critic_loss     | 0.9416   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 65899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=67000, episode_reward=--66.72 +/- 1.6722643195270783 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -66.72  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 67000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.2884   | \n",
      " |    critic_loss     | 1.5721   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 66899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=68000, episode_reward=--66.25 +/- 1.6018595328262328 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -66.25  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 68000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.3258   | \n",
      " |    critic_loss     | 1.0814   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 67899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=69000, episode_reward=--65.87 +/- 1.4114421626884215 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -65.87  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 69000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.4591   | \n",
      " |    critic_loss     | 1.1148   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 68899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=70000, episode_reward=--65.16 +/- 1.2769390933176525 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -65.16  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 70000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.2511   | \n",
      " |    critic_loss     | 1.0628   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 69899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=71000, episode_reward=--64.36 +/- 1.8384288997704847 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -64.36  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 71000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.2908   | \n",
      " |    critic_loss     | 1.3227   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 70899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=72000, episode_reward=--63.76 +/- 1.82845564068076 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -63.76  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 72000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.7013   | \n",
      " |    critic_loss     | 1.6753   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 71899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=73000, episode_reward=--63.77 +/- 1.7085188709318717 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -63.77  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 73000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.7732   | \n",
      " |    critic_loss     | 0.9433   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 72899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=74000, episode_reward=--62.66 +/- 1.4642546601778155 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -62.66  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 74000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.4042   | \n",
      " |    critic_loss     | 1.1011   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 73899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=75000, episode_reward=--62.21 +/- 1.7468128989260956 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -62.21  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 75000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.1824   | \n",
      " |    critic_loss     | 1.0456   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 74899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=76000, episode_reward=--62.37 +/- 1.7107955732955242 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -62.37  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 76000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.4085   | \n",
      " |    critic_loss     | 1.6021   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 75899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=77000, episode_reward=--61.37 +/- 1.4450208573584868 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -61.37  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 77000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.3149   | \n",
      " |    critic_loss     | 1.5787   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 76899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=78000, episode_reward=--61.19 +/- 1.0099297540763663 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -61.19  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 78000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.0943   | \n",
      " |    critic_loss     | 1.0236   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 77899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=79000, episode_reward=--60.11 +/- 1.5582623065109948 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -60.11  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 79000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.2568   | \n",
      " |    critic_loss     | 1.0642   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 78899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=80000, episode_reward=--60.33 +/- 1.4716901781129672 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -60.33  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 80000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.6318   | \n",
      " |    critic_loss     | 1.1579   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 79899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=81000, episode_reward=--59.58 +/- 1.1201636942996323 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -59.58  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 81000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.8771   | \n",
      " |    critic_loss     | 1.2193   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 80899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=82000, episode_reward=--58.6 +/- 1.0004460589172242 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   -58.6  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 82000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.3962   | \n",
      " |    critic_loss     | 1.5991   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 81899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=83000, episode_reward=--58.48 +/- 1.970448804579813 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -58.48  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 83000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.7172   | \n",
      " |    critic_loss     | 1.1793   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 82899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=84000, episode_reward=--57.62 +/- 1.2403109564666037 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -57.62  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 84000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.2491   | \n",
      " |    critic_loss     | 1.5623   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 83899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=85000, episode_reward=--57.73 +/- 1.5419633479719606 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -57.73  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 85000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.7679   | \n",
      " |    critic_loss     | 1.6920   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 84899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=86000, episode_reward=--57.48 +/- 1.6405811005504845 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -57.48  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 86000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.7062   | \n",
      " |    critic_loss     | 1.1766   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 85899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=87000, episode_reward=--56.63 +/- 1.293556940722857 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -56.63  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 87000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.6267   | \n",
      " |    critic_loss     | 1.1567   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 86899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=88000, episode_reward=--56.31 +/- 1.734306545546183 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -56.31  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 88000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.5595   | \n",
      " |    critic_loss     | 1.6399   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 87899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=89000, episode_reward=--55.29 +/- 1.0187937381876666 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -55.29  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 89000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.2797   | \n",
      " |    critic_loss     | 1.5699   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 88899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=90000, episode_reward=--55.12 +/- 1.557704694185013 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -55.12  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 90000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.2575   | \n",
      " |    critic_loss     | 1.3144   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 89899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=91000, episode_reward=--54.04 +/- 1.0962237632127292 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -54.04  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 91000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 7.5289   | \n",
      " |    critic_loss     | 1.8822   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 90899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=92000, episode_reward=--53.51 +/- 1.7941493580968924 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -53.51  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 92000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.8758   | \n",
      " |    critic_loss     | 1.2189   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 91899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=93000, episode_reward=--53.42 +/- 1.7938642023385076 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -53.42  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 93000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.8770   | \n",
      " |    critic_loss     | 0.9693   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 92899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=94000, episode_reward=--53.42 +/- 1.8705401883656476 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -53.42  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 94000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.8176   | \n",
      " |    critic_loss     | 1.2044   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 93899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=95000, episode_reward=--52.75 +/- 1.7093291279164555 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -52.75  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 95000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.8104   | \n",
      " |    critic_loss     | 1.7026   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 94899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=96000, episode_reward=--52.3 +/- 1.5988572297744907 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   -52.3  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 96000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 7.4495   | \n",
      " |    critic_loss     | 1.8624   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 95899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=97000, episode_reward=--51.63 +/- 1.2535757828082312 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -51.63  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 97000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.3763   | \n",
      " |    critic_loss     | 1.0941   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 96899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=98000, episode_reward=--50.61 +/- 1.6616542079025884 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -50.61  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 98000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.6338   | \n",
      " |    critic_loss     | 0.9084   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 97899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=99000, episode_reward=--50.82 +/- 1.859466765473296 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -50.82  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 99000    | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 7.3144   | \n",
      " |    critic_loss     | 1.8286   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 98899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=100000, episode_reward=--49.71 +/- 1.8750170310696737 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -49.71  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 100000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 7.2215   | \n",
      " |    critic_loss     | 1.8054   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 99899    | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=101000, episode_reward=--49.66 +/- 1.1478546381725807 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -49.66  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 101000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.5498   | \n",
      " |    critic_loss     | 1.6374   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 100899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=102000, episode_reward=--49.06 +/- 1.558978299901862 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -49.06  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 102000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.0158   | \n",
      " |    critic_loss     | 1.5040   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 101899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=103000, episode_reward=--48.85 +/- 1.0435614310361694 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -48.85  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 103000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.7826   | \n",
      " |    critic_loss     | 1.1957   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 102899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=104000, episode_reward=--48.23 +/- 1.320304560330126 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -48.23  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 104000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 7.0435   | \n",
      " |    critic_loss     | 1.7609   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 103899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=105000, episode_reward=--47.54 +/- 1.4938737229255312 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -47.54  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 105000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.0376   | \n",
      " |    critic_loss     | 1.2594   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 104899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=106000, episode_reward=--47.48 +/- 1.8000539496748598 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -47.48  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 106000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.1782   | \n",
      " |    critic_loss     | 1.2946   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 105899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=107000, episode_reward=--46.83 +/- 1.6038391476135039 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -46.83  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 107000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.2075   | \n",
      " |    critic_loss     | 1.5519   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 106899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=108000, episode_reward=--45.56 +/- 1.5797028552778483 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -45.56  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 108000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.5518   | \n",
      " |    critic_loss     | 1.6380   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 107899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=109000, episode_reward=--45.6 +/- 1.4400164900670305 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   -45.6  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 109000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.6509   | \n",
      " |    critic_loss     | 1.6627   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 108899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=110000, episode_reward=--45.45 +/- 1.441750898181186 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -45.45  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 110000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.7664   | \n",
      " |    critic_loss     | 0.9416   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 109899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=111000, episode_reward=--44.66 +/- 1.7732371293214055 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -44.66  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 111000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.5891   | \n",
      " |    critic_loss     | 1.1473   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 110899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=112000, episode_reward=--44.35 +/- 1.888883354836816 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -44.35  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 112000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.9812   | \n",
      " |    critic_loss     | 1.4953   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 111899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=113000, episode_reward=--43.66 +/- 1.0242490594334308 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -43.66  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 113000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.5601   | \n",
      " |    critic_loss     | 0.8900   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 112899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=114000, episode_reward=--43.49 +/- 1.8977341261905447 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -43.49  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 114000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.8185   | \n",
      " |    critic_loss     | 0.9546   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 113899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=115000, episode_reward=--42.13 +/- 1.7082690742453368 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -42.13  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 115000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 7.3364   | \n",
      " |    critic_loss     | 1.8341   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 114899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=116000, episode_reward=--42.31 +/- 1.2020132328754123 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -42.31  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 116000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.9396   | \n",
      " |    critic_loss     | 0.9849   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 115899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=117000, episode_reward=--41.34 +/- 1.7472889997029046 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -41.34  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 117000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.6166   | \n",
      " |    critic_loss     | 1.6541   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 116899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=118000, episode_reward=--40.6 +/- 1.2095395745104618 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   -40.6  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 118000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.3181   | \n",
      " |    critic_loss     | 1.5795   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 117899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=119000, episode_reward=--40.29 +/- 1.6580813053405565 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -40.29  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 119000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.4065   | \n",
      " |    critic_loss     | 1.3516   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 118899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=120000, episode_reward=--40.4 +/- 1.2666155590242476 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   -40.4  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 120000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.5442   | \n",
      " |    critic_loss     | 1.3860   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 119899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=121000, episode_reward=--39.41 +/- 1.9774242592142635 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -39.41  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 121000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.9953   | \n",
      " |    critic_loss     | 1.4988   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 120899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=122000, episode_reward=--38.59 +/- 1.7122306720581864 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -38.59  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 122000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.3420   | \n",
      " |    critic_loss     | 1.5855   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 121899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=123000, episode_reward=--38.95 +/- 1.8742303313097617 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -38.95  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 123000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.4769   | \n",
      " |    critic_loss     | 1.3692   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 122899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=124000, episode_reward=--38.41 +/- 1.5515282484440798 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -38.41  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 124000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.3630   | \n",
      " |    critic_loss     | 1.5908   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 123899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=125000, episode_reward=--37.15 +/- 1.5352533036324645 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -37.15  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 125000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.8984   | \n",
      " |    critic_loss     | 1.4746   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 124899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=126000, episode_reward=--36.79 +/- 1.0634804033483314 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -36.79  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 126000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.9452   | \n",
      " |    critic_loss     | 1.4863   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 125899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=127000, episode_reward=--36.38 +/- 1.980084443612436 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -36.38  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 127000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.3146   | \n",
      " |    critic_loss     | 1.0787   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 126899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=128000, episode_reward=--35.69 +/- 1.1446940859476744 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -35.69  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 128000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.3283   | \n",
      " |    critic_loss     | 1.5821   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 127899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=129000, episode_reward=--35.99 +/- 1.5948047294152143 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -35.99  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 129000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.4155   | \n",
      " |    critic_loss     | 1.1039   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 128899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=130000, episode_reward=--34.73 +/- 1.4360731353919798 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -34.73  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 130000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.0394   | \n",
      " |    critic_loss     | 1.5099   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 129899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=131000, episode_reward=--34.61 +/- 1.2965782941229016 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -34.61  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 131000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.0225   | \n",
      " |    critic_loss     | 1.2556   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 130899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=132000, episode_reward=--34.42 +/- 1.3817375867791593 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -34.42  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 132000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.8536   | \n",
      " |    critic_loss     | 1.7134   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 131899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=133000, episode_reward=--33.35 +/- 1.5504844674696052 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -33.35  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 133000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.5365   | \n",
      " |    critic_loss     | 1.6341   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 132899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=134000, episode_reward=--33.0 +/- 1.4116734704356322 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   -33.0  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 134000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.4602   | \n",
      " |    critic_loss     | 1.6150   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 133899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=135000, episode_reward=--32.3 +/- 1.3518511471426113 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   -32.3  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 135000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.3976   | \n",
      " |    critic_loss     | 1.5994   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 134899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=136000, episode_reward=--32.45 +/- 1.263676726986159 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -32.45  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 136000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.0218   | \n",
      " |    critic_loss     | 1.0055   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 135899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=137000, episode_reward=--31.6 +/- 1.262787616267882 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   -31.6  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 137000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.1794   | \n",
      " |    critic_loss     | 1.5448   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 136899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=138000, episode_reward=--31.44 +/- 1.8436719077015673 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -31.44  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 138000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 7.0669   | \n",
      " |    critic_loss     | 1.7667   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 137899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=139000, episode_reward=--30.15 +/- 1.835593386329828 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -30.15  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 139000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.5449   | \n",
      " |    critic_loss     | 0.8862   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 138899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=140000, episode_reward=--29.72 +/- 1.7278351054001548 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -29.72  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 140000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.4406   | \n",
      " |    critic_loss     | 1.6102   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 139899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=141000, episode_reward=--29.22 +/- 1.527887209526933 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -29.22  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 141000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.8377   | \n",
      " |    critic_loss     | 1.2094   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 140899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=142000, episode_reward=--29.24 +/- 1.1422908511385628 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -29.24  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 142000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.6401   | \n",
      " |    critic_loss     | 0.9100   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 141899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=143000, episode_reward=--28.69 +/- 1.2176381090210695 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -28.69  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 143000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.2735   | \n",
      " |    critic_loss     | 1.3184   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 142899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=144000, episode_reward=--28.12 +/- 1.593318417270949 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -28.12  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 144000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.9838   | \n",
      " |    critic_loss     | 0.9959   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 143899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=145000, episode_reward=--27.15 +/- 1.0417586182049137 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -27.15  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 145000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.0623   | \n",
      " |    critic_loss     | 1.5156   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 144899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=146000, episode_reward=--27.01 +/- 1.7328609419049934 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -27.01  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 146000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.5755   | \n",
      " |    critic_loss     | 1.6439   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 145899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=147000, episode_reward=--26.96 +/- 1.80486489795498 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -26.96  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 147000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.7784   | \n",
      " |    critic_loss     | 1.1946   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 146899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=148000, episode_reward=--26.41 +/- 1.379952047892425 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -26.41  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 148000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 7.3545   | \n",
      " |    critic_loss     | 1.8386   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 147899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=149000, episode_reward=--25.87 +/- 1.739045950263447 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -25.87  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 149000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.0684   | \n",
      " |    critic_loss     | 1.0171   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 148899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=150000, episode_reward=--24.95 +/- 1.3240309402581416 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -24.95  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 150000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.7935   | \n",
      " |    critic_loss     | 1.1984   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 149899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=151000, episode_reward=--24.6 +/- 1.0551199782948202 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   -24.6  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 151000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.8627   | \n",
      " |    critic_loss     | 1.2157   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 150899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=152000, episode_reward=--24.38 +/- 1.7392750646199007 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -24.38  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 152000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.2822   | \n",
      " |    critic_loss     | 1.3206   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 151899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=153000, episode_reward=--23.3 +/- 1.4730692241322612 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   -23.3  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 153000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 7.1036   | \n",
      " |    critic_loss     | 1.7759   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 152899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=154000, episode_reward=--22.79 +/- 1.8584441007025116 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -22.79  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 154000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.8243   | \n",
      " |    critic_loss     | 1.4561   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 153899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=155000, episode_reward=--22.74 +/- 1.702026219093738 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -22.74  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 155000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.1752   | \n",
      " |    critic_loss     | 1.2938   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 154899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=156000, episode_reward=--21.56 +/- 1.9533701526656457 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -21.56  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 156000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.6894   | \n",
      " |    critic_loss     | 1.1723   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 155899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=157000, episode_reward=--21.12 +/- 1.8141223461763505 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -21.12  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 157000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.9515   | \n",
      " |    critic_loss     | 1.7379   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 156899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=158000, episode_reward=--20.71 +/- 1.7987386272917734 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -20.71  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 158000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.9175   | \n",
      " |    critic_loss     | 1.7294   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 157899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=159000, episode_reward=--20.87 +/- 1.2521082917691018 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -20.87  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 159000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.8301   | \n",
      " |    critic_loss     | 0.9575   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 158899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=160000, episode_reward=--19.65 +/- 1.7678938745951986 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -19.65  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 160000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.4337   | \n",
      " |    critic_loss     | 1.6084   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 159899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=161000, episode_reward=--19.54 +/- 1.0448882272772768 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -19.54  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 161000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.6253   | \n",
      " |    critic_loss     | 0.9063   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 160899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=162000, episode_reward=--18.63 +/- 1.5597834360365748 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -18.63  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 162000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.6596   | \n",
      " |    critic_loss     | 1.6649   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 161899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=163000, episode_reward=--18.7 +/- 1.1976756048771886 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   -18.7  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 163000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.0189   | \n",
      " |    critic_loss     | 1.2547   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 162899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=164000, episode_reward=--18.0 +/- 1.6880958674816324 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   -18.0  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 164000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.9500   | \n",
      " |    critic_loss     | 1.7375   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 163899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=165000, episode_reward=--17.82 +/- 1.3730170337927745 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -17.82  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 165000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.7686   | \n",
      " |    critic_loss     | 0.9422   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 164899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=166000, episode_reward=--17.48 +/- 1.3696201553795095 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -17.48  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 166000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.2144   | \n",
      " |    critic_loss     | 1.5536   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 165899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=167000, episode_reward=--16.7 +/- 1.8020068324385436 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   -16.7  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 167000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 7.0423   | \n",
      " |    critic_loss     | 1.7606   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 166899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=168000, episode_reward=--15.55 +/- 1.2635059211742492 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -15.55  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 168000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.4582   | \n",
      " |    critic_loss     | 0.8646   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 167899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=169000, episode_reward=--15.3 +/- 1.4685826079072344 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   -15.3  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 169000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.5390   | \n",
      " |    critic_loss     | 1.6347   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 168899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=170000, episode_reward=--15.04 +/- 1.0488784860295506 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -15.04  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 170000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.7594   | \n",
      " |    critic_loss     | 1.6898   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 169899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=171000, episode_reward=--14.65 +/- 1.4647186868384234 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -14.65  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 171000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.8449   | \n",
      " |    critic_loss     | 1.7112   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 170899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=172000, episode_reward=--13.7 +/- 1.7720851233739037 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   -13.7  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 172000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.7917   | \n",
      " |    critic_loss     | 1.1979   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 171899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=173000, episode_reward=--13.07 +/- 1.4395139476101693 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -13.07  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 173000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.4837   | \n",
      " |    critic_loss     | 1.6209   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 172899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=174000, episode_reward=--12.67 +/- 1.5686291758343205 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -12.67  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 174000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.7898   | \n",
      " |    critic_loss     | 0.9475   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 173899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=175000, episode_reward=--12.47 +/- 1.0627108584357454 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -12.47  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 175000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 7.1641   | \n",
      " |    critic_loss     | 1.7910   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 174899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=176000, episode_reward=--12.3 +/- 1.0163039426140374 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   -12.3  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 176000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 7.0455   | \n",
      " |    critic_loss     | 1.7614   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 175899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=177000, episode_reward=--11.89 +/- 1.9003505623878527 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -11.89  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 177000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.3627   | \n",
      " |    critic_loss     | 1.0907   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 176899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=178000, episode_reward=--11.33 +/- 1.0281494523682473 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  -11.33  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 178000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.8965   | \n",
      " |    critic_loss     | 1.2241   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 177899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=179000, episode_reward=--10.8 +/- 1.072655249799964 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   -10.8  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 179000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.6457   | \n",
      " |    critic_loss     | 1.1614   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 178899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=180000, episode_reward=--10.0 +/- 1.4670309944444457 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   -10.0  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 180000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.0187   | \n",
      " |    critic_loss     | 1.5047   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 179899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=181000, episode_reward=--9.42 +/- 1.3669459196105942 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   -9.42  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 181000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.8651   | \n",
      " |    critic_loss     | 1.7163   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 180899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=182000, episode_reward=--9.03 +/- 1.136181244135547 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   -9.03  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 182000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.0598   | \n",
      " |    critic_loss     | 1.2650   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 181899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=183000, episode_reward=--8.29 +/- 1.3687766913337227 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   -8.29  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 183000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.1633   | \n",
      " |    critic_loss     | 1.0408   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 182899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=184000, episode_reward=--8.35 +/- 1.6470869829190926 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   -8.35  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 184000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.2621   | \n",
      " |    critic_loss     | 1.5655   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 183899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=185000, episode_reward=--7.92 +/- 1.7589944646138675 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   -7.92  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 185000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.6609   | \n",
      " |    critic_loss     | 1.1652   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 184899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=186000, episode_reward=--6.91 +/- 1.5777131212364288 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   -6.91  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 186000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.3658   | \n",
      " |    critic_loss     | 0.8414   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 185899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=187000, episode_reward=--6.33 +/- 1.486355593716727 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   -6.33  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 187000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.0515   | \n",
      " |    critic_loss     | 1.0129   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 186899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=188000, episode_reward=--5.71 +/- 1.3758998726638878 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   -5.71  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 188000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.9503   | \n",
      " |    critic_loss     | 1.4876   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 187899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=189000, episode_reward=--5.72 +/- 1.282923720509374 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   -5.72  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 189000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.6759   | \n",
      " |    critic_loss     | 1.4190   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 188899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=190000, episode_reward=--4.51 +/- 1.181197266552997 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   -4.51  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 190000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.4794   | \n",
      " |    critic_loss     | 0.8698   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 189899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=191000, episode_reward=--4.15 +/- 1.7726718274173001 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   -4.15  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 191000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.8177   | \n",
      " |    critic_loss     | 1.4544   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 190899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=192000, episode_reward=--4.17 +/- 1.3680003055263636 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   -4.17  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 192000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.4506   | \n",
      " |    critic_loss     | 1.6126   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 191899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=193000, episode_reward=--3.37 +/- 1.599903777935841 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   -3.37  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 193000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.0130   | \n",
      " |    critic_loss     | 1.2532   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 192899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=194000, episode_reward=--3.4 +/- 1.5801682426352581 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |    -3.4  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 194000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.4985   | \n",
      " |    critic_loss     | 1.3746   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 193899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=195000, episode_reward=--2.42 +/- 1.9025086924415482 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   -2.42  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 195000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.4115   | \n",
      " |    critic_loss     | 1.6029   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 194899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=196000, episode_reward=--1.84 +/- 1.458721220627314 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   -1.84  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 196000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.6953   | \n",
      " |    critic_loss     | 0.9238   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 195899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=197000, episode_reward=--1.41 +/- 1.3237339110183084 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   -1.41  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 197000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.3082   | \n",
      " |    critic_loss     | 0.8270   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 196899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=198000, episode_reward=--1.08 +/- 1.3837961554915506 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   -1.08  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 198000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.8520   | \n",
      " |    critic_loss     | 1.4630   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 197899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=199000, episode_reward=--0.83 +/- 1.3608435319307206 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   -0.83  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 199000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.2934   | \n",
      " |    critic_loss     | 1.5733   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 198899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=200000, episode_reward=--0.41 +/- 1.54801602392735 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   -0.41  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 200000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.7337   | \n",
      " |    critic_loss     | 0.9334   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 199899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=201000, episode_reward=-0.68 +/- 1.406019075636245 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |    0.68  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 201000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.5206   | \n",
      " |    critic_loss     | 1.1302   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 200899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=202000, episode_reward=-0.69 +/- 1.6851298589851544 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |    0.69  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 202000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.2734   | \n",
      " |    critic_loss     | 0.8183   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 201899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=203000, episode_reward=-1.82 +/- 1.3887879255663722 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |    1.82  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 203000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.8887   | \n",
      " |    critic_loss     | 1.2222   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 202899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=204000, episode_reward=-2.13 +/- 1.33337651206223 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |    2.13  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 204000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.5502   | \n",
      " |    critic_loss     | 1.1375   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 203899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=205000, episode_reward=-2.85 +/- 1.975207707016844 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |    2.85  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 205000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.1491   | \n",
      " |    critic_loss     | 1.0373   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 204899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=206000, episode_reward=-3.43 +/- 1.219572535878466 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |    3.43  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 206000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.3284   | \n",
      " |    critic_loss     | 1.0821   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 205899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=207000, episode_reward=-3.18 +/- 1.7101472087129437 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |    3.18  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 207000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.4547   | \n",
      " |    critic_loss     | 1.1137   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 206899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=208000, episode_reward=-4.15 +/- 1.2517152704920291 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |    4.15  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 208000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.7170   | \n",
      " |    critic_loss     | 1.1792   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 207899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=209000, episode_reward=-4.78 +/- 1.2953065504360164 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |    4.78  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 209000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.8954   | \n",
      " |    critic_loss     | 0.9738   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 208899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=210000, episode_reward=-4.56 +/- 1.0251141989074015 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |    4.56  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 210000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.1728   | \n",
      " |    critic_loss     | 0.7932   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 209899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=211000, episode_reward=-5.43 +/- 1.5200270679171393 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |    5.43  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 211000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.9023   | \n",
      " |    critic_loss     | 1.4756   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 210899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=212000, episode_reward=-5.53 +/- 1.2098953857643502 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |    5.53  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 212000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.2631   | \n",
      " |    critic_loss     | 1.5658   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 211899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=213000, episode_reward=-6.61 +/- 1.0308242998815933 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |    6.61  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 213000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.7269   | \n",
      " |    critic_loss     | 1.1817   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 212899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=214000, episode_reward=-6.78 +/- 1.4023274317682062 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |    6.78  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 214000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.7363   | \n",
      " |    critic_loss     | 1.6841   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 213899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=215000, episode_reward=-7.03 +/- 1.435295100735599 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |    7.03  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 215000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.3176   | \n",
      " |    critic_loss     | 1.5794   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 214899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=216000, episode_reward=-8.21 +/- 1.9799612173377454 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |    8.21  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 216000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.1881   | \n",
      " |    critic_loss     | 1.2970   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 215899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=217000, episode_reward=-8.81 +/- 1.7918226710029024 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |    8.81  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 217000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.5330   | \n",
      " |    critic_loss     | 1.3832   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 216899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=218000, episode_reward=-8.76 +/- 1.632164160940108 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |    8.76  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 218000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.4597   | \n",
      " |    critic_loss     | 0.8649   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 217899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=219000, episode_reward=-9.79 +/- 1.351139263238928 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |    9.79  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 219000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.9732   | \n",
      " |    critic_loss     | 1.7433   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 218899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=220000, episode_reward=-10.25 +/- 1.2209209803234295 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   10.25  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 220000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.2046   | \n",
      " |    critic_loss     | 1.0511   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 219899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=221000, episode_reward=-10.56 +/- 1.4196064937862611 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   10.56  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 221000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.0391   | \n",
      " |    critic_loss     | 1.5098   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 220899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=222000, episode_reward=-10.83 +/- 1.464930856660713 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   10.83  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 222000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.2901   | \n",
      " |    critic_loss     | 1.3225   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 221899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=223000, episode_reward=-11.4 +/- 1.8007331433547666 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |    11.4  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 223000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.5660   | \n",
      " |    critic_loss     | 1.1415   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 222899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=224000, episode_reward=-11.61 +/- 1.1789331095574993 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   11.61  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 224000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.3076   | \n",
      " |    critic_loss     | 1.5769   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 223899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=225000, episode_reward=-12.49 +/- 1.4122025568909051 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   12.49  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 225000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.0145   | \n",
      " |    critic_loss     | 1.0036   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 224899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=226000, episode_reward=-12.65 +/- 1.6578308849926033 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   12.65  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 226000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.4627   | \n",
      " |    critic_loss     | 1.3657   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 225899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=227000, episode_reward=-13.96 +/- 1.214069755565807 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   13.96  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 227000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.2564   | \n",
      " |    critic_loss     | 1.0641   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 226899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=228000, episode_reward=-14.15 +/- 1.1487786813457792 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   14.15  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 228000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.9623   | \n",
      " |    critic_loss     | 1.2406   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 227899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=229000, episode_reward=-14.61 +/- 1.3155231253092312 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   14.61  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 229000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.1693   | \n",
      " |    critic_loss     | 0.7923   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 228899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=230000, episode_reward=-14.77 +/- 1.3744995248861005 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   14.77  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 230000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.7919   | \n",
      " |    critic_loss     | 1.1980   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 229899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=231000, episode_reward=-15.29 +/- 1.6914273491025582 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   15.29  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 231000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.1899   | \n",
      " |    critic_loss     | 1.0475   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 230899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=232000, episode_reward=-16.39 +/- 1.0457952259634435 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   16.39  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 232000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.3118   | \n",
      " |    critic_loss     | 1.3280   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 231899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=233000, episode_reward=-16.67 +/- 1.3701789954410426 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   16.67  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 233000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.3466   | \n",
      " |    critic_loss     | 0.8366   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 232899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=234000, episode_reward=-16.71 +/- 1.5032747374970628 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   16.71  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 234000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.0727   | \n",
      " |    critic_loss     | 1.0182   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 233899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=235000, episode_reward=-17.29 +/- 1.8311615697070196 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   17.29  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 235000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.1286   | \n",
      " |    critic_loss     | 1.0322   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 234899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=236000, episode_reward=-17.63 +/- 1.7612469318829431 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   17.63  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 236000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.7639   | \n",
      " |    critic_loss     | 0.9410   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 235899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=237000, episode_reward=-18.47 +/- 1.380825109006428 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   18.47  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 237000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.7822   | \n",
      " |    critic_loss     | 0.9455   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 236899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=238000, episode_reward=-18.93 +/- 1.8950018286847912 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   18.93  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 238000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.5308   | \n",
      " |    critic_loss     | 1.6327   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 237899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=239000, episode_reward=-19.54 +/- 1.0819607885296132 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   19.54  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 239000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.8129   | \n",
      " |    critic_loss     | 1.2032   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 238899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=240000, episode_reward=-20.05 +/- 1.3323211058733921 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   20.05  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 240000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.2967   | \n",
      " |    critic_loss     | 0.8242   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 239899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=241000, episode_reward=-20.63 +/- 1.5001778457185289 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   20.63  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 241000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.1343   | \n",
      " |    critic_loss     | 1.5336   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 240899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=242000, episode_reward=-20.97 +/- 1.0491701170494738 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   20.97  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 242000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.1688   | \n",
      " |    critic_loss     | 1.2922   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 241899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=243000, episode_reward=-21.46 +/- 1.4034905426657849 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   21.46  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 243000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.4440   | \n",
      " |    critic_loss     | 1.3610   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 242899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=244000, episode_reward=-22.4 +/- 1.5280905424087958 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |    22.4  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 244000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.1711   | \n",
      " |    critic_loss     | 1.5428   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 243899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=245000, episode_reward=-22.12 +/- 1.0400622965450466 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   22.12  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 245000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.6975   | \n",
      " |    critic_loss     | 0.9244   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 244899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=246000, episode_reward=-22.75 +/- 1.5541743021819174 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   22.75  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 246000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.3963   | \n",
      " |    critic_loss     | 1.3491   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 245899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=247000, episode_reward=-23.58 +/- 1.9111034144750043 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   23.58  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 247000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.6739   | \n",
      " |    critic_loss     | 1.4185   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 246899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=248000, episode_reward=-24.1 +/- 1.194393893794769 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |    24.1  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 248000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.2321   | \n",
      " |    critic_loss     | 0.8080   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 247899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=249000, episode_reward=-24.61 +/- 1.3162196750638877 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   24.61  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 249000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.2796   | \n",
      " |    critic_loss     | 1.3199   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 248899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=250000, episode_reward=-25.35 +/- 1.7085909061597535 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   25.35  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 250000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.7625   | \n",
      " |    critic_loss     | 0.9406   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 249899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=251000, episode_reward=-25.58 +/- 1.3930096547384698 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   25.58  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 251000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.8653   | \n",
      " |    critic_loss     | 1.2163   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 250899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=252000, episode_reward=-26.28 +/- 1.15115093113653 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   26.28  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 252000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.8861   | \n",
      " |    critic_loss     | 0.9715   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 251899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=253000, episode_reward=-26.89 +/- 1.3016131812383387 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   26.89  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 253000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.3063   | \n",
      " |    critic_loss     | 0.8266   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 252899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=254000, episode_reward=-27.09 +/- 1.7324772628849774 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   27.09  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 254000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.5435   | \n",
      " |    critic_loss     | 1.3859   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 253899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=255000, episode_reward=-27.22 +/- 1.7455499578945046 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   27.22  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 255000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.3410   | \n",
      " |    critic_loss     | 1.0852   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 254899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=256000, episode_reward=-28.26 +/- 1.4818266075885618 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   28.26  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 256000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.5319   | \n",
      " |    critic_loss     | 1.3830   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 255899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=257000, episode_reward=-28.37 +/- 1.3647384573045918 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   28.37  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 257000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.8657   | \n",
      " |    critic_loss     | 1.4664   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 256899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=258000, episode_reward=-28.71 +/- 1.238723855458793 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   28.71  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 258000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.1157   | \n",
      " |    critic_loss     | 1.5289   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 257899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=259000, episode_reward=-29.81 +/- 1.4718888170941333 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   29.81  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 259000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.3470   | \n",
      " |    critic_loss     | 1.0868   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 258899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=260000, episode_reward=-30.42 +/- 1.6538178020494172 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   30.42  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 260000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.8915   | \n",
      " |    critic_loss     | 1.7229   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 259899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=261000, episode_reward=-30.95 +/- 1.204592085247113 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   30.95  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 261000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.8084   | \n",
      " |    critic_loss     | 1.4521   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 260899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=262000, episode_reward=-30.58 +/- 1.0898538865510172 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   30.58  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 262000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.8080   | \n",
      " |    critic_loss     | 1.4520   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 261899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=263000, episode_reward=-31.67 +/- 1.6522325095548247 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   31.67  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 263000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.4428   | \n",
      " |    critic_loss     | 1.1107   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 262899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=264000, episode_reward=-32.23 +/- 1.2329671246907303 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   32.23  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 264000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.3190   | \n",
      " |    critic_loss     | 1.3298   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 263899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=265000, episode_reward=-32.64 +/- 1.2898940899909594 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   32.64  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 265000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.6404   | \n",
      " |    critic_loss     | 1.1601   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 264899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=266000, episode_reward=-33.07 +/- 1.338772214649445 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   33.07  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 266000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.0730   | \n",
      " |    critic_loss     | 1.5182   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 265899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=267000, episode_reward=-33.55 +/- 1.22685139802954 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   33.55  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 267000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.3877   | \n",
      " |    critic_loss     | 1.3469   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 266899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=268000, episode_reward=-34.27 +/- 1.873589137058676 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   34.27  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 268000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.6965   | \n",
      " |    critic_loss     | 1.6741   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 267899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=269000, episode_reward=-34.85 +/- 1.2197923911437831 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   34.85  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 269000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.5330   | \n",
      " |    critic_loss     | 1.6333   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 268899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=270000, episode_reward=-35.15 +/- 1.6032475124029544 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   35.15  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 270000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.1891   | \n",
      " |    critic_loss     | 1.5473   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 269899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=271000, episode_reward=-35.47 +/- 1.689178348384922 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   35.47  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 271000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.3471   | \n",
      " |    critic_loss     | 1.0868   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 270899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=272000, episode_reward=-36.36 +/- 1.7239413250749702 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   36.36  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 272000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.5034   | \n",
      " |    critic_loss     | 1.1258   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 271899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=273000, episode_reward=-36.52 +/- 1.8379804684005678 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   36.52  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 273000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.1343   | \n",
      " |    critic_loss     | 0.7836   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 272899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=274000, episode_reward=-37.28 +/- 1.6937780741236703 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   37.28  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 274000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.4057   | \n",
      " |    critic_loss     | 1.1014   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 273899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=275000, episode_reward=-37.12 +/- 1.499913384063628 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   37.12  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 275000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.5576   | \n",
      " |    critic_loss     | 1.1394   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 274899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=276000, episode_reward=-37.7 +/- 1.6288779501044683 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |    37.7  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 276000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.1634   | \n",
      " |    critic_loss     | 1.2908   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 275899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=277000, episode_reward=-38.14 +/- 1.2768902770600778 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   38.14  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 277000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.7013   | \n",
      " |    critic_loss     | 0.9253   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 276899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=278000, episode_reward=-38.64 +/- 1.0877824284056496 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   38.64  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 278000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.7002   | \n",
      " |    critic_loss     | 1.6751   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 277899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=279000, episode_reward=-39.89 +/- 1.1169291400793635 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   39.89  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 279000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.2339   | \n",
      " |    critic_loss     | 1.3085   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 278899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=280000, episode_reward=-39.88 +/- 1.3963891615563042 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   39.88  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 280000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.1016   | \n",
      " |    critic_loss     | 1.2754   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 279899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=281000, episode_reward=-40.53 +/- 1.9498202864551695 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   40.53  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 281000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.0336   | \n",
      " |    critic_loss     | 0.7584   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 280899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=282000, episode_reward=-40.65 +/- 1.8372441628370422 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   40.65  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 282000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.5405   | \n",
      " |    critic_loss     | 1.3851   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 281899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=283000, episode_reward=-41.17 +/- 1.3391722575833827 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   41.17  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 283000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.6544   | \n",
      " |    critic_loss     | 0.9136   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 282899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=284000, episode_reward=-42.16 +/- 1.1448527447492056 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   42.16  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 284000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.2931   | \n",
      " |    critic_loss     | 0.8233   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 283899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=285000, episode_reward=-42.26 +/- 1.4385924609563032 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   42.26  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 285000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.5242   | \n",
      " |    critic_loss     | 1.6310   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 284899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=286000, episode_reward=-43.34 +/- 1.2490888612752538 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   43.34  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 286000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.0722   | \n",
      " |    critic_loss     | 1.5180   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 285899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=287000, episode_reward=-43.37 +/- 1.9378199557605649 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   43.37  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 287000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.0568   | \n",
      " |    critic_loss     | 0.7642   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 286899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=288000, episode_reward=-44.4 +/- 1.5149683570540247 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |    44.4  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 288000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.4087   | \n",
      " |    critic_loss     | 1.6022   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 287899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=289000, episode_reward=-44.14 +/- 1.2283744186324341 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   44.14  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 289000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.2718   | \n",
      " |    critic_loss     | 1.0679   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 288899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=290000, episode_reward=-44.58 +/- 1.0414442369161683 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   44.58  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 290000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.9397   | \n",
      " |    critic_loss     | 1.4849   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 289899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=291000, episode_reward=-45.65 +/- 1.3812878052395763 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   45.65  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 291000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.9802   | \n",
      " |    critic_loss     | 0.7450   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 290899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=292000, episode_reward=-45.81 +/- 1.7612710928013806 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   45.81  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 292000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.4545   | \n",
      " |    critic_loss     | 1.1136   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 291899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=293000, episode_reward=-46.84 +/- 1.6163739717791927 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   46.84  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 293000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.5401   | \n",
      " |    critic_loss     | 0.8850   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 292899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=294000, episode_reward=-47.08 +/- 1.916415613323424 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   47.08  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 294000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.0943   | \n",
      " |    critic_loss     | 1.0236   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 293899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=295000, episode_reward=-47.84 +/- 1.0323161685236508 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   47.84  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 295000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.0357   | \n",
      " |    critic_loss     | 0.7589   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 294899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=296000, episode_reward=-48.37 +/- 1.1579013192817382 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   48.37  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 296000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.1154   | \n",
      " |    critic_loss     | 1.2789   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 295899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=297000, episode_reward=-48.43 +/- 1.5729558234005376 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   48.43  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 297000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.3355   | \n",
      " |    critic_loss     | 1.0839   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 296899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=298000, episode_reward=-48.53 +/- 1.1864011005748023 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   48.53  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 298000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.9001   | \n",
      " |    critic_loss     | 0.9750   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 297899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=299000, episode_reward=-49.89 +/- 1.7655516697005602 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   49.89  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 299000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.6699   | \n",
      " |    critic_loss     | 1.1675   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 298899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=300000, episode_reward=-50.01 +/- 1.7678374375557402 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   50.01  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 300000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.5688   | \n",
      " |    critic_loss     | 1.6422   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 299899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=301000, episode_reward=-50.42 +/- 1.0013118403745094 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   50.42  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 301000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.6292   | \n",
      " |    critic_loss     | 1.6573   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 300899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=302000, episode_reward=-51.28 +/- 1.3985957570750047 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   51.28  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 302000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.8015   | \n",
      " |    critic_loss     | 1.4504   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 301899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=303000, episode_reward=-51.84 +/- 1.1812182379850356 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   51.84  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 303000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.6158   | \n",
      " |    critic_loss     | 1.4040   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 302899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=304000, episode_reward=-51.52 +/- 1.0922846318638009 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   51.52  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 304000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.9135   | \n",
      " |    critic_loss     | 1.4784   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 303899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=305000, episode_reward=-52.56 +/- 1.845983843244027 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   52.56  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 305000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.3585   | \n",
      " |    critic_loss     | 1.3396   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 304899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=306000, episode_reward=-52.56 +/- 1.8690526100461022 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   52.56  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 306000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.7060   | \n",
      " |    critic_loss     | 1.6765   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 305899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=307000, episode_reward=-53.18 +/- 1.5854072002775779 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   53.18  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 307000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.5347   | \n",
      " |    critic_loss     | 1.3837   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 306899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=308000, episode_reward=-54.03 +/- 1.0585266195885683 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   54.03  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 308000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.0878   | \n",
      " |    critic_loss     | 0.7720   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 307899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=309000, episode_reward=-54.73 +/- 1.8968790520576808 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   54.73  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 309000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.1722   | \n",
      " |    critic_loss     | 1.0431   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 308899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=310000, episode_reward=-55.0 +/- 1.3447505573141 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |    55.0  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 310000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.6806   | \n",
      " |    critic_loss     | 0.9201   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 309899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=311000, episode_reward=-55.73 +/- 1.3056066631278491 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   55.73  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 311000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.5688   | \n",
      " |    critic_loss     | 1.1422   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 310899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=312000, episode_reward=-55.68 +/- 1.143413698362596 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   55.68  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 312000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.2821   | \n",
      " |    critic_loss     | 1.0705   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 311899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=313000, episode_reward=-56.05 +/- 1.5790161362360817 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   56.05  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 313000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.2779   | \n",
      " |    critic_loss     | 1.5695   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 312899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=314000, episode_reward=-56.93 +/- 1.4483136215072525 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   56.93  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 314000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.2535   | \n",
      " |    critic_loss     | 0.8134   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 313899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=315000, episode_reward=-57.04 +/- 1.5843307203998829 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   57.04  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 315000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.5229   | \n",
      " |    critic_loss     | 0.8807   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 314899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=316000, episode_reward=-58.22 +/- 1.671451757675635 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   58.22  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 316000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.1108   | \n",
      " |    critic_loss     | 1.2777   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 315899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=317000, episode_reward=-58.51 +/- 1.3118119708291118 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   58.51  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 317000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.7588   | \n",
      " |    critic_loss     | 1.4397   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 316899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=318000, episode_reward=-59.44 +/- 1.6000337295267442 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   59.44  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 318000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.8433   | \n",
      " |    critic_loss     | 1.2108   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 317899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=319000, episode_reward=-59.22 +/- 1.9561549083766652 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   59.22  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 319000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.9172   | \n",
      " |    critic_loss     | 0.7293   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 318899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=320000, episode_reward=-59.63 +/- 1.9496437113966893 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   59.63  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 320000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.5155   | \n",
      " |    critic_loss     | 1.1289   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 319899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=321000, episode_reward=-60.99 +/- 1.5170232999930318 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   60.99  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 321000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.0265   | \n",
      " |    critic_loss     | 1.2566   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 320899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=322000, episode_reward=-61.3 +/- 1.7792488007172964 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |    61.3  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 322000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.7453   | \n",
      " |    critic_loss     | 0.6863   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 321899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=323000, episode_reward=-61.69 +/- 1.31304207404128 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   61.69  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 323000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.3826   | \n",
      " |    critic_loss     | 1.5957   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 322899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=324000, episode_reward=-62.23 +/- 1.2055166453445152 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   62.23  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 324000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.7783   | \n",
      " |    critic_loss     | 1.1946   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 323899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=325000, episode_reward=-62.74 +/- 1.9130206827593814 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   62.74  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 325000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.3023   | \n",
      " |    critic_loss     | 1.5756   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 324899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=326000, episode_reward=-62.89 +/- 1.3696888989518303 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   62.89  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 326000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.4787   | \n",
      " |    critic_loss     | 0.8697   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 325899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=327000, episode_reward=-63.16 +/- 1.6254614088223638 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   63.16  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 327000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.6314   | \n",
      " |    critic_loss     | 0.9078   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 326899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=328000, episode_reward=-63.71 +/- 1.4508654910110268 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   63.71  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 328000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.5000   | \n",
      " |    critic_loss     | 0.8750   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 327899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=329000, episode_reward=-64.7 +/- 1.392996389095023 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |    64.7  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 329000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.1594   | \n",
      " |    critic_loss     | 1.2899   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 328899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=330000, episode_reward=-64.87 +/- 1.124998676852454 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   64.87  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 330000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.9910   | \n",
      " |    critic_loss     | 1.4977   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 329899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=331000, episode_reward=-65.6 +/- 1.8700716369342283 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |    65.6  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 331000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.5261   | \n",
      " |    critic_loss     | 0.8815   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 330899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=332000, episode_reward=-65.78 +/- 1.9379506143294045 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   65.78  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 332000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.5420   | \n",
      " |    critic_loss     | 1.1355   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 331899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=333000, episode_reward=-66.31 +/- 1.4710624128649603 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   66.31  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 333000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.1016   | \n",
      " |    critic_loss     | 0.7754   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 332899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=334000, episode_reward=-67.46 +/- 1.2583326006245588 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   67.46  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 334000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.9443   | \n",
      " |    critic_loss     | 0.7361   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 333899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=335000, episode_reward=-67.98 +/- 1.8447927431643714 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   67.98  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 335000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.7133   | \n",
      " |    critic_loss     | 1.1783   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 334899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=336000, episode_reward=-68.09 +/- 1.5315487259776592 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   68.09  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 336000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.2896   | \n",
      " |    critic_loss     | 1.0724   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 335899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=337000, episode_reward=-68.44 +/- 1.0576107328858897 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   68.44  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 337000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.4317   | \n",
      " |    critic_loss     | 0.8579   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 336899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=338000, episode_reward=-69.41 +/- 1.025336520352247 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   69.41  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 338000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.8320   | \n",
      " |    critic_loss     | 1.4580   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 337899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=339000, episode_reward=-69.37 +/- 1.9214127379475823 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   69.37  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 339000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.9057   | \n",
      " |    critic_loss     | 1.2264   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 338899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=340000, episode_reward=-70.2 +/- 1.651496010627961 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |    70.2  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 340000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.2442   | \n",
      " |    critic_loss     | 1.5611   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 339899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=341000, episode_reward=-70.83 +/- 1.9826117619586112 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   70.83  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 341000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.0455   | \n",
      " |    critic_loss     | 1.5114   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 340899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=342000, episode_reward=-70.72 +/- 1.3661999536273854 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   70.72  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 342000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.2701   | \n",
      " |    critic_loss     | 0.8175   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 341899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=343000, episode_reward=-71.1 +/- 1.5579215485820446 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |    71.1  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 343000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.0495   | \n",
      " |    critic_loss     | 0.7624   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 342899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=344000, episode_reward=-72.06 +/- 1.3258271400897383 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   72.06  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 344000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.2522   | \n",
      " |    critic_loss     | 1.0630   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 343899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=345000, episode_reward=-72.27 +/- 1.8070749050124952 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   72.27  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 345000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.1391   | \n",
      " |    critic_loss     | 1.0348   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 344899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=346000, episode_reward=-72.75 +/- 1.1943454933464466 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   72.75  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 346000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.1148   | \n",
      " |    critic_loss     | 1.2787   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 345899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=347000, episode_reward=-73.68 +/- 1.0670785432254428 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   73.68  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 347000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.1866   | \n",
      " |    critic_loss     | 0.7967   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 346899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=348000, episode_reward=-73.57 +/- 1.2311224084706547 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   73.57  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 348000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.6445   | \n",
      " |    critic_loss     | 0.6611   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 347899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=349000, episode_reward=-74.41 +/- 1.0336512087444754 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   74.41  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 349000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.7292   | \n",
      " |    critic_loss     | 1.1823   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 348899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=350000, episode_reward=-74.95 +/- 1.7874231411696035 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   74.95  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 350000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.0367   | \n",
      " |    critic_loss     | 1.0092   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 349899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=351000, episode_reward=-75.18 +/- 1.0891035108869858 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   75.18  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 351000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.5930   | \n",
      " |    critic_loss     | 1.1483   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 350899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=352000, episode_reward=-75.62 +/- 1.8324836795638821 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   75.62  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 352000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.1694   | \n",
      " |    critic_loss     | 0.7924   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 351899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=353000, episode_reward=-76.42 +/- 1.7393327302243389 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   76.42  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 353000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.0274   | \n",
      " |    critic_loss     | 1.5068   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 352899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=354000, episode_reward=-76.5 +/- 1.5150893953444453 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |    76.5  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 354000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.3732   | \n",
      " |    critic_loss     | 0.8433   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 353899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=355000, episode_reward=-77.19 +/- 1.9747833695932286 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   77.19  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 355000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.9752   | \n",
      " |    critic_loss     | 0.7438   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 354899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=356000, episode_reward=-78.15 +/- 1.1716394024280439 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   78.15  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 356000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.1781   | \n",
      " |    critic_loss     | 0.7945   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 355899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=357000, episode_reward=-78.89 +/- 1.2113101090032574 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   78.89  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 357000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.3627   | \n",
      " |    critic_loss     | 1.5907   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 356899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=358000, episode_reward=-78.96 +/- 1.9980107494333623 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   78.96  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 358000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.3329   | \n",
      " |    critic_loss     | 1.0832   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 357899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=359000, episode_reward=-79.69 +/- 1.170642956625517 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   79.69  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 359000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.7109   | \n",
      " |    critic_loss     | 0.9277   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 358899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=360000, episode_reward=-79.7 +/- 1.7397733813481313 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |    79.7  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 360000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.0303   | \n",
      " |    critic_loss     | 1.5076   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 359899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=361000, episode_reward=-80.13 +/- 1.9023330648840284 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   80.13  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 361000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.5735   | \n",
      " |    critic_loss     | 0.8934   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 360899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=362000, episode_reward=-81.3 +/- 1.123798045413604 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |    81.3  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 362000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.6830   | \n",
      " |    critic_loss     | 1.1708   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 361899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=363000, episode_reward=-81.01 +/- 1.3625136584974178 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   81.01  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 363000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.2880   | \n",
      " |    critic_loss     | 1.3220   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 362899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=364000, episode_reward=-82.13 +/- 1.503190359579356 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   82.13  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 364000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.1384   | \n",
      " |    critic_loss     | 1.5346   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 363899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=365000, episode_reward=-82.13 +/- 1.032943465656683 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   82.13  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 365000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.1499   | \n",
      " |    critic_loss     | 1.5375   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 364899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=366000, episode_reward=-83.37 +/- 1.6685708323046178 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   83.37  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 366000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.4186   | \n",
      " |    critic_loss     | 1.3547   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 365899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=367000, episode_reward=-83.5 +/- 1.9905863228032372 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |    83.5  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 367000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.6932   | \n",
      " |    critic_loss     | 0.6733   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 366899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=368000, episode_reward=-84.44 +/- 1.43729146036344 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   84.44  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 368000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.7191   | \n",
      " |    critic_loss     | 1.4298   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 367899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=369000, episode_reward=-84.59 +/- 1.9815878094677757 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   84.59  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 369000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.1253   | \n",
      " |    critic_loss     | 0.7813   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 368899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=370000, episode_reward=-84.85 +/- 1.5394185417733037 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   84.85  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 370000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.7748   | \n",
      " |    critic_loss     | 1.1937   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 369899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=371000, episode_reward=-85.03 +/- 1.885175527306823 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   85.03  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 371000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.8586   | \n",
      " |    critic_loss     | 1.4647   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 370899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=372000, episode_reward=-85.75 +/- 1.6215232106875928 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   85.75  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 372000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.1676   | \n",
      " |    critic_loss     | 1.5419   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 371899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=373000, episode_reward=-86.14 +/- 1.0918349672526702 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   86.14  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 373000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.9608   | \n",
      " |    critic_loss     | 0.9902   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 372899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=374000, episode_reward=-87.03 +/- 1.3179384958304317 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   87.03  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 374000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.5422   | \n",
      " |    critic_loss     | 0.8855   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 373899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=375000, episode_reward=-87.47 +/- 1.1185995393645158 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   87.47  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 375000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.1514   | \n",
      " |    critic_loss     | 1.5378   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 374899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=376000, episode_reward=-88.2 +/- 1.549608315696477 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |    88.2  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 376000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.8311   | \n",
      " |    critic_loss     | 0.7078   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 375899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=377000, episode_reward=-88.92 +/- 1.9843697648325516 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   88.92  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 377000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.6063   | \n",
      " |    critic_loss     | 0.6516   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 376899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=378000, episode_reward=-89.46 +/- 1.465154348317684 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   89.46  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 378000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.8418   | \n",
      " |    critic_loss     | 1.2105   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 377899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=379000, episode_reward=-89.67 +/- 1.255505387100086 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   89.67  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 379000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.5845   | \n",
      " |    critic_loss     | 1.1461   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 378899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=380000, episode_reward=-90.5 +/- 1.3092652827521134 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |    90.5  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 380000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.7997   | \n",
      " |    critic_loss     | 1.1999   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 379899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=381000, episode_reward=-90.68 +/- 1.4922913069171313 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   90.68  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 381000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.3871   | \n",
      " |    critic_loss     | 1.0968   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 380899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=382000, episode_reward=-91.37 +/- 1.5905945444484577 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   91.37  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 382000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.2083   | \n",
      " |    critic_loss     | 1.5521   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 381899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=383000, episode_reward=-91.91 +/- 1.7768844425724186 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   91.91  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 383000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.0317   | \n",
      " |    critic_loss     | 0.7579   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 382899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=384000, episode_reward=-91.61 +/- 1.419375396856573 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   91.61  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 384000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.7771   | \n",
      " |    critic_loss     | 0.6943   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 383899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=385000, episode_reward=-92.87 +/- 1.4782192271102454 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   92.87  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 385000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.6604   | \n",
      " |    critic_loss     | 1.1651   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 384899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=386000, episode_reward=-93.1 +/- 1.126831425871218 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |    93.1  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 386000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.5629   | \n",
      " |    critic_loss     | 1.1407   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 385899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=387000, episode_reward=-93.52 +/- 1.378218418763896 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   93.52  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 387000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.7590   | \n",
      " |    critic_loss     | 1.1898   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 386899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=388000, episode_reward=-94.44 +/- 1.1373865935376881 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   94.44  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 388000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.2418   | \n",
      " |    critic_loss     | 1.5604   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 387899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=389000, episode_reward=-94.0 +/- 1.2242047689181892 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |    94.0  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 389000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.9176   | \n",
      " |    critic_loss     | 1.4794   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 388899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=390000, episode_reward=-95.09 +/- 1.4766751819626336 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   95.09  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 390000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.5006   | \n",
      " |    critic_loss     | 1.1252   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 389899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=391000, episode_reward=-95.43 +/- 1.078753766497302 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   95.43  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 391000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.5737   | \n",
      " |    critic_loss     | 0.8934   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 390899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=392000, episode_reward=-96.07 +/- 1.3528225498398865 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   96.07  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 392000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.4678   | \n",
      " |    critic_loss     | 0.6169   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 391899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=393000, episode_reward=-96.98 +/- 1.7045975941619296 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   96.98  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 393000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.9196   | \n",
      " |    critic_loss     | 0.7299   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 392899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=394000, episode_reward=-97.02 +/- 1.695199173035986 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   97.02  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 394000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.2532   | \n",
      " |    critic_loss     | 1.0633   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 393899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=395000, episode_reward=-97.2 +/- 1.8285113425154726 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |    97.2  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 395000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.7970   | \n",
      " |    critic_loss     | 0.6992   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 394899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=396000, episode_reward=-97.76 +/- 1.01775100257086 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   97.76  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 396000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.5603   | \n",
      " |    critic_loss     | 1.3901   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 395899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=397000, episode_reward=-98.26 +/- 1.8047064960961858 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   98.26  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 397000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.2530   | \n",
      " |    critic_loss     | 0.8133   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 396899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=398000, episode_reward=-98.83 +/- 1.6084245106143031 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   98.83  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 398000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.1721   | \n",
      " |    critic_loss     | 0.7930   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 397899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=399000, episode_reward=-99.11 +/- 1.163921265513437 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   99.11  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 399000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.9668   | \n",
      " |    critic_loss     | 0.7417   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 398899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=400000, episode_reward=-100.47 +/- 1.9036584128451919 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  100.47  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 400000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.5903   | \n",
      " |    critic_loss     | 0.8976   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 399899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=401000, episode_reward=-100.13 +/- 1.4918642058055989 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  100.13  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 401000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.2579   | \n",
      " |    critic_loss     | 0.8145   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 400899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=402000, episode_reward=-101.03 +/- 1.3753867528365782 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  101.03  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 402000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.2571   | \n",
      " |    critic_loss     | 1.3143   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 401899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=403000, episode_reward=-101.07 +/- 1.9078186082505706 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  101.07  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 403000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.3921   | \n",
      " |    critic_loss     | 1.0980   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 402899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=404000, episode_reward=-101.67 +/- 1.2403552831064666 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  101.67  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 404000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.0878   | \n",
      " |    critic_loss     | 1.2719   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 403899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=405000, episode_reward=-102.4 +/- 1.635889182925256 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   102.4  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 405000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.8336   | \n",
      " |    critic_loss     | 0.7084   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 404899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=406000, episode_reward=-102.84 +/- 1.193708042038854 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  102.84  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 406000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.0133   | \n",
      " |    critic_loss     | 1.5033   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 405899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=407000, episode_reward=-103.27 +/- 1.14261070585462 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  103.27  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 407000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.5225   | \n",
      " |    critic_loss     | 0.8806   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 406899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=408000, episode_reward=-103.76 +/- 1.1901935744728291 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  103.76  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 408000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.5362   | \n",
      " |    critic_loss     | 1.1341   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 407899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=409000, episode_reward=-104.48 +/- 1.347303265685293 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  104.48  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 409000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.5445   | \n",
      " |    critic_loss     | 1.3861   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 408899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=410000, episode_reward=-105.49 +/- 1.30457101363612 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  105.49  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 410000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.1156   | \n",
      " |    critic_loss     | 0.7789   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 409899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=411000, episode_reward=-105.51 +/- 1.5398290170271822 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  105.51  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 411000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.4035   | \n",
      " |    critic_loss     | 1.1009   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 410899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=412000, episode_reward=-105.79 +/- 1.888653974003905 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  105.79  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 412000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.3706   | \n",
      " |    critic_loss     | 1.0926   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 411899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=413000, episode_reward=-106.7 +/- 1.5603420742913001 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   106.7  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 413000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.8628   | \n",
      " |    critic_loss     | 0.9657   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 412899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=414000, episode_reward=-106.75 +/- 1.2271513074764897 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  106.75  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 414000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.9595   | \n",
      " |    critic_loss     | 1.4899   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 413899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=415000, episode_reward=-107.55 +/- 1.1013917817919432 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  107.55  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 415000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.3433   | \n",
      " |    critic_loss     | 0.8358   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 414899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=416000, episode_reward=-107.92 +/- 1.1387315651723453 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  107.92  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 416000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.1979   | \n",
      " |    critic_loss     | 1.5495   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 415899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=417000, episode_reward=-108.43 +/- 1.066426131403881 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  108.43  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 417000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.6613   | \n",
      " |    critic_loss     | 0.9153   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 416899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=418000, episode_reward=-108.97 +/- 1.5233122340372511 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  108.97  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 418000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.7336   | \n",
      " |    critic_loss     | 0.9334   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 417899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=419000, episode_reward=-109.78 +/- 1.400680182914936 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  109.78  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 419000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.3503   | \n",
      " |    critic_loss     | 1.0876   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 418899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=420000, episode_reward=-109.71 +/- 1.045322772304317 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  109.71  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 420000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.9788   | \n",
      " |    critic_loss     | 0.7447   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 419899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=421000, episode_reward=-110.52 +/- 1.7036257364346516 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  110.52  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 421000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.6854   | \n",
      " |    critic_loss     | 0.9213   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 420899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=422000, episode_reward=-110.77 +/- 1.381407474894354 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  110.77  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 422000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.8432   | \n",
      " |    critic_loss     | 1.2108   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 421899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=423000, episode_reward=-111.13 +/- 1.31826019105249 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  111.13  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 423000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.3725   | \n",
      " |    critic_loss     | 0.8431   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 422899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=424000, episode_reward=-112.01 +/- 1.6520787455668153 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  112.01  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 424000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.4582   | \n",
      " |    critic_loss     | 1.3646   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 423899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=425000, episode_reward=-112.12 +/- 1.5713180320254487 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  112.12  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 425000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.6155   | \n",
      " |    critic_loss     | 1.4039   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 424899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=426000, episode_reward=-113.22 +/- 1.5987276846458531 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  113.22  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 426000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.1751   | \n",
      " |    critic_loss     | 1.5438   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 425899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=427000, episode_reward=-113.27 +/- 1.9111459630044265 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  113.27  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 427000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.4991   | \n",
      " |    critic_loss     | 1.1248   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 426899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=428000, episode_reward=-113.92 +/- 1.5312965838160904 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  113.92  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 428000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.7215   | \n",
      " |    critic_loss     | 0.6804   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 427899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=429000, episode_reward=-114.85 +/- 1.4795298732697337 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  114.85  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 429000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.6026   | \n",
      " |    critic_loss     | 1.1506   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 428899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=430000, episode_reward=-115.14 +/- 1.4743190564157405 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  115.14  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 430000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.2429   | \n",
      " |    critic_loss     | 0.8107   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 429899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=431000, episode_reward=-115.68 +/- 1.1432369907634543 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  115.68  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 431000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.1577   | \n",
      " |    critic_loss     | 1.2894   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 430899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=432000, episode_reward=-115.51 +/- 1.1244733692135178 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  115.51  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 432000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.9797   | \n",
      " |    critic_loss     | 0.9949   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 431899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=433000, episode_reward=-116.08 +/- 1.220571692300846 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  116.08  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 433000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.0638   | \n",
      " |    critic_loss     | 1.2659   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 432899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=434000, episode_reward=-117.35 +/- 1.2962654714567987 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  117.35  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 434000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.1828   | \n",
      " |    critic_loss     | 0.7957   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 433899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=435000, episode_reward=-117.57 +/- 1.5541833235644924 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  117.57  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 435000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.0204   | \n",
      " |    critic_loss     | 0.7551   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 434899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=436000, episode_reward=-118.37 +/- 1.2201609683454009 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  118.37  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 436000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.5103   | \n",
      " |    critic_loss     | 1.1276   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 435899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=437000, episode_reward=-118.35 +/- 1.5656153164671895 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  118.35  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 437000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.4541   | \n",
      " |    critic_loss     | 0.8635   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 436899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=438000, episode_reward=-119.46 +/- 1.887635879052302 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  119.46  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 438000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.6793   | \n",
      " |    critic_loss     | 1.4198   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 437899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=439000, episode_reward=-119.64 +/- 1.0318892426757325 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  119.64  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 439000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.9583   | \n",
      " |    critic_loss     | 0.9896   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 438899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=440000, episode_reward=-120.23 +/- 1.7788549517987406 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  120.23  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 440000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.6999   | \n",
      " |    critic_loss     | 1.1750   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 439899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=441000, episode_reward=-120.05 +/- 1.6971334612743842 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  120.05  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 441000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.0224   | \n",
      " |    critic_loss     | 0.7556   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 440899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=442000, episode_reward=-120.69 +/- 1.4310747237322086 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  120.69  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 442000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.0244   | \n",
      " |    critic_loss     | 1.2561   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 441899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=443000, episode_reward=-121.66 +/- 1.3607406930866675 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  121.66  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 443000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.0662   | \n",
      " |    critic_loss     | 1.2666   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 442899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=444000, episode_reward=-122.23 +/- 1.8014937357268714 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  122.23  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 444000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.9899   | \n",
      " |    critic_loss     | 1.4975   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 443899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=445000, episode_reward=-122.19 +/- 1.9200538587571283 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  122.19  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 445000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.4309   | \n",
      " |    critic_loss     | 0.6077   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 444899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=446000, episode_reward=-122.96 +/- 1.3670130827380995 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  122.96  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 446000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.6440   | \n",
      " |    critic_loss     | 0.6610   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 445899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=447000, episode_reward=-123.32 +/- 1.554238363444798 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  123.32  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 447000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.3771   | \n",
      " |    critic_loss     | 1.0943   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 446899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=448000, episode_reward=-124.41 +/- 1.6003632060047306 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  124.41  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 448000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.2277   | \n",
      " |    critic_loss     | 1.0569   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 447899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=449000, episode_reward=-124.1 +/- 1.4945200043025146 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   124.1  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 449000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.1239   | \n",
      " |    critic_loss     | 1.5310   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 448899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=450000, episode_reward=-124.53 +/- 1.8955670129707658 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  124.53  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 450000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.4220   | \n",
      " |    critic_loss     | 1.3555   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 449899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=451000, episode_reward=-125.89 +/- 1.4789825745203058 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  125.89  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 451000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 6.0141   | \n",
      " |    critic_loss     | 1.5035   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 450899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=452000, episode_reward=-125.71 +/- 1.6301502432267725 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  125.71  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 452000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.4083   | \n",
      " |    critic_loss     | 0.8521   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 451899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=453000, episode_reward=-126.64 +/- 1.6600993568641882 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  126.64  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 453000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.3514   | \n",
      " |    critic_loss     | 1.0878   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 452899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=454000, episode_reward=-126.69 +/- 1.2439758651312665 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  126.69  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 454000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.3071   | \n",
      " |    critic_loss     | 1.3268   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 453899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=455000, episode_reward=-127.63 +/- 1.1262464789471578 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  127.63  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 455000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.0112   | \n",
      " |    critic_loss     | 1.2528   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 454899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=456000, episode_reward=-128.49 +/- 1.9335114825637285 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  128.49  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 456000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.8498   | \n",
      " |    critic_loss     | 0.9624   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 455899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=457000, episode_reward=-128.34 +/- 1.8783594865742645 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  128.34  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 457000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.5694   | \n",
      " |    critic_loss     | 0.8923   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 456899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=458000, episode_reward=-129.18 +/- 1.342561804459614 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  129.18  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 458000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.9419   | \n",
      " |    critic_loss     | 1.2355   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 457899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=459000, episode_reward=-129.2 +/- 1.1799387915921025 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   129.2  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 459000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.6394   | \n",
      " |    critic_loss     | 1.1598   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 458899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=460000, episode_reward=-129.51 +/- 1.584308848247784 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  129.51  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 460000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.6653   | \n",
      " |    critic_loss     | 0.6663   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 459899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=461000, episode_reward=-130.74 +/- 1.4550854770224904 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  130.74  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 461000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.7277   | \n",
      " |    critic_loss     | 0.9319   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 460899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=462000, episode_reward=-130.97 +/- 1.8813436378041155 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  130.97  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 462000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.8563   | \n",
      " |    critic_loss     | 1.4641   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 461899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=463000, episode_reward=-131.47 +/- 1.2384130204853037 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  131.47  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 463000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.6523   | \n",
      " |    critic_loss     | 0.6631   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 462899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=464000, episode_reward=-132.31 +/- 1.8255794304238457 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  132.31  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 464000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.1250   | \n",
      " |    critic_loss     | 1.2813   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 463899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=465000, episode_reward=-132.13 +/- 1.7890634215560683 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  132.13  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 465000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.8321   | \n",
      " |    critic_loss     | 0.9580   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 464899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=466000, episode_reward=-132.5 +/- 1.1296352077677208 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   132.5  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 466000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.5735   | \n",
      " |    critic_loss     | 1.3934   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 465899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=467000, episode_reward=-133.95 +/- 1.7723179000610243 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  133.95  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 467000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.9131   | \n",
      " |    critic_loss     | 0.9783   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 466899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=468000, episode_reward=-134.16 +/- 1.338084745106726 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  134.16  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 468000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.4240   | \n",
      " |    critic_loss     | 1.1060   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 467899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=469000, episode_reward=-134.61 +/- 1.0790339564775273 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  134.61  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 469000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.0189   | \n",
      " |    critic_loss     | 0.7547   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 468899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=470000, episode_reward=-134.93 +/- 1.2169251044395482 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  134.93  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 470000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.6836   | \n",
      " |    critic_loss     | 1.4209   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 469899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=471000, episode_reward=-135.3 +/- 1.1141183827390857 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   135.3  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 471000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.9759   | \n",
      " |    critic_loss     | 0.7440   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 470899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=472000, episode_reward=-135.98 +/- 1.7057951301997716 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  135.98  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 472000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.7019   | \n",
      " |    critic_loss     | 0.6755   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 471899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=473000, episode_reward=-136.86 +/- 1.310559405964225 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  136.86  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 473000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.2867   | \n",
      " |    critic_loss     | 1.3217   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 472899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=474000, episode_reward=-137.46 +/- 1.6771619154525148 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  137.46  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 474000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.6646   | \n",
      " |    critic_loss     | 0.6661   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 473899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=475000, episode_reward=-137.74 +/- 1.3417603179316306 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  137.74  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 475000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.7669   | \n",
      " |    critic_loss     | 1.1917   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 474899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=476000, episode_reward=-137.54 +/- 1.8860842189510052 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  137.54  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 476000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.8768   | \n",
      " |    critic_loss     | 1.2192   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 475899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=477000, episode_reward=-138.89 +/- 1.685795968314494 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  138.89  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 477000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.6614   | \n",
      " |    critic_loss     | 0.9154   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 476899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=478000, episode_reward=-138.61 +/- 1.4448084221532067 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  138.61  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 478000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.5225   | \n",
      " |    critic_loss     | 0.6306   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 477899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=479000, episode_reward=-139.25 +/- 1.062910148328705 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  139.25  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 479000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.1279   | \n",
      " |    critic_loss     | 1.2820   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 478899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=480000, episode_reward=-140.17 +/- 1.475100683990958 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  140.17  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 480000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.5773   | \n",
      " |    critic_loss     | 1.1443   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 479899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=481000, episode_reward=-140.9 +/- 1.0446726534829265 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   140.9  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 481000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.6720   | \n",
      " |    critic_loss     | 0.6680   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 480899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=482000, episode_reward=-140.85 +/- 1.5488030141545837 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  140.85  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 482000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.1876   | \n",
      " |    critic_loss     | 0.7969   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 481899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=483000, episode_reward=-141.68 +/- 1.6708467168223369 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  141.68  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 483000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.8490   | \n",
      " |    critic_loss     | 0.9623   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 482899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=484000, episode_reward=-141.75 +/- 1.9888726104787462 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  141.75  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 484000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.2845   | \n",
      " |    critic_loss     | 0.5711   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 483899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=485000, episode_reward=-142.58 +/- 1.2203808034598027 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  142.58  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 485000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.8780   | \n",
      " |    critic_loss     | 0.9695   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 484899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=486000, episode_reward=-142.8 +/- 1.1142308403973225 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   142.8  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 486000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.9363   | \n",
      " |    critic_loss     | 0.9841   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 485899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=487000, episode_reward=-143.89 +/- 1.7342112564029222 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  143.89  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 487000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.3354   | \n",
      " |    critic_loss     | 0.8339   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 486899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=488000, episode_reward=-144.06 +/- 1.0188062392253525 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  144.06  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 488000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.0877   | \n",
      " |    critic_loss     | 0.5219   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 487899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=489000, episode_reward=-144.91 +/- 1.9585991051600813 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  144.91  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 489000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.4650   | \n",
      " |    critic_loss     | 1.1162   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 488899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=490000, episode_reward=-145.19 +/- 1.1184901268898675 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  145.19  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 490000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.4090   | \n",
      " |    critic_loss     | 1.3523   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 489899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=491000, episode_reward=-145.33 +/- 1.073261555968137 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  145.33  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 491000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.0399   | \n",
      " |    critic_loss     | 1.0100   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 490899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=492000, episode_reward=-145.63 +/- 1.2902117704164295 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  145.63  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 492000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.0263   | \n",
      " |    critic_loss     | 1.0066   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 491899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=493000, episode_reward=-146.77 +/- 1.0246272034819948 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  146.77  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 493000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.3012   | \n",
      " |    critic_loss     | 0.8253   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 492899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=494000, episode_reward=-147.3 +/- 1.9001979625297127 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   147.3  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 494000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.8097   | \n",
      " |    critic_loss     | 1.2024   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 493899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=495000, episode_reward=-147.85 +/- 1.582651562479334 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  147.85  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 495000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.1897   | \n",
      " |    critic_loss     | 0.7974   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 494899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=496000, episode_reward=-147.98 +/- 1.7537040065849956 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  147.98  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 496000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.1281   | \n",
      " |    critic_loss     | 0.7820   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 495899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=497000, episode_reward=-148.81 +/- 1.1142401528368526 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  148.81  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 497000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.0141   | \n",
      " |    critic_loss     | 1.0035   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 496899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=498000, episode_reward=-148.87 +/- 1.3501703823991467 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  148.87  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 498000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.2797   | \n",
      " |    critic_loss     | 1.3199   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 497899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=499000, episode_reward=-149.47 +/- 1.7921055443753344 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  149.47  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 499000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.9018   | \n",
      " |    critic_loss     | 1.4754   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 498899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=500000, episode_reward=-150.08 +/- 1.0123240187989055 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  150.08  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 500000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.7298   | \n",
      " |    critic_loss     | 1.1825   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 499899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=501000, episode_reward=-150.18 +/- 1.9359806515602198 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  150.18  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 501000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.3845   | \n",
      " |    critic_loss     | 1.3461   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 500899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=502000, episode_reward=-151.13 +/- 1.3400407850567804 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  151.13  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 502000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.7376   | \n",
      " |    critic_loss     | 1.4344   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 501899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=503000, episode_reward=-151.56 +/- 1.9037117765393905 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  151.56  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 503000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.1052   | \n",
      " |    critic_loss     | 1.2763   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 502899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=504000, episode_reward=-151.99 +/- 1.083017019264136 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  151.99  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 504000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.0196   | \n",
      " |    critic_loss     | 0.7549   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 503899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=505000, episode_reward=-152.89 +/- 1.4949599282840182 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  152.89  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 505000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.9178   | \n",
      " |    critic_loss     | 1.2294   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 504899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=506000, episode_reward=-153.24 +/- 1.4048440558384687 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  153.24  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 506000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.8103   | \n",
      " |    critic_loss     | 1.4526   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 505899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=507000, episode_reward=-153.31 +/- 1.025075492408837 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  153.31  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 507000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.4076   | \n",
      " |    critic_loss     | 0.6019   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 506899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=508000, episode_reward=-153.74 +/- 1.9733074676190498 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  153.74  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 508000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.7137   | \n",
      " |    critic_loss     | 1.1784   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 507899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=509000, episode_reward=-154.65 +/- 1.650700962744306 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  154.65  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 509000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.7578   | \n",
      " |    critic_loss     | 1.1895   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 508899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=510000, episode_reward=-155.2 +/- 1.7590715475198573 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   155.2  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 510000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.4748   | \n",
      " |    critic_loss     | 1.1187   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 509899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=511000, episode_reward=-155.84 +/- 1.8116151019417805 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  155.84  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 511000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.2046   | \n",
      " |    critic_loss     | 1.3011   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 510899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=512000, episode_reward=-155.63 +/- 1.0519645154683386 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  155.63  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 512000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.8021   | \n",
      " |    critic_loss     | 1.4505   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 511899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=513000, episode_reward=-156.6 +/- 1.133532341146517 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   156.6  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 513000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.1489   | \n",
      " |    critic_loss     | 0.7872   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 512899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=514000, episode_reward=-157.47 +/- 1.9481325532601588 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  157.47  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 514000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.6380   | \n",
      " |    critic_loss     | 0.9095   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 513899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=515000, episode_reward=-157.03 +/- 1.710505670481634 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  157.03  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 515000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.6494   | \n",
      " |    critic_loss     | 1.4124   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 514899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=516000, episode_reward=-158.03 +/- 1.7485818998255702 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  158.03  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 516000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.6607   | \n",
      " |    critic_loss     | 1.1652   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 515899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=517000, episode_reward=-158.79 +/- 1.9659526934107103 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  158.79  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 517000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.8454   | \n",
      " |    critic_loss     | 0.9614   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 516899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=518000, episode_reward=-158.68 +/- 1.903478582069989 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  158.68  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 518000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.6460   | \n",
      " |    critic_loss     | 1.4115   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 517899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=519000, episode_reward=-159.11 +/- 1.5273904202510726 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  159.11  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 519000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.9797   | \n",
      " |    critic_loss     | 1.2449   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 518899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=520000, episode_reward=-160.47 +/- 1.3547969315404116 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  160.47  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 520000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.9360   | \n",
      " |    critic_loss     | 1.2340   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 519899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=521000, episode_reward=-160.91 +/- 1.7078577028316184 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  160.91  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 521000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.2492   | \n",
      " |    critic_loss     | 0.8123   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 520899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=522000, episode_reward=-160.61 +/- 1.5606711085286715 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  160.61  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 522000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.9292   | \n",
      " |    critic_loss     | 0.9823   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 521899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=523000, episode_reward=-161.21 +/- 1.612942862403649 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  161.21  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 523000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.0455   | \n",
      " |    critic_loss     | 0.5114   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 522899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=524000, episode_reward=-161.91 +/- 1.2944329967869697 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  161.91  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 524000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.7573   | \n",
      " |    critic_loss     | 0.6893   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 523899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=525000, episode_reward=-162.86 +/- 1.975748735158016 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  162.86  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 525000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.6129   | \n",
      " |    critic_loss     | 1.1532   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 524899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=526000, episode_reward=-163.33 +/- 1.5870255977161962 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  163.33  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 526000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.5870   | \n",
      " |    critic_loss     | 1.3967   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 525899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=527000, episode_reward=-163.71 +/- 1.3825534991793482 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  163.71  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 527000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.1190   | \n",
      " |    critic_loss     | 1.2797   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 526899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=528000, episode_reward=-163.74 +/- 1.3744835757696017 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  163.74  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 528000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.9784   | \n",
      " |    critic_loss     | 1.2446   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 527899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=529000, episode_reward=-164.14 +/- 1.6489408083167871 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  164.14  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 529000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.4531   | \n",
      " |    critic_loss     | 0.6133   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 528899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=530000, episode_reward=-165.33 +/- 1.7841473396252288 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  165.33  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 530000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.9411   | \n",
      " |    critic_loss     | 0.4853   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 529899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=531000, episode_reward=-165.52 +/- 1.3656226378910232 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  165.52  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 531000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.8005   | \n",
      " |    critic_loss     | 1.4501   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 530899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=532000, episode_reward=-166.17 +/- 1.9400430690884731 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  166.17  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 532000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.3065   | \n",
      " |    critic_loss     | 1.3266   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 531899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=533000, episode_reward=-166.37 +/- 1.0103678344105835 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  166.37  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 533000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.5101   | \n",
      " |    critic_loss     | 0.6275   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 532899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=534000, episode_reward=-166.86 +/- 1.5300548701872352 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  166.86  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 534000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.5445   | \n",
      " |    critic_loss     | 1.3861   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 533899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=535000, episode_reward=-167.14 +/- 1.055118651418625 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  167.14  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 535000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.7009   | \n",
      " |    critic_loss     | 1.1752   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 534899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=536000, episode_reward=-167.64 +/- 1.7368273073832041 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  167.64  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 536000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.8623   | \n",
      " |    critic_loss     | 0.4656   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 535899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=537000, episode_reward=-168.16 +/- 1.2919812019362518 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  168.16  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 537000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.3550   | \n",
      " |    critic_loss     | 0.8387   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 536899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=538000, episode_reward=-169.06 +/- 1.8460887498527718 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  169.06  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 538000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.6426   | \n",
      " |    critic_loss     | 0.6606   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 537899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=539000, episode_reward=-169.96 +/- 1.4179800178008208 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  169.96  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 539000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.7974   | \n",
      " |    critic_loss     | 1.4494   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 538899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=540000, episode_reward=-170.31 +/- 1.861300735703427 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  170.31  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 540000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.3831   | \n",
      " |    critic_loss     | 1.0958   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 539899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=541000, episode_reward=-170.28 +/- 1.926052316865018 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  170.28  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 541000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.5695   | \n",
      " |    critic_loss     | 1.3924   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 540899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=542000, episode_reward=-170.62 +/- 1.6886762239313713 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  170.62  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 542000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.3236   | \n",
      " |    critic_loss     | 1.3309   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 541899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=543000, episode_reward=-171.87 +/- 1.5566523155572916 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  171.87  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 543000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.2469   | \n",
      " |    critic_loss     | 0.5617   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 542899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=544000, episode_reward=-172.39 +/- 1.454755840726928 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  172.39  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 544000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.2536   | \n",
      " |    critic_loss     | 0.8134   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 543899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=545000, episode_reward=-172.51 +/- 1.0655609714972143 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  172.51  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 545000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.1512   | \n",
      " |    critic_loss     | 0.7878   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 544899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=546000, episode_reward=-173.07 +/- 1.6418750138429865 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  173.07  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 546000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.8339   | \n",
      " |    critic_loss     | 1.2085   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 545899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=547000, episode_reward=-173.29 +/- 1.4332326939976614 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  173.29  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 547000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.4746   | \n",
      " |    critic_loss     | 0.6186   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 546899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=548000, episode_reward=-173.65 +/- 1.0315713536112852 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  173.65  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 548000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.0099   | \n",
      " |    critic_loss     | 0.5025   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 547899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=549000, episode_reward=-174.93 +/- 1.1942305336777284 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  174.93  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 549000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.4952   | \n",
      " |    critic_loss     | 0.8738   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 548899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=550000, episode_reward=-174.77 +/- 1.206711337602187 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  174.77  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 550000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.3316   | \n",
      " |    critic_loss     | 0.8329   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 549899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=551000, episode_reward=-175.49 +/- 1.6062791055309875 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  175.49  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 551000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.7669   | \n",
      " |    critic_loss     | 1.4417   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 550899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=552000, episode_reward=-176.2 +/- 1.1236913830751203 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   176.2  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 552000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.9899   | \n",
      " |    critic_loss     | 1.2475   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 551899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=553000, episode_reward=-176.15 +/- 1.8991976181694699 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  176.15  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 553000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.6635   | \n",
      " |    critic_loss     | 0.9159   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 552899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=554000, episode_reward=-177.37 +/- 1.2295712733596122 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  177.37  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 554000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.1184   | \n",
      " |    critic_loss     | 0.5296   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 553899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=555000, episode_reward=-177.61 +/- 1.7949157938247122 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  177.61  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 555000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.2937   | \n",
      " |    critic_loss     | 0.5734   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 554899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=556000, episode_reward=-177.8 +/- 1.0457107075357466 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   177.8  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 556000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.8292   | \n",
      " |    critic_loss     | 0.7073   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 555899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=557000, episode_reward=-178.23 +/- 1.6247630751900641 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  178.23  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 557000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.7257   | \n",
      " |    critic_loss     | 1.4314   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 556899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=558000, episode_reward=-178.83 +/- 1.4154431957703535 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  178.83  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 558000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.7244   | \n",
      " |    critic_loss     | 0.6811   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 557899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=559000, episode_reward=-179.78 +/- 1.773910429857057 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  179.78  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 559000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.8855   | \n",
      " |    critic_loss     | 1.2214   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 558899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=560000, episode_reward=-180.02 +/- 1.2194653894117815 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  180.02  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 560000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.2077   | \n",
      " |    critic_loss     | 1.0519   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 559899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=561000, episode_reward=-180.1 +/- 1.429028641662934 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   180.1  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 561000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.5437   | \n",
      " |    critic_loss     | 1.3859   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 560899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=562000, episode_reward=-181.26 +/- 1.9707952769342243 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  181.26  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 562000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.2741   | \n",
      " |    critic_loss     | 0.5685   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 561899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=563000, episode_reward=-181.65 +/- 1.1645636618613535 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  181.65  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 563000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.7507   | \n",
      " |    critic_loss     | 1.4377   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 562899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=564000, episode_reward=-182.31 +/- 1.5632045460288786 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  182.31  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 564000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.6891   | \n",
      " |    critic_loss     | 1.1723   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 563899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=565000, episode_reward=-182.58 +/- 1.7179197792791348 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  182.58  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 565000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.8712   | \n",
      " |    critic_loss     | 0.7178   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 564899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=566000, episode_reward=-183.34 +/- 1.697507089647134 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  183.34  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 566000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.7186   | \n",
      " |    critic_loss     | 1.4296   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 565899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=567000, episode_reward=-183.3 +/- 1.395828920060861 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   183.3  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 567000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.7742   | \n",
      " |    critic_loss     | 0.6935   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 566899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=568000, episode_reward=-184.25 +/- 1.5678617581273855 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  184.25  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 568000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.9677   | \n",
      " |    critic_loss     | 0.9919   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 567899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=569000, episode_reward=-184.81 +/- 1.8109169636017084 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  184.81  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 569000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.9510   | \n",
      " |    critic_loss     | 1.2377   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 568899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=570000, episode_reward=-185.07 +/- 1.998987604183608 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  185.07  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 570000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.8312   | \n",
      " |    critic_loss     | 0.4578   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 569899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=571000, episode_reward=-185.22 +/- 1.6291900265594008 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  185.22  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 571000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.8322   | \n",
      " |    critic_loss     | 0.4580   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 570899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=572000, episode_reward=-186.26 +/- 1.8693681593534437 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  186.26  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 572000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.4751   | \n",
      " |    critic_loss     | 1.3688   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 571899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=573000, episode_reward=-186.73 +/- 1.2497304331920507 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  186.73  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 573000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.1902   | \n",
      " |    critic_loss     | 1.2976   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 572899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=574000, episode_reward=-187.48 +/- 1.7982321766949396 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  187.48  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 574000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.8894   | \n",
      " |    critic_loss     | 0.9723   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 573899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=575000, episode_reward=-187.08 +/- 1.0045019587249386 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  187.08  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 575000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.6764   | \n",
      " |    critic_loss     | 0.6691   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 574899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=576000, episode_reward=-187.94 +/- 1.0183147105154577 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  187.94  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 576000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.9288   | \n",
      " |    critic_loss     | 0.4822   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 575899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=577000, episode_reward=-188.81 +/- 1.8129754351254768 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  188.81  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 577000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.7288   | \n",
      " |    critic_loss     | 1.1822   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 576899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=578000, episode_reward=-188.7 +/- 1.781578163201198 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   188.7  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 578000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.3521   | \n",
      " |    critic_loss     | 0.8380   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 577899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=579000, episode_reward=-189.45 +/- 1.9708131198635208 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  189.45  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 579000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.6191   | \n",
      " |    critic_loss     | 1.1548   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 578899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=580000, episode_reward=-189.89 +/- 1.982243587256287 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  189.89  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 580000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.2260   | \n",
      " |    critic_loss     | 0.5565   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 579899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=581000, episode_reward=-190.79 +/- 1.1554084283244248 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  190.79  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 581000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.5778   | \n",
      " |    critic_loss     | 1.1445   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 580899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=582000, episode_reward=-191.23 +/- 1.8520633291449566 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  191.23  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 582000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.3704   | \n",
      " |    critic_loss     | 1.3426   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 581899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=583000, episode_reward=-191.1 +/- 1.4195395400107644 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   191.1  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 583000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.7779   | \n",
      " |    critic_loss     | 1.1945   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 582899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=584000, episode_reward=-191.88 +/- 1.0515752804011633 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  191.88  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 584000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.4608   | \n",
      " |    critic_loss     | 1.3652   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 583899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=585000, episode_reward=-192.18 +/- 1.9053720379405994 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  192.18  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 585000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.6634   | \n",
      " |    critic_loss     | 1.4159   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 584899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=586000, episode_reward=-192.78 +/- 1.932600953042397 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  192.78  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 586000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.0045   | \n",
      " |    critic_loss     | 1.2511   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 585899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=587000, episode_reward=-193.74 +/- 1.5682312984853037 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  193.74  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 587000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.8554   | \n",
      " |    critic_loss     | 0.9639   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 586899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=588000, episode_reward=-194.5 +/- 1.971195943598591 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   194.5  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 588000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.7104   | \n",
      " |    critic_loss     | 1.1776   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 587899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=589000, episode_reward=-194.32 +/- 1.6212185870856564 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  194.32  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 589000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.7774   | \n",
      " |    critic_loss     | 0.4443   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 588899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=590000, episode_reward=-194.75 +/- 1.8056341349642282 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  194.75  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 590000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.5089   | \n",
      " |    critic_loss     | 0.8772   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 589899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=591000, episode_reward=-195.49 +/- 1.5883505713827097 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  195.49  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 591000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.3136   | \n",
      " |    critic_loss     | 0.5784   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 590899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=592000, episode_reward=-196.38 +/- 1.761682497529803 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  196.38  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 592000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.1299   | \n",
      " |    critic_loss     | 1.0325   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 591899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=593000, episode_reward=-196.54 +/- 1.624398954332878 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  196.54  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 593000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.3240   | \n",
      " |    critic_loss     | 0.5810   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 592899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=594000, episode_reward=-197.16 +/- 1.0652717702511598 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  197.16  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 594000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.6188   | \n",
      " |    critic_loss     | 1.4047   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 593899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=595000, episode_reward=-197.11 +/- 1.8882238432604577 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  197.11  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 595000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.3830   | \n",
      " |    critic_loss     | 0.5958   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 594899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=596000, episode_reward=-198.12 +/- 1.4829878963263823 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  198.12  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 596000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.1550   | \n",
      " |    critic_loss     | 1.2887   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 595899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=597000, episode_reward=-198.72 +/- 1.213592703186219 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  198.72  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 597000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.3865   | \n",
      " |    critic_loss     | 1.3466   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 596899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=598000, episode_reward=-198.98 +/- 1.7690372479012857 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  198.98  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 598000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.0632   | \n",
      " |    critic_loss     | 1.2658   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 597899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=599000, episode_reward=-199.25 +/- 1.2076604537739741 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  199.25  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 599000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.5205   | \n",
      " |    critic_loss     | 0.8801   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 598899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=600000, episode_reward=-199.66 +/- 1.8535865007988652 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  199.66  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 600000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.8761   | \n",
      " |    critic_loss     | 0.4690   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 599899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=601000, episode_reward=-200.65 +/- 1.8135985438736477 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  200.65  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 601000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.9882   | \n",
      " |    critic_loss     | 0.7470   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 600899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=602000, episode_reward=-201.04 +/- 1.8733498095829952 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  201.04  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 602000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.2025   | \n",
      " |    critic_loss     | 0.8006   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 601899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=603000, episode_reward=-201.34 +/- 1.3309715423103454 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  201.34  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 603000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.5983   | \n",
      " |    critic_loss     | 1.1496   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 602899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=604000, episode_reward=-201.82 +/- 1.0221170240785096 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  201.82  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 604000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.1182   | \n",
      " |    critic_loss     | 1.0296   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 603899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=605000, episode_reward=-202.64 +/- 1.1297131755311853 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  202.64  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 605000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.0170   | \n",
      " |    critic_loss     | 1.2542   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 604899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=606000, episode_reward=-202.87 +/- 1.9729105634727222 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  202.87  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 606000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.5847   | \n",
      " |    critic_loss     | 0.3962   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 605899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=607000, episode_reward=-203.97 +/- 1.0616925942344282 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  203.97  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 607000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.6089   | \n",
      " |    critic_loss     | 0.9022   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 606899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=608000, episode_reward=-203.5 +/- 1.012023113024401 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   203.5  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 608000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.1415   | \n",
      " |    critic_loss     | 1.0354   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 607899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=609000, episode_reward=-204.24 +/- 1.7518513338245527 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  204.24  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 609000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.4791   | \n",
      " |    critic_loss     | 0.8698   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 608899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=610000, episode_reward=-204.75 +/- 1.86809157459603 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  204.75  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 610000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.2510   | \n",
      " |    critic_loss     | 1.0627   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 609899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=611000, episode_reward=-205.86 +/- 1.9249193976479821 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  205.86  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 611000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.5522   | \n",
      " |    critic_loss     | 0.8881   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 610899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=612000, episode_reward=-205.67 +/- 1.0341733992526625 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  205.67  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 612000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.3217   | \n",
      " |    critic_loss     | 1.0804   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 611899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=613000, episode_reward=-206.08 +/- 1.6524500738744703 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  206.08  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 613000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.0948   | \n",
      " |    critic_loss     | 0.7737   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 612899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=614000, episode_reward=-206.77 +/- 1.215663956509728 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  206.77  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 614000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.8134   | \n",
      " |    critic_loss     | 0.9534   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 613899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=615000, episode_reward=-207.81 +/- 1.2972443610850166 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  207.81  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 615000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.2158   | \n",
      " |    critic_loss     | 1.3039   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 614899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=616000, episode_reward=-207.98 +/- 1.7690912786665947 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  207.98  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 616000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.0288   | \n",
      " |    critic_loss     | 1.0072   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 615899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=617000, episode_reward=-208.56 +/- 1.481029434286584 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  208.56  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 617000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.3491   | \n",
      " |    critic_loss     | 0.5873   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 616899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=618000, episode_reward=-208.88 +/- 1.9657459257100367 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  208.88  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 618000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.7598   | \n",
      " |    critic_loss     | 0.6899   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 617899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=619000, episode_reward=-209.79 +/- 1.1069215092446023 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  209.79  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 619000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.1467   | \n",
      " |    critic_loss     | 1.0367   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 618899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=620000, episode_reward=-209.61 +/- 1.9210482653780383 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  209.61  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 620000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.4693   | \n",
      " |    critic_loss     | 1.1173   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 619899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=621000, episode_reward=-210.01 +/- 1.2422071130536059 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  210.01  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 621000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.3000   | \n",
      " |    critic_loss     | 1.0750   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 620899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=622000, episode_reward=-210.56 +/- 1.3661935156919611 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  210.56  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 622000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.6046   | \n",
      " |    critic_loss     | 0.9012   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 621899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=623000, episode_reward=-211.74 +/- 1.7325002717856943 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  211.74  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 623000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.6010   | \n",
      " |    critic_loss     | 0.4003   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 622899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=624000, episode_reward=-211.92 +/- 1.021543257570972 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  211.92  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 624000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.5502   | \n",
      " |    critic_loss     | 0.3875   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 623899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=625000, episode_reward=-212.57 +/- 1.9849332662483428 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  212.57  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 625000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.3707   | \n",
      " |    critic_loss     | 1.0927   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 624899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=626000, episode_reward=-212.94 +/- 1.911247077235375 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  212.94  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 626000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.6848   | \n",
      " |    critic_loss     | 0.6712   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 625899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=627000, episode_reward=-213.19 +/- 1.3386565815649702 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  213.19  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 627000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.4125   | \n",
      " |    critic_loss     | 0.8531   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 626899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=628000, episode_reward=-213.92 +/- 1.0553923532807872 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  213.92  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 628000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.4197   | \n",
      " |    critic_loss     | 1.3549   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 627899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=629000, episode_reward=-214.34 +/- 1.0290975441422858 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  214.34  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 629000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.5388   | \n",
      " |    critic_loss     | 0.6347   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 628899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=630000, episode_reward=-215.44 +/- 1.467436855954915 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  215.44  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 630000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.4052   | \n",
      " |    critic_loss     | 0.6013   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 629899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=631000, episode_reward=-215.69 +/- 1.0035009462975795 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  215.69  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 631000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.1985   | \n",
      " |    critic_loss     | 0.7996   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 630899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=632000, episode_reward=-216.43 +/- 1.352466078897334 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  216.43  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 632000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.6602   | \n",
      " |    critic_loss     | 0.6651   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 631899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=633000, episode_reward=-216.49 +/- 1.6532511072214042 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  216.49  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 633000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.6335   | \n",
      " |    critic_loss     | 0.9084   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 632899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=634000, episode_reward=-217.19 +/- 1.1214179199640653 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  217.19  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 634000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.8716   | \n",
      " |    critic_loss     | 0.7179   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 633899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=635000, episode_reward=-217.95 +/- 1.4159707058702766 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  217.95  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 635000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.1121   | \n",
      " |    critic_loss     | 0.7780   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 634899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=636000, episode_reward=-218.46 +/- 1.4113585665769905 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  218.46  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 636000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.8535   | \n",
      " |    critic_loss     | 0.7134   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 635899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=637000, episode_reward=-218.4 +/- 1.4993905066560642 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   218.4  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 637000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.8548   | \n",
      " |    critic_loss     | 0.7137   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 636899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=638000, episode_reward=-219.25 +/- 1.3043096876253366 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  219.25  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 638000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.2791   | \n",
      " |    critic_loss     | 1.0698   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 637899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=639000, episode_reward=-219.86 +/- 1.4427038051553924 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  219.86  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 639000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.3572   | \n",
      " |    critic_loss     | 0.8393   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 638899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=640000, episode_reward=-219.58 +/- 1.4344827217836673 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  219.58  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 640000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.9793   | \n",
      " |    critic_loss     | 0.9948   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 639899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=641000, episode_reward=-220.81 +/- 1.005480071635693 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  220.81  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 641000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.5257   | \n",
      " |    critic_loss     | 0.3814   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 640899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=642000, episode_reward=-221.36 +/- 1.0974797214249945 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  221.36  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 642000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.2154   | \n",
      " |    critic_loss     | 1.0538   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 641899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=643000, episode_reward=-221.62 +/- 1.4859635253176329 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  221.62  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 643000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.7263   | \n",
      " |    critic_loss     | 0.6816   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 642899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=644000, episode_reward=-222.05 +/- 1.8510760095512189 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  222.05  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 644000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.0952   | \n",
      " |    critic_loss     | 1.2738   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 643899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=645000, episode_reward=-222.08 +/- 1.929315388722475 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  222.08  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 645000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.5501   | \n",
      " |    critic_loss     | 1.1375   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 644899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=646000, episode_reward=-223.26 +/- 1.6251482902867753 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  223.26  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 646000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.3950   | \n",
      " |    critic_loss     | 1.0987   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 645899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=647000, episode_reward=-223.85 +/- 1.1741824922408952 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  223.85  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 647000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.4993   | \n",
      " |    critic_loss     | 1.1248   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 646899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=648000, episode_reward=-223.6 +/- 1.8203679671414856 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   223.6  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 648000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.2934   | \n",
      " |    critic_loss     | 1.3234   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 647899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=649000, episode_reward=-224.45 +/- 1.0491257039965174 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  224.45  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 649000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.3602   | \n",
      " |    critic_loss     | 0.5901   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 648899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=650000, episode_reward=-225.35 +/- 1.0987937834750117 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  225.35  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 650000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.2689   | \n",
      " |    critic_loss     | 1.0672   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 649899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=651000, episode_reward=-225.21 +/- 1.6069658810538465 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  225.21  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 651000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.5612   | \n",
      " |    critic_loss     | 0.8903   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 650899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=652000, episode_reward=-225.92 +/- 1.4435224238217033 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  225.92  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 652000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.7332   | \n",
      " |    critic_loss     | 1.1833   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 651899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=653000, episode_reward=-226.5 +/- 1.2451738681716509 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   226.5  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 653000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.3314   | \n",
      " |    critic_loss     | 1.3329   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 652899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=654000, episode_reward=-226.83 +/- 1.0859319023145821 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  226.83  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 654000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.9280   | \n",
      " |    critic_loss     | 1.2320   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 653899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=655000, episode_reward=-227.6 +/- 1.4567008934036423 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   227.6  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 655000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.1977   | \n",
      " |    critic_loss     | 0.5494   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 654899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=656000, episode_reward=-227.71 +/- 1.3468447142116435 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  227.71  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 656000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.3717   | \n",
      " |    critic_loss     | 1.0929   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 655899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=657000, episode_reward=-228.0 +/- 1.2798516086448428 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   228.0  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 657000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.0469   | \n",
      " |    critic_loss     | 1.0117   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 656899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=658000, episode_reward=-229.09 +/- 1.3547501894848117 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  229.09  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 658000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.7794   | \n",
      " |    critic_loss     | 0.4449   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 657899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=659000, episode_reward=-229.71 +/- 1.1191769174474886 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  229.71  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 659000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.3633   | \n",
      " |    critic_loss     | 1.0908   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 658899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=660000, episode_reward=-229.86 +/- 1.5271595884559832 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  229.86  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 660000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.5052   | \n",
      " |    critic_loss     | 1.1263   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 659899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=661000, episode_reward=-230.77 +/- 1.7101703652684614 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  230.77  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 661000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.9569   | \n",
      " |    critic_loss     | 0.7392   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 660899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=662000, episode_reward=-230.88 +/- 1.4616019884090226 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  230.88  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 662000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.1072   | \n",
      " |    critic_loss     | 0.5268   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 661899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=663000, episode_reward=-231.48 +/- 1.5348689773058029 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  231.48  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 663000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.2439   | \n",
      " |    critic_loss     | 1.0610   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 662899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=664000, episode_reward=-232.43 +/- 1.582946765320595 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  232.43  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 664000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.7055   | \n",
      " |    critic_loss     | 0.9264   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 663899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=665000, episode_reward=-232.83 +/- 1.6095637854247515 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  232.83  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 665000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.0980   | \n",
      " |    critic_loss     | 1.0245   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 664899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=666000, episode_reward=-232.53 +/- 1.3701613393966388 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  232.53  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 666000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.3584   | \n",
      " |    critic_loss     | 0.5896   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 665899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=667000, episode_reward=-233.68 +/- 1.0128160922219023 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  233.68  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 667000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.8462   | \n",
      " |    critic_loss     | 0.9615   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 666899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=668000, episode_reward=-234.09 +/- 1.6749070325372877 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  234.09  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 668000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.7311   | \n",
      " |    critic_loss     | 0.6828   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 667899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=669000, episode_reward=-234.8 +/- 1.4530663139750422 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   234.8  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 669000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.1321   | \n",
      " |    critic_loss     | 0.7830   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 668899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=670000, episode_reward=-234.89 +/- 1.1957781322048717 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  234.89  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 670000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.2843   | \n",
      " |    critic_loss     | 0.8211   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 669899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=671000, episode_reward=-235.92 +/- 1.8821596841627195 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  235.92  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 671000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.8883   | \n",
      " |    critic_loss     | 0.7221   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 670899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=672000, episode_reward=-235.71 +/- 1.9970086826815305 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  235.71  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 672000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.6003   | \n",
      " |    critic_loss     | 1.1501   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 671899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=673000, episode_reward=-236.79 +/- 1.6075140849179403 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  236.79  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 673000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.6967   | \n",
      " |    critic_loss     | 0.9242   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 672899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=674000, episode_reward=-237.45 +/- 1.8700082482927198 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  237.45  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 674000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.2352   | \n",
      " |    critic_loss     | 1.3088   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 673899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=675000, episode_reward=-237.87 +/- 1.3542692541187475 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  237.87  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 675000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.0186   | \n",
      " |    critic_loss     | 0.7546   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 674899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=676000, episode_reward=-237.59 +/- 1.2530214962962245 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  237.59  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 676000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.2967   | \n",
      " |    critic_loss     | 0.8242   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 675899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=677000, episode_reward=-238.52 +/- 1.0903421736311576 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  238.52  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 677000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.0457   | \n",
      " |    critic_loss     | 1.2614   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 676899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=678000, episode_reward=-239.43 +/- 1.0144721175222098 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  239.43  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 678000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.0450   | \n",
      " |    critic_loss     | 0.7612   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 677899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=679000, episode_reward=-239.12 +/- 1.8134162020925935 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  239.12  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 679000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.1826   | \n",
      " |    critic_loss     | 0.7957   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 678899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=680000, episode_reward=-239.66 +/- 1.9977365706734251 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  239.66  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 680000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.9722   | \n",
      " |    critic_loss     | 0.7430   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 679899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=681000, episode_reward=-240.61 +/- 1.2991336458674327 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  240.61  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 681000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.4385   | \n",
      " |    critic_loss     | 0.6096   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 680899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=682000, episode_reward=-240.7 +/- 1.0601650790136015 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   240.7  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 682000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.6019   | \n",
      " |    critic_loss     | 0.4005   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 681899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=683000, episode_reward=-241.89 +/- 1.9307240881922318 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  241.89  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 683000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.4911   | \n",
      " |    critic_loss     | 0.6228   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 682899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=684000, episode_reward=-241.89 +/- 1.8356683223935863 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  241.89  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 684000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.9631   | \n",
      " |    critic_loss     | 0.4908   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 683899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=685000, episode_reward=-242.04 +/- 1.8636410472346832 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  242.04  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 685000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.5949   | \n",
      " |    critic_loss     | 1.1487   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 684899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=686000, episode_reward=-243.11 +/- 1.7502504776604701 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  243.11  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 686000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.7464   | \n",
      " |    critic_loss     | 0.9366   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 685899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=687000, episode_reward=-243.48 +/- 1.5324830254549648 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  243.48  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 687000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.5720   | \n",
      " |    critic_loss     | 0.3930   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 686899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=688000, episode_reward=-244.42 +/- 1.3757007305897462 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  244.42  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 688000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.5231   | \n",
      " |    critic_loss     | 0.3808   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 687899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=689000, episode_reward=-244.05 +/- 1.8311772341351762 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  244.05  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 689000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.8009   | \n",
      " |    critic_loss     | 0.4502   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 688899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=690000, episode_reward=-245.23 +/- 1.980406576142968 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  245.23  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 690000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.7808   | \n",
      " |    critic_loss     | 1.1952   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 689899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=691000, episode_reward=-245.08 +/- 1.767346471560315 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  245.08  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 691000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.8924   | \n",
      " |    critic_loss     | 0.7231   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 690899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=692000, episode_reward=-245.53 +/- 1.6505149427643269 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  245.53  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 692000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.5499   | \n",
      " |    critic_loss     | 0.6375   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 691899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=693000, episode_reward=-246.21 +/- 1.6516255828899755 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  246.21  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 693000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.3624   | \n",
      " |    critic_loss     | 0.8406   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 692899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=694000, episode_reward=-246.81 +/- 1.3156022605840851 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  246.81  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 694000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.3804   | \n",
      " |    critic_loss     | 0.8451   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 693899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=695000, episode_reward=-247.69 +/- 1.22542535516474 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  247.69  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 695000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.0614   | \n",
      " |    critic_loss     | 0.7653   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 694899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=696000, episode_reward=-248.5 +/- 1.8894552991437354 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   248.5  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 696000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.1330   | \n",
      " |    critic_loss     | 1.0333   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 695899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=697000, episode_reward=-248.09 +/- 1.0035874037986825 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  248.09  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 697000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.8873   | \n",
      " |    critic_loss     | 1.2218   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 696899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=698000, episode_reward=-249.13 +/- 1.6860090201833056 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  249.13  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 698000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.0169   | \n",
      " |    critic_loss     | 1.2542   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 697899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=699000, episode_reward=-249.09 +/- 1.5921274556939462 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  249.09  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 699000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 5.1660   | \n",
      " |    critic_loss     | 1.2915   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 698899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=700000, episode_reward=-249.89 +/- 1.243593581497175 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  249.89  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 700000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.4533   | \n",
      " |    critic_loss     | 0.6133   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 699899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=701000, episode_reward=-250.68 +/- 1.1141196263636703 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  250.68  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 701000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.7918   | \n",
      " |    critic_loss     | 0.6979   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 700899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=702000, episode_reward=-251.49 +/- 1.3959628356030604 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  251.49  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 702000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.7800   | \n",
      " |    critic_loss     | 0.6950   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 701899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=703000, episode_reward=-251.33 +/- 1.3642710430475304 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  251.33  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 703000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.6437   | \n",
      " |    critic_loss     | 0.4109   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 702899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=704000, episode_reward=-252.26 +/- 1.8786392209185212 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  252.26  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 704000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.7422   | \n",
      " |    critic_loss     | 0.4355   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 703899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=705000, episode_reward=-252.71 +/- 1.1740531797339426 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  252.71  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 705000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.6822   | \n",
      " |    critic_loss     | 0.4205   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 704899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=706000, episode_reward=-252.66 +/- 1.1600492621996348 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  252.66  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 706000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.1820   | \n",
      " |    critic_loss     | 0.7955   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 705899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=707000, episode_reward=-253.58 +/- 1.209051575136333 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  253.58  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 707000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.7002   | \n",
      " |    critic_loss     | 0.6750   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 706899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=708000, episode_reward=-254.0 +/- 1.7879231321951385 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   254.0  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 708000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.5144   | \n",
      " |    critic_loss     | 0.6286   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 707899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=709000, episode_reward=-254.25 +/- 1.5899929035787026 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  254.25  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 709000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.4809   | \n",
      " |    critic_loss     | 1.1202   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 708899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=710000, episode_reward=-254.9 +/- 1.6855458968444113 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   254.9  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 710000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.1535   | \n",
      " |    critic_loss     | 0.5384   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 709899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=711000, episode_reward=-255.98 +/- 1.9969881149878188 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  255.98  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 711000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.9621   | \n",
      " |    critic_loss     | 0.9905   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 710899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=712000, episode_reward=-256.09 +/- 1.8710985220836933 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  256.09  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 712000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.1448   | \n",
      " |    critic_loss     | 1.0362   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 711899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=713000, episode_reward=-256.33 +/- 1.898638151672411 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  256.33  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 713000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.7025   | \n",
      " |    critic_loss     | 0.6756   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 712899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=714000, episode_reward=-257.16 +/- 1.2831003167997466 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  257.16  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 714000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.2803   | \n",
      " |    critic_loss     | 1.0701   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 713899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=715000, episode_reward=-257.22 +/- 1.7682354893307768 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  257.22  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 715000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.8552   | \n",
      " |    critic_loss     | 0.7138   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 714899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=716000, episode_reward=-258.29 +/- 1.6498373192591858 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  258.29  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 716000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.6925   | \n",
      " |    critic_loss     | 0.6731   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 715899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=717000, episode_reward=-258.91 +/- 1.4746223802416734 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  258.91  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 717000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.3951   | \n",
      " |    critic_loss     | 0.3488   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 716899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=718000, episode_reward=-259.17 +/- 1.7604013452074863 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  259.17  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 718000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.7899   | \n",
      " |    critic_loss     | 0.9475   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 717899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=719000, episode_reward=-259.8 +/- 1.431831187935896 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   259.8  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 719000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.4401   | \n",
      " |    critic_loss     | 0.3600   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 718899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=720000, episode_reward=-260.34 +/- 1.567806826976288 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  260.34  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 720000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.4377   | \n",
      " |    critic_loss     | 0.3594   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 719899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=721000, episode_reward=-260.83 +/- 1.253168624818146 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  260.83  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 721000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.0313   | \n",
      " |    critic_loss     | 0.7578   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 720899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=722000, episode_reward=-260.96 +/- 1.8901169818297894 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  260.96  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 722000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.6010   | \n",
      " |    critic_loss     | 1.1502   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 721899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=723000, episode_reward=-261.91 +/- 1.538789963991872 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  261.91  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 723000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.9238   | \n",
      " |    critic_loss     | 0.9810   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 722899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=724000, episode_reward=-262.23 +/- 1.2315956511743686 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  262.23  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 724000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.8658   | \n",
      " |    critic_loss     | 0.7164   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 723899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=725000, episode_reward=-262.73 +/- 1.2033539293808486 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  262.73  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 725000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.7027   | \n",
      " |    critic_loss     | 0.6757   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 724899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=726000, episode_reward=-262.62 +/- 1.778652479597299 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  262.62  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 726000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.1137   | \n",
      " |    critic_loss     | 0.2784   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 725899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=727000, episode_reward=-263.11 +/- 1.8865500770145052 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  263.11  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 727000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.7803   | \n",
      " |    critic_loss     | 0.9451   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 726899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=728000, episode_reward=-264.11 +/- 1.6968631157961611 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  264.11  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 728000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.8968   | \n",
      " |    critic_loss     | 0.7242   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 727899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=729000, episode_reward=-264.3 +/- 1.8311361904488077 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   264.3  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 729000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.4806   | \n",
      " |    critic_loss     | 0.8701   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 728899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=730000, episode_reward=-264.6 +/- 1.4682437432427076 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   264.6  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 730000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.5041   | \n",
      " |    critic_loss     | 0.3760   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 729899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=731000, episode_reward=-265.12 +/- 1.9087138017365954 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  265.12  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 731000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.1992   | \n",
      " |    critic_loss     | 0.2998   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 730899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=732000, episode_reward=-265.95 +/- 1.0071448969280086 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  265.95  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 732000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.0794   | \n",
      " |    critic_loss     | 0.7699   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 731899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=733000, episode_reward=-266.67 +/- 1.7816706987033064 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  266.67  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 733000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.0830   | \n",
      " |    critic_loss     | 0.7708   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 732899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=734000, episode_reward=-266.77 +/- 1.4498704348254492 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  266.77  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 734000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.0943   | \n",
      " |    critic_loss     | 0.7736   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 733899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=735000, episode_reward=-267.06 +/- 1.9349037813256613 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  267.06  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 735000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.7044   | \n",
      " |    critic_loss     | 1.1761   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 734899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=736000, episode_reward=-268.09 +/- 1.0116212057179934 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  268.09  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 736000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.3900   | \n",
      " |    critic_loss     | 0.3475   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 735899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=737000, episode_reward=-268.14 +/- 1.9541792445148736 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  268.14  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 737000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.7221   | \n",
      " |    critic_loss     | 0.4305   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 736899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=738000, episode_reward=-268.83 +/- 1.5806789858780848 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  268.83  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 738000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.8439   | \n",
      " |    critic_loss     | 0.9610   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 737899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=739000, episode_reward=-269.49 +/- 1.9738458542609705 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  269.49  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 739000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.0370   | \n",
      " |    critic_loss     | 0.5093   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 738899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=740000, episode_reward=-270.37 +/- 1.9460810018820718 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  270.37  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 740000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.7058   | \n",
      " |    critic_loss     | 0.6764   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 739899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=741000, episode_reward=-270.32 +/- 1.6591415417547453 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  270.32  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 741000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.3681   | \n",
      " |    critic_loss     | 0.8420   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 740899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=742000, episode_reward=-271.23 +/- 1.151355683472186 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  271.23  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 742000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.8737   | \n",
      " |    critic_loss     | 1.2184   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 741899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=743000, episode_reward=-271.38 +/- 1.3699057882851573 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  271.38  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 743000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.0692   | \n",
      " |    critic_loss     | 0.2673   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 742899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=744000, episode_reward=-272.26 +/- 1.8914806149846966 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  272.26  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 744000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.6126   | \n",
      " |    critic_loss     | 0.6532   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 743899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=745000, episode_reward=-272.06 +/- 1.84231613791935 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  272.06  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 745000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.4193   | \n",
      " |    critic_loss     | 1.1048   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 744899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=746000, episode_reward=-273.34 +/- 1.3478423945018772 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  273.34  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 746000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.1810   | \n",
      " |    critic_loss     | 0.5453   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 745899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=747000, episode_reward=-273.82 +/- 1.6843412161772071 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  273.82  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 747000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.3134   | \n",
      " |    critic_loss     | 0.8284   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 746899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=748000, episode_reward=-274.25 +/- 1.794109782766891 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  274.25  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 748000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.6562   | \n",
      " |    critic_loss     | 1.1640   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 747899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=749000, episode_reward=-274.22 +/- 1.7404794273051394 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  274.22  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 749000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.0926   | \n",
      " |    critic_loss     | 0.7732   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 748899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=750000, episode_reward=-274.56 +/- 1.5865034646690623 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  274.56  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 750000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.3836   | \n",
      " |    critic_loss     | 1.0959   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 749899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=751000, episode_reward=-275.14 +/- 1.8762734080622592 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  275.14  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 751000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.9095   | \n",
      " |    critic_loss     | 0.9774   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 750899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=752000, episode_reward=-276.37 +/- 1.1568344698175648 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  276.37  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 752000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.0971   | \n",
      " |    critic_loss     | 0.7743   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 751899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=753000, episode_reward=-276.91 +/- 1.8224886794739983 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  276.91  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 753000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.7098   | \n",
      " |    critic_loss     | 0.4275   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 752899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=754000, episode_reward=-277.49 +/- 1.1545552886404042 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  277.49  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 754000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.0357   | \n",
      " |    critic_loss     | 0.2589   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 753899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=755000, episode_reward=-277.74 +/- 1.8991535925311718 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  277.74  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 755000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.3087   | \n",
      " |    critic_loss     | 1.0772   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 754899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=756000, episode_reward=-277.86 +/- 1.6542042418600402 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  277.86  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 756000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.3055   | \n",
      " |    critic_loss     | 0.5764   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 755899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=757000, episode_reward=-278.38 +/- 1.7881384418156587 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  278.38  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 757000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.8119   | \n",
      " |    critic_loss     | 0.4530   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 756899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=758000, episode_reward=-279.24 +/- 1.0597768424523137 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  279.24  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 758000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.5757   | \n",
      " |    critic_loss     | 0.3939   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 757899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=759000, episode_reward=-279.01 +/- 1.0008841537518292 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  279.01  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 759000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.0552   | \n",
      " |    critic_loss     | 1.0138   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 758899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=760000, episode_reward=-280.05 +/- 1.263639334360835 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  280.05  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 760000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.6218   | \n",
      " |    critic_loss     | 0.9054   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 759899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=761000, episode_reward=-280.43 +/- 1.9740152813376457 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  280.43  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 761000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.0589   | \n",
      " |    critic_loss     | 0.5147   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 760899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=762000, episode_reward=-280.99 +/- 1.2597625377759631 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  280.99  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 762000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.1740   | \n",
      " |    critic_loss     | 1.0435   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 761899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=763000, episode_reward=-281.27 +/- 1.9896773219061006 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  281.27  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 763000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.7239   | \n",
      " |    critic_loss     | 0.9310   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 762899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=764000, episode_reward=-281.6 +/- 1.4879824248669251 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   281.6  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 764000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.1504   | \n",
      " |    critic_loss     | 1.0376   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 763899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=765000, episode_reward=-282.34 +/- 1.9807086690794604 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  282.34  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 765000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.5366   | \n",
      " |    critic_loss     | 1.1341   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 764899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=766000, episode_reward=-283.23 +/- 1.9713944018138725 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  283.23  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 766000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.3914   | \n",
      " |    critic_loss     | 0.5979   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 765899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=767000, episode_reward=-283.85 +/- 1.2651360726192387 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  283.85  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 767000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.8664   | \n",
      " |    critic_loss     | 1.2166   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 766899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=768000, episode_reward=-284.45 +/- 1.091214468829488 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  284.45  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 768000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.3272   | \n",
      " |    critic_loss     | 0.8318   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 767899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=769000, episode_reward=-284.23 +/- 1.0058156670974019 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  284.23  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 769000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.9680   | \n",
      " |    critic_loss     | 0.9920   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 768899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=770000, episode_reward=-285.42 +/- 1.3661718712618738 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  285.42  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 770000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 0.9270   | \n",
      " |    critic_loss     | 0.2317   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 769899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=771000, episode_reward=-285.92 +/- 1.9986770870261297 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  285.92  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 771000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.1770   | \n",
      " |    critic_loss     | 1.0443   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 770899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=772000, episode_reward=-286.46 +/- 1.5869984244367497 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  286.46  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 772000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.5232   | \n",
      " |    critic_loss     | 0.8808   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 771899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=773000, episode_reward=-286.56 +/- 1.7224705144009866 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  286.56  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 773000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.2025   | \n",
      " |    critic_loss     | 0.3006   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 772899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=774000, episode_reward=-286.78 +/- 1.5776770835316014 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  286.78  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 774000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.8264   | \n",
      " |    critic_loss     | 0.4566   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 773899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=775000, episode_reward=-287.06 +/- 1.1675397277319983 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  287.06  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 775000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.8271   | \n",
      " |    critic_loss     | 0.9568   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 774899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=776000, episode_reward=-287.69 +/- 1.7886135766203863 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  287.69  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 776000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.4301   | \n",
      " |    critic_loss     | 1.1075   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 775899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=777000, episode_reward=-288.89 +/- 1.6341766267681233 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  288.89  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 777000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.7328   | \n",
      " |    critic_loss     | 0.4332   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 776899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=778000, episode_reward=-288.98 +/- 1.2939291403052953 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  288.98  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 778000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.0182   | \n",
      " |    critic_loss     | 0.5045   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 777899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=779000, episode_reward=-289.06 +/- 1.8408164455304092 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  289.06  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 779000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.6359   | \n",
      " |    critic_loss     | 1.1590   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 778899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=780000, episode_reward=-290.36 +/- 1.8859713511246388 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  290.36  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 780000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.8624   | \n",
      " |    critic_loss     | 0.4656   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 779899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=781000, episode_reward=-290.59 +/- 1.5441739735889186 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  290.59  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 781000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.0145   | \n",
      " |    critic_loss     | 1.0036   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 780899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=782000, episode_reward=-291.4 +/- 1.1527528254072656 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   291.4  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 782000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.5068   | \n",
      " |    critic_loss     | 0.3767   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 781899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=783000, episode_reward=-291.08 +/- 1.9839117113174045 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  291.08  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 783000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.7068   | \n",
      " |    critic_loss     | 0.4267   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 782899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=784000, episode_reward=-291.99 +/- 1.1984730823806427 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  291.99  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 784000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.0080   | \n",
      " |    critic_loss     | 0.7520   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 783899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=785000, episode_reward=-292.61 +/- 1.1239036942822453 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  292.61  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 785000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.2831   | \n",
      " |    critic_loss     | 0.5708   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 784899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=786000, episode_reward=-293.49 +/- 1.017988068375259 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  293.49  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 786000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.7060   | \n",
      " |    critic_loss     | 0.4265   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 785899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=787000, episode_reward=-293.32 +/- 1.950285403389495 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  293.32  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 787000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.7877   | \n",
      " |    critic_loss     | 1.1969   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 786899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=788000, episode_reward=-294.12 +/- 1.2618344275133624 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  294.12  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 788000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.6726   | \n",
      " |    critic_loss     | 0.4181   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 787899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=789000, episode_reward=-294.95 +/- 1.9086816951503878 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  294.95  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 789000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.7093   | \n",
      " |    critic_loss     | 0.4273   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 788899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=790000, episode_reward=-294.84 +/- 1.7759940811190584 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  294.84  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 790000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.5724   | \n",
      " |    critic_loss     | 0.6431   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 789899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=791000, episode_reward=-295.02 +/- 1.2869015756786957 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  295.02  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 791000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.2699   | \n",
      " |    critic_loss     | 0.3175   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 790899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=792000, episode_reward=-295.7 +/- 1.6370703373518718 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   295.7  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 792000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.1504   | \n",
      " |    critic_loss     | 0.7876   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 791899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=793000, episode_reward=-296.83 +/- 1.741437326691266 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  296.83  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 793000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.3362   | \n",
      " |    critic_loss     | 0.5840   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 792899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=794000, episode_reward=-297.05 +/- 1.742172456565184 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  297.05  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 794000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.8297   | \n",
      " |    critic_loss     | 0.9574   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 793899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=795000, episode_reward=-297.38 +/- 1.9203578311882337 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  297.38  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 795000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.9072   | \n",
      " |    critic_loss     | 0.4768   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 794899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=796000, episode_reward=-297.58 +/- 1.1576382687088596 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  297.58  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 796000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.7273   | \n",
      " |    critic_loss     | 1.1818   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 795899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=797000, episode_reward=-298.38 +/- 1.0354592073982727 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  298.38  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 797000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.8023   | \n",
      " |    critic_loss     | 1.2006   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 796899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=798000, episode_reward=-298.94 +/- 1.093646382371269 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  298.94  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 798000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.1205   | \n",
      " |    critic_loss     | 1.0301   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 797899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=799000, episode_reward=-299.8 +/- 1.8052176185299516 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   299.8  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 799000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.6373   | \n",
      " |    critic_loss     | 1.1593   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 798899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=800000, episode_reward=-300.29 +/- 1.5688340565232695 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  300.29  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 800000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.1353   | \n",
      " |    critic_loss     | 0.5338   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 799899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=801000, episode_reward=-300.61 +/- 1.3425662671820193 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  300.61  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 801000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.3003   | \n",
      " |    critic_loss     | 0.3251   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 800899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=802000, episode_reward=-300.81 +/- 1.564636640551432 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  300.81  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 802000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.1526   | \n",
      " |    critic_loss     | 0.5382   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 801899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=803000, episode_reward=-301.35 +/- 1.5135046281017583 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  301.35  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 803000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.7697   | \n",
      " |    critic_loss     | 0.9424   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 802899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=804000, episode_reward=-301.7 +/- 1.519386298298563 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   301.7  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 804000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.9468   | \n",
      " |    critic_loss     | 0.4867   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 803899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=805000, episode_reward=-302.97 +/- 1.7729303998349106 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  302.97  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 805000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.1732   | \n",
      " |    critic_loss     | 1.0433   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 804899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=806000, episode_reward=-303.35 +/- 1.763751368101934 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  303.35  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 806000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.6749   | \n",
      " |    critic_loss     | 0.4187   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 805899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=807000, episode_reward=-303.78 +/- 1.6520867139755178 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  303.78  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 807000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.1007   | \n",
      " |    critic_loss     | 0.2752   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 806899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=808000, episode_reward=-303.85 +/- 1.095329171175368 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  303.85  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 808000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.1098   | \n",
      " |    critic_loss     | 0.5274   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 807899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=809000, episode_reward=-304.7 +/- 1.7241311449816856 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   304.7  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 809000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.9863   | \n",
      " |    critic_loss     | 0.7466   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 808899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=810000, episode_reward=-305.25 +/- 1.3495742588446764 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  305.25  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 810000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.7739   | \n",
      " |    critic_loss     | 0.6935   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 809899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=811000, episode_reward=-305.83 +/- 1.889454012835236 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  305.83  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 811000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.1622   | \n",
      " |    critic_loss     | 0.2905   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 810899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=812000, episode_reward=-306.48 +/- 1.8845825564043988 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  306.48  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 812000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.8510   | \n",
      " |    critic_loss     | 0.7128   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 811899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=813000, episode_reward=-306.24 +/- 1.1458544500808734 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  306.24  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 813000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.5584   | \n",
      " |    critic_loss     | 0.6396   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 812899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=814000, episode_reward=-307.23 +/- 1.1214003544700701 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  307.23  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 814000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.4284   | \n",
      " |    critic_loss     | 1.1071   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 813899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=815000, episode_reward=-307.18 +/- 1.7353466832487077 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  307.18  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 815000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.8603   | \n",
      " |    critic_loss     | 0.9651   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 814899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=816000, episode_reward=-308.28 +/- 1.3174316305206988 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  308.28  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 816000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.4296   | \n",
      " |    critic_loss     | 0.8574   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 815899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=817000, episode_reward=-308.52 +/- 1.18029966631161 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  308.52  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 817000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.5839   | \n",
      " |    critic_loss     | 0.6460   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 816899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=818000, episode_reward=-309.02 +/- 1.4511582127740528 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  309.02  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 818000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.0186   | \n",
      " |    critic_loss     | 0.5046   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 817899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=819000, episode_reward=-309.58 +/- 1.3815594442808496 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  309.58  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 819000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.0675   | \n",
      " |    critic_loss     | 0.2669   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 818899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=820000, episode_reward=-309.74 +/- 1.5751609246952971 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  309.74  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 820000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.4352   | \n",
      " |    critic_loss     | 0.6088   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 819899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=821000, episode_reward=-310.5 +/- 1.8163495289969958 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   310.5  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 821000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.5341   | \n",
      " |    critic_loss     | 0.3835   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 820899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=822000, episode_reward=-310.68 +/- 1.710937823581158 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  310.68  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 822000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.9093   | \n",
      " |    critic_loss     | 0.9773   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 821899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=823000, episode_reward=-311.84 +/- 1.019344586434186 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  311.84  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 823000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.8241   | \n",
      " |    critic_loss     | 0.9560   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 822899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=824000, episode_reward=-311.77 +/- 1.401130115456803 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  311.77  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 824000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.6929   | \n",
      " |    critic_loss     | 0.6732   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 823899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=825000, episode_reward=-312.3 +/- 1.6006785344180963 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   312.3  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 825000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.5750   | \n",
      " |    critic_loss     | 0.6438   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 824899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=826000, episode_reward=-312.62 +/- 1.0630309991365827 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  312.62  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 826000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.4945   | \n",
      " |    critic_loss     | 0.6236   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 825899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=827000, episode_reward=-313.49 +/- 1.8806461930968412 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  313.49  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 827000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.3725   | \n",
      " |    critic_loss     | 0.8431   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 826899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=828000, episode_reward=-314.03 +/- 1.4940732001096517 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  314.03  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 828000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.0054   | \n",
      " |    critic_loss     | 0.5013   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 827899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=829000, episode_reward=-314.52 +/- 1.015134588187483 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  314.52  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 829000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.9431   | \n",
      " |    critic_loss     | 0.4858   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 828899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=830000, episode_reward=-314.8 +/- 1.8712724862627157 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   314.8  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 830000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.2679   | \n",
      " |    critic_loss     | 0.5670   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 829899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=831000, episode_reward=-315.81 +/- 1.8911918676234143 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  315.81  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 831000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.1730   | \n",
      " |    critic_loss     | 1.0432   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 830899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=832000, episode_reward=-316.04 +/- 1.34148898478389 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  316.04  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 832000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.4954   | \n",
      " |    critic_loss     | 1.1238   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 831899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=833000, episode_reward=-316.06 +/- 1.0566979024277146 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  316.06  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 833000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.4794   | \n",
      " |    critic_loss     | 0.3698   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 832899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=834000, episode_reward=-316.81 +/- 1.7529039263525057 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  316.81  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 834000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.3023   | \n",
      " |    critic_loss     | 0.5756   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 833899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=835000, episode_reward=-317.47 +/- 1.8256023332366391 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  317.47  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 835000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.5311   | \n",
      " |    critic_loss     | 0.8828   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 834899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=836000, episode_reward=-318.04 +/- 1.7876541471718665 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  318.04  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 836000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.8031   | \n",
      " |    critic_loss     | 0.4508   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 835899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=837000, episode_reward=-318.37 +/- 1.3505491130332348 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  318.37  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 837000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.6243   | \n",
      " |    critic_loss     | 0.4061   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 836899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=838000, episode_reward=-318.89 +/- 1.9017382960931233 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  318.89  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 838000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.3003   | \n",
      " |    critic_loss     | 0.3251   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 837899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=839000, episode_reward=-319.88 +/- 1.9414947252459143 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  319.88  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 839000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 0.7404   | \n",
      " |    critic_loss     | 0.1851   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 838899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=840000, episode_reward=-320.13 +/- 1.1408391169229275 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  320.13  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 840000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.7532   | \n",
      " |    critic_loss     | 0.4383   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 839899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=841000, episode_reward=-320.05 +/- 1.0396568892841214 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  320.05  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 841000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.2050   | \n",
      " |    critic_loss     | 0.5513   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 840899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=842000, episode_reward=-321.35 +/- 1.294350380181309 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  321.35  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 842000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.9852   | \n",
      " |    critic_loss     | 0.7463   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 841899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=843000, episode_reward=-321.51 +/- 1.4319329269258667 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  321.51  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 843000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.2083   | \n",
      " |    critic_loss     | 0.8021   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 842899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=844000, episode_reward=-321.7 +/- 1.1152236495620387 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   321.7  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 844000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.4179   | \n",
      " |    critic_loss     | 0.6045   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 843899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=845000, episode_reward=-322.3 +/- 1.4499292236247838 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   322.3  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 845000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.7454   | \n",
      " |    critic_loss     | 0.9364   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 844899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=846000, episode_reward=-323.1 +/- 1.1038809345347578 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   323.1  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 846000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.4112   | \n",
      " |    critic_loss     | 0.8528   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 845899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=847000, episode_reward=-323.5 +/- 1.1717749630210013 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   323.5  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 847000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.2279   | \n",
      " |    critic_loss     | 0.5570   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 846899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=848000, episode_reward=-323.83 +/- 1.5098614190306336 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  323.83  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 848000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.4770   | \n",
      " |    critic_loss     | 0.6192   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 847899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=849000, episode_reward=-324.31 +/- 1.0363357461555953 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  324.31  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 849000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.8815   | \n",
      " |    critic_loss     | 0.4704   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 848899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=850000, episode_reward=-325.26 +/- 1.0407338501344245 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  325.26  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 850000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 0.7661   | \n",
      " |    critic_loss     | 0.1915   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 849899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=851000, episode_reward=-325.87 +/- 1.5045824030722939 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  325.87  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 851000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.9901   | \n",
      " |    critic_loss     | 0.4975   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 850899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=852000, episode_reward=-325.82 +/- 1.5155395915574923 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  325.82  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 852000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.0399   | \n",
      " |    critic_loss     | 1.0100   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 851899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=853000, episode_reward=-326.55 +/- 1.1938901714631158 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  326.55  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 853000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.9301   | \n",
      " |    critic_loss     | 0.7325   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 852899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=854000, episode_reward=-326.93 +/- 1.6967771081624825 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  326.93  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 854000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.4217   | \n",
      " |    critic_loss     | 1.1054   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 853899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=855000, episode_reward=-327.34 +/- 1.5039974355216563 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  327.34  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 855000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.8314   | \n",
      " |    critic_loss     | 0.4578   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 854899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=856000, episode_reward=-327.86 +/- 1.638675284284445 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  327.86  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 856000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.1252   | \n",
      " |    critic_loss     | 0.7813   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 855899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=857000, episode_reward=-328.68 +/- 1.6480487260609087 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  328.68  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 857000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.2878   | \n",
      " |    critic_loss     | 0.3219   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 856899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=858000, episode_reward=-328.62 +/- 1.8606815737674762 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  328.62  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 858000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 0.8294   | \n",
      " |    critic_loss     | 0.2074   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 857899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=859000, episode_reward=-329.22 +/- 1.4292703234647512 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  329.22  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 859000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.0991   | \n",
      " |    critic_loss     | 0.2748   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 858899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=860000, episode_reward=-329.93 +/- 1.3563848292271228 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  329.93  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 860000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.1973   | \n",
      " |    critic_loss     | 0.7993   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 859899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=861000, episode_reward=-330.38 +/- 1.4087260200624407 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  330.38  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 861000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.3431   | \n",
      " |    critic_loss     | 0.5858   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 860899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=862000, episode_reward=-331.41 +/- 1.594149562187762 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  331.41  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 862000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.9650   | \n",
      " |    critic_loss     | 0.7412   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 861899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=863000, episode_reward=-331.72 +/- 1.3095754864816729 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  331.72  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 863000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.5770   | \n",
      " |    critic_loss     | 0.8942   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 862899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=864000, episode_reward=-331.78 +/- 1.7405581808263153 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  331.78  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 864000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.7189   | \n",
      " |    critic_loss     | 0.4297   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 863899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=865000, episode_reward=-332.5 +/- 1.4333070847493294 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   332.5  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 865000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.4358   | \n",
      " |    critic_loss     | 0.8590   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 864899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=866000, episode_reward=-333.33 +/- 1.1510890983446556 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  333.33  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 866000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.6187   | \n",
      " |    critic_loss     | 0.9047   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 865899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=867000, episode_reward=-333.16 +/- 1.4203001750514703 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  333.16  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 867000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.7584   | \n",
      " |    critic_loss     | 0.4396   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 866899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=868000, episode_reward=-333.96 +/- 1.2800089088820745 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  333.96  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 868000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.7117   | \n",
      " |    critic_loss     | 0.6779   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 867899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=869000, episode_reward=-334.74 +/- 1.060580427603564 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  334.74  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 869000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.3041   | \n",
      " |    critic_loss     | 0.3260   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 868899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=870000, episode_reward=-335.27 +/- 1.35588768450904 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  335.27  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 870000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 0.7299   | \n",
      " |    critic_loss     | 0.1825   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 869899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=871000, episode_reward=-335.8 +/- 1.5409714114664754 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   335.8  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 871000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.2675   | \n",
      " |    critic_loss     | 0.5669   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 870899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=872000, episode_reward=-336.05 +/- 1.8486171445283333 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  336.05  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 872000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.6502   | \n",
      " |    critic_loss     | 0.4125   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 871899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=873000, episode_reward=-336.9 +/- 1.2352165098941232 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   336.9  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 873000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.3248   | \n",
      " |    critic_loss     | 0.5812   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 872899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=874000, episode_reward=-337.31 +/- 1.8215409884067222 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  337.31  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 874000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.9065   | \n",
      " |    critic_loss     | 0.7266   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 873899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=875000, episode_reward=-337.81 +/- 1.4062644303225207 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  337.81  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 875000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.9166   | \n",
      " |    critic_loss     | 0.9791   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 874899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=876000, episode_reward=-338.16 +/- 1.9163955063994145 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  338.16  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 876000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 0.7993   | \n",
      " |    critic_loss     | 0.1998   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 875899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=877000, episode_reward=-338.03 +/- 1.6299165506498148 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  338.03  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 877000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.2106   | \n",
      " |    critic_loss     | 0.3026   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 876899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=878000, episode_reward=-339.41 +/- 1.2751890291689623 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  339.41  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 878000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.5994   | \n",
      " |    critic_loss     | 0.3998   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 877899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=879000, episode_reward=-339.39 +/- 1.4315006567434723 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  339.39  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 879000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.8760   | \n",
      " |    critic_loss     | 0.4690   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 878899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=880000, episode_reward=-339.68 +/- 1.13455827733322 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  339.68  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 880000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.3200   | \n",
      " |    critic_loss     | 0.5800   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 879899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=881000, episode_reward=-340.26 +/- 1.9853261791451249 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  340.26  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 881000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.7334   | \n",
      " |    critic_loss     | 0.6834   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 880899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=882000, episode_reward=-341.47 +/- 1.9045597276661854 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  341.47  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 882000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 0.7117   | \n",
      " |    critic_loss     | 0.1779   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 881899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=883000, episode_reward=-341.36 +/- 1.5146134851930095 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  341.36  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 883000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.1747   | \n",
      " |    critic_loss     | 0.2937   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 882899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=884000, episode_reward=-342.04 +/- 1.1487096582474474 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  342.04  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 884000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.2211   | \n",
      " |    critic_loss     | 1.0553   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 883899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=885000, episode_reward=-342.31 +/- 1.1210105141589635 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  342.31  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 885000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.1264   | \n",
      " |    critic_loss     | 0.5316   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 884899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=886000, episode_reward=-343.49 +/- 1.7647517028171849 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  343.49  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 886000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.5233   | \n",
      " |    critic_loss     | 0.3808   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 885899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=887000, episode_reward=-343.26 +/- 1.4190604636379747 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  343.26  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 887000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.4394   | \n",
      " |    critic_loss     | 0.6098   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 886899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=888000, episode_reward=-343.76 +/- 1.837731764038618 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  343.76  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 888000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.4474   | \n",
      " |    critic_loss     | 1.1119   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 887899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=889000, episode_reward=-344.66 +/- 1.145478174206163 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  344.66  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 889000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.0894   | \n",
      " |    critic_loss     | 0.2724   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 888899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=890000, episode_reward=-345.49 +/- 1.636385860934556 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  345.49  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 890000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.6149   | \n",
      " |    critic_loss     | 0.6537   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 889899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=891000, episode_reward=-345.4 +/- 1.771226589241178 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   345.4  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 891000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 0.8028   | \n",
      " |    critic_loss     | 0.2007   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 890899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=892000, episode_reward=-346.35 +/- 1.7023189169923074 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  346.35  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 892000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.1985   | \n",
      " |    critic_loss     | 0.2996   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 891899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=893000, episode_reward=-346.44 +/- 1.9659259549545165 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  346.44  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 893000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.5154   | \n",
      " |    critic_loss     | 0.6288   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 892899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=894000, episode_reward=-347.39 +/- 1.0325395459382642 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  347.39  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 894000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.9493   | \n",
      " |    critic_loss     | 0.9873   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 893899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=895000, episode_reward=-348.0 +/- 1.1377086693337168 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   348.0  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 895000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.1656   | \n",
      " |    critic_loss     | 0.2914   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 894899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=896000, episode_reward=-348.05 +/- 1.6407943638112565 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  348.05  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 896000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.4220   | \n",
      " |    critic_loss     | 0.3555   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 895899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=897000, episode_reward=-348.98 +/- 1.5274404388973841 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  348.98  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 897000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.5784   | \n",
      " |    critic_loss     | 0.8946   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 896899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=898000, episode_reward=-348.91 +/- 1.4807005753945695 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  348.91  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 898000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.0090   | \n",
      " |    critic_loss     | 0.5022   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 897899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=899000, episode_reward=-349.38 +/- 1.6958540556403472 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  349.38  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 899000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.9486   | \n",
      " |    critic_loss     | 0.9872   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 898899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=900000, episode_reward=-350.44 +/- 1.8955223987898524 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  350.44  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 900000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.3780   | \n",
      " |    critic_loss     | 0.3445   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 899899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=901000, episode_reward=-350.0 +/- 1.2417326607033603 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   350.0  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 901000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 0.5307   | \n",
      " |    critic_loss     | 0.1327   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 900899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=902000, episode_reward=-350.75 +/- 1.6043223539758085 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  350.75  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 902000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.6003   | \n",
      " |    critic_loss     | 0.6501   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 901899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=903000, episode_reward=-351.86 +/- 1.293951542106201 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  351.86  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 903000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.3270   | \n",
      " |    critic_loss     | 0.3317   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 902899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=904000, episode_reward=-351.98 +/- 1.4180712910831899 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  351.98  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 904000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 0.9185   | \n",
      " |    critic_loss     | 0.2296   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 903899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=905000, episode_reward=-352.49 +/- 1.2660026641055133 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  352.49  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 905000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.1685   | \n",
      " |    critic_loss     | 1.0421   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 904899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=906000, episode_reward=-352.7 +/- 1.352533224614096 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   352.7  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 906000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.4352   | \n",
      " |    critic_loss     | 0.6088   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 905899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=907000, episode_reward=-353.05 +/- 1.8932178679560683 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  353.05  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 907000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.3638   | \n",
      " |    critic_loss     | 0.5910   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 906899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=908000, episode_reward=-353.68 +/- 1.8706535304734964 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  353.68  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 908000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.1894   | \n",
      " |    critic_loss     | 1.0473   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 907899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=909000, episode_reward=-354.6 +/- 1.8368893224627383 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   354.6  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 909000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.5033   | \n",
      " |    critic_loss     | 0.6258   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 908899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=910000, episode_reward=-354.56 +/- 1.4507042350615849 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  354.56  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 910000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.5020   | \n",
      " |    critic_loss     | 0.3755   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 909899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=911000, episode_reward=-355.99 +/- 1.2487148714121383 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  355.99  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 911000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.6937   | \n",
      " |    critic_loss     | 0.6734   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 910899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=912000, episode_reward=-356.03 +/- 1.9139971804597469 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  356.03  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 912000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.1186   | \n",
      " |    critic_loss     | 0.7796   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 911899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=913000, episode_reward=-356.66 +/- 1.368619443556358 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  356.66  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 913000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.2587   | \n",
      " |    critic_loss     | 0.3147   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 912899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=914000, episode_reward=-357.48 +/- 1.9506765258745846 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  357.48  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 914000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.2179   | \n",
      " |    critic_loss     | 0.5545   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 913899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=915000, episode_reward=-357.5 +/- 1.9259566370551586 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   357.5  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 915000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.4888   | \n",
      " |    critic_loss     | 0.8722   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 914899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=916000, episode_reward=-358.35 +/- 1.9527755145018129 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  358.35  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 916000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.4937   | \n",
      " |    critic_loss     | 0.6234   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 915899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=917000, episode_reward=-358.95 +/- 1.2028272742748225 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  358.95  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 917000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.7779   | \n",
      " |    critic_loss     | 0.4445   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 916899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=918000, episode_reward=-359.26 +/- 1.4844023200062626 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  359.26  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 918000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.9251   | \n",
      " |    critic_loss     | 0.9813   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 917899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=919000, episode_reward=-359.6 +/- 1.3285176755519816 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   359.6  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 919000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 0.6222   | \n",
      " |    critic_loss     | 0.1556   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 918899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=920000, episode_reward=-359.9 +/- 1.7840820281962717 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   359.9  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 920000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.1959   | \n",
      " |    critic_loss     | 1.0490   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 919899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=921000, episode_reward=-360.58 +/- 1.1287509025883393 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  360.58  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 921000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.9591   | \n",
      " |    critic_loss     | 0.7398   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 920899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=922000, episode_reward=-361.16 +/- 1.9682300148944119 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  361.16  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 922000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.1677   | \n",
      " |    critic_loss     | 1.0419   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 921899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=923000, episode_reward=-361.28 +/- 1.201663967004038 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  361.28  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 923000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.0110   | \n",
      " |    critic_loss     | 0.5028   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 922899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=924000, episode_reward=-362.02 +/- 1.2004329226960442 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  362.02  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 924000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.7598   | \n",
      " |    critic_loss     | 0.6899   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 923899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=925000, episode_reward=-362.1 +/- 1.0717700218662425 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   362.1  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 925000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 0.4344   | \n",
      " |    critic_loss     | 0.1086   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 924899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=926000, episode_reward=-362.9 +/- 1.1145361492403625 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   362.9  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 926000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 4.1409   | \n",
      " |    critic_loss     | 1.0352   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 925899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=927000, episode_reward=-363.76 +/- 1.2281017922974118 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  363.76  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 927000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.8361   | \n",
      " |    critic_loss     | 0.9590   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 926899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=928000, episode_reward=-363.85 +/- 1.2410940932692074 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  363.85  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 928000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 0.8015   | \n",
      " |    critic_loss     | 0.2004   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 927899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=929000, episode_reward=-364.29 +/- 1.4144819257518395 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  364.29  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 929000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.7626   | \n",
      " |    critic_loss     | 0.4406   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 928899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=930000, episode_reward=-364.66 +/- 1.2404511333711032 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  364.66  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 930000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.8040   | \n",
      " |    critic_loss     | 0.9510   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 929899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=931000, episode_reward=-365.46 +/- 1.6696968522890905 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  365.46  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 931000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.8256   | \n",
      " |    critic_loss     | 0.7064   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 930899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=932000, episode_reward=-365.8 +/- 1.7431064785692763 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   365.8  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 932000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.2373   | \n",
      " |    critic_loss     | 0.5593   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 931899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=933000, episode_reward=-366.63 +/- 1.7906858458784254 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  366.63  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 933000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 0.5164   | \n",
      " |    critic_loss     | 0.1291   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 932899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=934000, episode_reward=-367.39 +/- 1.8322452723423899 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  367.39  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 934000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.2996   | \n",
      " |    critic_loss     | 0.8249   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 933899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=935000, episode_reward=-367.92 +/- 1.5138294385056936 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  367.92  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 935000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 0.6162   | \n",
      " |    critic_loss     | 0.1541   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 934899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=936000, episode_reward=-368.06 +/- 1.4437018433312951 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  368.06  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 936000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.4479   | \n",
      " |    critic_loss     | 0.6120   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 935899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=937000, episode_reward=-368.79 +/- 1.8397602979609573 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  368.79  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 937000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 0.3458   | \n",
      " |    critic_loss     | 0.0864   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 936899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=938000, episode_reward=-369.15 +/- 1.9846207306504953 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  369.15  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 938000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.1033   | \n",
      " |    critic_loss     | 0.7758   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 937899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=939000, episode_reward=-369.49 +/- 1.589497996257253 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  369.49  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 939000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.8635   | \n",
      " |    critic_loss     | 0.4659   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 938899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=940000, episode_reward=-370.43 +/- 1.3270587597993049 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  370.43  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 940000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.0469   | \n",
      " |    critic_loss     | 0.2617   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 939899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=941000, episode_reward=-370.25 +/- 1.0609266990108521 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  370.25  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 941000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.9746   | \n",
      " |    critic_loss     | 0.9936   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 940899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=942000, episode_reward=-371.36 +/- 1.9459141603945478 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  371.36  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 942000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.1105   | \n",
      " |    critic_loss     | 0.7776   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 941899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=943000, episode_reward=-371.19 +/- 1.7331079810103018 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  371.19  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 943000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.3230   | \n",
      " |    critic_loss     | 0.3307   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 942899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=944000, episode_reward=-372.3 +/- 1.1499415209113675 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   372.3  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 944000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.4087   | \n",
      " |    critic_loss     | 0.6022   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 943899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=945000, episode_reward=-372.93 +/- 1.2967409245845576 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  372.93  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 945000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.3067   | \n",
      " |    critic_loss     | 0.8267   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 944899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=946000, episode_reward=-373.39 +/- 1.7028884790571417 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  373.39  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 946000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.5020   | \n",
      " |    critic_loss     | 0.6255   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 945899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=947000, episode_reward=-373.47 +/- 1.2489741866393778 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  373.47  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 947000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.9180   | \n",
      " |    critic_loss     | 0.9795   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 946899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=948000, episode_reward=-374.3 +/- 1.3963670245708677 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   374.3  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 948000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.7816   | \n",
      " |    critic_loss     | 0.4454   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 947899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=949000, episode_reward=-374.23 +/- 1.7405323670041262 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  374.23  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 949000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.3682   | \n",
      " |    critic_loss     | 0.8421   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 948899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=950000, episode_reward=-374.66 +/- 1.4718355458073826 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  374.66  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 950000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 0.7998   | \n",
      " |    critic_loss     | 0.1999   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 949899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=951000, episode_reward=-375.29 +/- 1.832432928687058 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  375.29  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 951000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.7908   | \n",
      " |    critic_loss     | 0.9477   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 950899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=952000, episode_reward=-375.82 +/- 1.6462767521166457 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  375.82  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 952000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.8276   | \n",
      " |    critic_loss     | 0.4569   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 951899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=953000, episode_reward=-376.58 +/- 1.7234492158978645 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  376.58  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 953000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.5993   | \n",
      " |    critic_loss     | 0.6498   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 952899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=954000, episode_reward=-377.11 +/- 1.6026047258829044 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  377.11  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 954000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.6357   | \n",
      " |    critic_loss     | 0.4089   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 953899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=955000, episode_reward=-377.73 +/- 1.904687387787936 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  377.73  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 955000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.5268   | \n",
      " |    critic_loss     | 0.6317   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 954899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=956000, episode_reward=-378.14 +/- 1.663548857225458 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  378.14  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 956000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 0.9608   | \n",
      " |    critic_loss     | 0.2402   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 955899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=957000, episode_reward=-378.23 +/- 1.725364369402044 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  378.23  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 957000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.1566   | \n",
      " |    critic_loss     | 0.5391   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 956899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=958000, episode_reward=-379.29 +/- 1.1053761849102162 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  379.29  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 958000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.3540   | \n",
      " |    critic_loss     | 0.5885   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 957899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=959000, episode_reward=-379.14 +/- 1.2603850240729844 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  379.14  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 959000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.9322   | \n",
      " |    critic_loss     | 0.4830   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 958899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=960000, episode_reward=-379.93 +/- 1.0192752916602619 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  379.93  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 960000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.9930   | \n",
      " |    critic_loss     | 0.7483   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 959899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=961000, episode_reward=-380.11 +/- 1.0805798253704826 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  380.11  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 961000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.2703   | \n",
      " |    critic_loss     | 0.3176   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 960899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=962000, episode_reward=-380.97 +/- 1.7690834666895345 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  380.97  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 962000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 0.5642   | \n",
      " |    critic_loss     | 0.1411   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 961899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=963000, episode_reward=-381.94 +/- 1.201372716091711 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  381.94  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 963000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.1737   | \n",
      " |    critic_loss     | 0.7934   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 962899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=964000, episode_reward=-381.59 +/- 1.4715747077052377 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  381.59  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 964000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 0.6392   | \n",
      " |    critic_loss     | 0.1598   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 963899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=965000, episode_reward=-382.82 +/- 1.9168755516854046 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  382.82  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 965000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.7057   | \n",
      " |    critic_loss     | 0.9264   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 964899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=966000, episode_reward=-382.52 +/- 1.3324602630095643 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  382.52  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 966000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.9351   | \n",
      " |    critic_loss     | 0.4838   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 965899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=967000, episode_reward=-383.3 +/- 1.7439891709182502 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   383.3  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 967000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 0.8092   | \n",
      " |    critic_loss     | 0.2023   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 966899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=968000, episode_reward=-383.61 +/- 1.6705099708631275 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  383.61  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 968000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.3717   | \n",
      " |    critic_loss     | 0.5929   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 967899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=969000, episode_reward=-384.62 +/- 1.0462304265098439 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  384.62  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 969000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.7910   | \n",
      " |    critic_loss     | 0.4478   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 968899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=970000, episode_reward=-385.33 +/- 1.9224968995166671 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  385.33  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 970000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 0.4625   | \n",
      " |    critic_loss     | 0.1156   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 969899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=971000, episode_reward=-385.1 +/- 1.9917852145120132 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   385.1  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 971000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 0.5089   | \n",
      " |    critic_loss     | 0.1272   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 970899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=972000, episode_reward=-386.03 +/- 1.855767079552118 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  386.03  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 972000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 0.1960   | \n",
      " |    critic_loss     | 0.0490   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 971899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=973000, episode_reward=-386.27 +/- 1.8796370533868085 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  386.27  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 973000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.8446   | \n",
      " |    critic_loss     | 0.4612   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 972899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=974000, episode_reward=-386.61 +/- 1.0588078189185817 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  386.61  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 974000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.3919   | \n",
      " |    critic_loss     | 0.8480   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 973899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=975000, episode_reward=-387.24 +/- 1.81079488301839 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  387.24  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 975000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.3822   | \n",
      " |    critic_loss     | 0.8456   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 974899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=976000, episode_reward=-387.9 +/- 1.8287152515392968 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   387.9  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 976000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.7770   | \n",
      " |    critic_loss     | 0.6942   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 975899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=977000, episode_reward=-388.43 +/- 1.5456783349073746 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  388.43  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 977000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.0648   | \n",
      " |    critic_loss     | 0.5162   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 976899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=978000, episode_reward=-388.65 +/- 1.6977284585489634 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  388.65  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 978000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.9955   | \n",
      " |    critic_loss     | 0.7489   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 977899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=979000, episode_reward=-389.55 +/- 1.9735885583800992 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  389.55  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 979000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.4579   | \n",
      " |    critic_loss     | 0.6145   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 978899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=980000, episode_reward=-389.92 +/- 1.2653730847158786 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  389.92  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 980000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 0.0973   | \n",
      " |    critic_loss     | 0.0243   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 979899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=981000, episode_reward=-390.1 +/- 1.9802280301308262 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   390.1  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 981000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.6173   | \n",
      " |    critic_loss     | 0.4043   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 980899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=982000, episode_reward=-391.31 +/- 1.450770814660536 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  391.31  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 982000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 0.3397   | \n",
      " |    critic_loss     | 0.0849   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 981899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=983000, episode_reward=-391.39 +/- 1.246470842962971 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  391.39  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 983000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 0.8354   | \n",
      " |    critic_loss     | 0.2089   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 982899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=984000, episode_reward=-392.03 +/- 1.2168757902057588 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  392.03  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 984000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.9899   | \n",
      " |    critic_loss     | 0.9975   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 983899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=985000, episode_reward=-392.94 +/- 1.9319713015655458 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  392.94  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 985000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 0.5090   | \n",
      " |    critic_loss     | 0.1272   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 984899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=986000, episode_reward=-393.06 +/- 1.9753201735850832 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  393.06  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 986000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.8649   | \n",
      " |    critic_loss     | 0.7162   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 985899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=987000, episode_reward=-393.8 +/- 1.030020138610717 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |   393.8  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 987000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 0.2830   | \n",
      " |    critic_loss     | 0.0707   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 986899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=988000, episode_reward=-394.09 +/- 1.2802823404851362 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  394.09  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 988000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.3349   | \n",
      " |    critic_loss     | 0.3337   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 987899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=989000, episode_reward=-394.35 +/- 1.0009000479057508 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  394.35  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 989000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.3397   | \n",
      " |    critic_loss     | 0.5849   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 988899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=990000, episode_reward=-394.71 +/- 1.2108062550675973 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  394.71  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 990000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.9613   | \n",
      " |    critic_loss     | 0.4903   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 989899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=991000, episode_reward=-395.81 +/- 1.9182296456137389 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  395.81  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 991000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.0830   | \n",
      " |    critic_loss     | 0.7707   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 990899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=992000, episode_reward=-395.92 +/- 1.9499394964949202 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  395.92  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 992000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.1728   | \n",
      " |    critic_loss     | 0.7932   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 991899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=993000, episode_reward=-396.52 +/- 1.9289351688656506 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  396.52  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 993000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.8102   | \n",
      " |    critic_loss     | 0.4525   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 992899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=994000, episode_reward=-396.62 +/- 1.8822224339077422 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  396.62  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 994000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.1690   | \n",
      " |    critic_loss     | 0.5422   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 993899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=995000, episode_reward=-397.82 +/- 1.588694421693132 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  397.82  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 995000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.6780   | \n",
      " |    critic_loss     | 0.9195   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 994899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=996000, episode_reward=-398.46 +/- 1.0206935928363374 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  398.46  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 996000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.6604   | \n",
      " |    critic_loss     | 0.4151   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 995899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=997000, episode_reward=-398.98 +/- 1.4340317476692295 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  398.98  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 997000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 2.0574   | \n",
      " |    critic_loss     | 0.5144   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 996899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=998000, episode_reward=-398.95 +/- 1.4356765234558273 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  398.95  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 998000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.9992   | \n",
      " |    critic_loss     | 0.9998   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 997899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=999000, episode_reward=-399.79 +/- 1.8915813869011342 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  399.79  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 999000   | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 3.8111   | \n",
      " |    critic_loss     | 0.9528   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 998899   | \n",
      " --------------------------------- \n",
      " Eval num_timesteps=1000000, episode_reward=-399.74 +/- 1.3690230357335897 \n",
      " Episode length: 1000.00 +/- 0.00 \n",
      " ---------------------------------\n",
      " | eval/              |          | \n",
      " |    mean_ep_length  | 1e+03    | \n",
      " |    mean_reward     |  399.74  | \n",
      " | time/              |          | \n",
      " |    total_timesteps | 1000000  | \n",
      " | train/             |          | \n",
      " |    actor_loss      | 1.6284   | \n",
      " |    critic_loss     | 0.4071   | \n",
      " |    learning_rate   | 5.67e-05 | \n",
      " |    n_updates       | 999899   | \n",
      " --------------------------------- \n"
     ]
    },
    {
     "data": {},
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Start training\n",
    "time_steps = 1000000\n",
    "model.learn(total_timesteps=time_steps, callback=eval_callback, log_interval=10)\n",
    "model.save(\"artifacts/model/td3_car_racing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from stable_baselines3.common import results_plotter\n",
    "from utils.visualization import render_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results_plotter.plot_results([log_dir], time_steps, results_plotter.X_EPISODES, \"TD3 Training on Car Racing V2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved evaluation GIF to \"artifacts/td3_car_racer.gif\""
     ]
    }
   ],
   "source": [
    "# Save as gif\n",
    "render_example(env_id=env_id, continuous=True, render_frames=True, output_file=\"artifacts/animation/td3_car_racer.gif\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
